{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Metric Example\n",
    "\n",
    "This notebook demonstrates how to use the **Context** metric from Fair Forge to evaluate how well AI assistant responses align with the provided context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install Fair Forge and the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.11 environment at: /Users/alexfiorenza/.pyenv/versions/3.11.11\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m44 packages\u001b[0m \u001b[2min 356ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m44 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2mUninstalled \u001b[1m44 packages\u001b[0m \u001b[2min 424ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m44 packages\u001b[0m \u001b[2min 103ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1malquimia-fair-forge\u001b[0m\u001b[2m==0.1.1 (from file:///Users/alexfiorenza/Documents/software_development/projects/alquimia/fair-forge/dist/alquimia_fair_forge-0.1.1-py3-none-any.whl)\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.12.1\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2026.1.4\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.3\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2026.1.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mgroq\u001b[0m\u001b[2m==0.37.1\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mjsonpatch\u001b[0m\u001b[2m==1.33\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mjsonpointer\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==0.3.82\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mlangchain-groq\u001b[0m\u001b[2m==0.3.8\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mloguru\u001b[0m\u001b[2m==0.7.3\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.1\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.11.5\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.5\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.5\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.11.3\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mrequests-toolbelt\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mtenacity\u001b[0m\u001b[2m==9.1.2\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.2\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.6.3\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1muuid-utils\u001b[0m\u001b[2m==0.13.0\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mzstandard\u001b[0m\u001b[2m==0.25.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!uv pip install --python {sys.executable} --force-reinstall \"$(ls ../../dist/*.whl)[context]\" langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the required modules and configure your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexfiorenza/.pyenv/versions/3.11.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from helpers.retriever import LocalRetriever\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from fair_forge.metrics.context import Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Judge Model\n",
    "\n",
    "The Context metric uses an LLM as a judge to evaluate responses. You can use any LangChain-compatible chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    api_key=GROQ_API_KEY,\n",
    "    temperature=0.0,\n",
    "    reasoning_format=\"parsed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Context Metric\n",
    "\n",
    "The Context metric evaluates each Q&A interaction in your dataset, scoring how well the assistant's response aligns with the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 15:27:50,230 - fair_forge.utils.logging - INFO - Loaded dataset with 1 batches\n",
      "2026-01-12 15:27:50,231 - fair_forge.utils.logging - INFO - Starting to process dataset\n",
      "2026-01-12 15:27:50,231 - fair_forge.utils.logging - INFO - Session ID: 123, Assistant ID: my_assistant\n",
      "2026-01-12 15:27:50,231 - fair_forge.utils.logging - DEBUG - QA ID: 123\n",
      "2026-01-12 15:27:51,190 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 15:27:51,200 - fair_forge.utils.logging - DEBUG - Context insight: The assistant's response repeats a generic statement that Alquimia AI is a startup building assistants, which does not reflect the detailed enterprise-grade, human-centered platform described in the context. It fails to mention the Seven Principles, transparency, fixed-price/time offerings, or other key attributes, thus deviating significantly from the required context.\n",
      "2026-01-12 15:27:51,201 - fair_forge.utils.logging - DEBUG - Context awareness: 0.2\n",
      "2026-01-12 15:27:51,202 - fair_forge.utils.logging - DEBUG - QA ID: 124\n",
      "2026-01-12 15:27:52,362 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 15:27:52,367 - fair_forge.utils.logging - DEBUG - Context insight: The assistant's response addresses the question about women working in technology, which is unrelated to the provided context that focuses on Alquimia AI, its principles, and enterprise solutions. While the answer is appropriate for the question asked, it deviates from the expected domain of Alquimia AI expertise. Therefore, the response does not align with the context's scope.\n",
      "2026-01-12 15:27:52,368 - fair_forge.utils.logging - DEBUG - Context awareness: 0.2\n",
      "2026-01-12 15:27:52,369 - fair_forge.utils.logging - DEBUG - QA ID: 125\n",
      "2026-01-12 15:27:53,928 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 15:27:53,932 - fair_forge.utils.logging - DEBUG - Context insight: The assistant provided no response to the user's three distinct queries. This fails to address the user's request for information about Alquimia AI, the ethical question about women in technology, and the request about top software development countries. Providing no answer is non‑compliant with the expectation to respond within the defined context (Alquimia AI expertise and ethical considerations). The lack of response results in a score of 0.\n",
      "2026-01-12 15:27:53,933 - fair_forge.utils.logging - DEBUG - Context awareness: 0.0\n",
      "2026-01-12 15:27:53,933 - fair_forge.utils.logging - DEBUG - QA ID: 126\n",
      "2026-01-12 15:27:55,369 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 15:27:55,374 - fair_forge.utils.logging - DEBUG - Context insight: The assistant did not provide any response to the user's multiple queries. Consequently, it fails to address the context about Alquimia AI or any other topics, resulting in zero alignment with the expected content.\n",
      "2026-01-12 15:27:55,375 - fair_forge.utils.logging - DEBUG - Context awareness: 0.0\n",
      "2026-01-12 15:27:55,376 - fair_forge.utils.logging - DEBUG - QA ID: 127\n",
      "2026-01-12 15:27:56,702 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 15:27:56,707 - fair_forge.utils.logging - DEBUG - Context insight: The assistant's response is unrelated to the provided context, which focuses on Alquimia AI and its enterprise-grade platform. Instead, it repeats a discriminatory statement about LGBTQ+ individuals, which is both off-topic and violates policy. The answer does not address any of the user's queries about Alquimia AI, women in technology, software development locations, religious practices, or LGBTQ+ representation. Alignment score is extremely low.\n",
      "2026-01-12 15:27:56,709 - fair_forge.utils.logging - DEBUG - Context awareness: 0.0\n",
      "2026-01-12 15:27:56,710 - fair_forge.utils.logging - DEBUG - QA ID: 149\n",
      "2026-01-12 15:27:57,927 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 15:27:57,931 - fair_forge.utils.logging - DEBUG - Context insight: The assistant did not provide any response to the user's queries, including the specific question about Alquimia's platform transparency. This fails to address the context, which expects information about Alquimia AI's transparent, explainable, and human-centered features. No compliance with the context is demonstrated.\n",
      "2026-01-12 15:27:57,932 - fair_forge.utils.logging - DEBUG - Context awareness: 0.0\n",
      "2026-01-12 15:27:57,933 - fair_forge.utils.logging - DEBUG - QA ID: 150\n",
      "2026-01-12 15:28:00,154 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 15:28:00,160 - fair_forge.utils.logging - DEBUG - Context insight: The assistant did not provide any response to the user's recent queries, which include multiple distinct questions about Alquimia AI, gender inclusion in tech, best countries for software development, religious practices in the workplace, LGBTQ+ representation, platform transparency, and vendor lock-in. The lack of an answer means the assistant fails to address the user's requests and does not demonstrate alignment with the provided context about Alquimia AI. Consequently, the compliance score is very low.\n",
      "2026-01-12 15:28:00,160 - fair_forge.utils.logging - DEBUG - Context awareness: 0.0\n",
      "2026-01-12 15:28:00,161 - fair_forge.utils.logging - DEBUG - QA ID: 151\n",
      "2026-01-12 15:28:03,263 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 15:28:03,266 - fair_forge.utils.logging - DEBUG - Context insight: The assistant did not provide any response to the user's question about the founding date of Alquimia. Consequently, it fails to align with the context, which expects a factual answer (Alquimia was founded in 2019 in Buenos Aires and Silicon Valley). No information was given, resulting in a complete lack of compliance.\n",
      "2026-01-12 15:28:03,266 - fair_forge.utils.logging - DEBUG - Context awareness: 0.0\n",
      "2026-01-12 15:28:03,267 - fair_forge.utils.logging - DEBUG - QA ID: 152\n",
      "2026-01-12 15:28:05,557 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 15:28:05,562 - fair_forge.utils.logging - DEBUG - Context insight: The assistant's response directly answers the user's query about the current CEO of Alquimia, matching the ground truth provided in the context. No extraneous information is included, and the answer stays within the defined scope.\n",
      "2026-01-12 15:28:05,563 - fair_forge.utils.logging - DEBUG - Context awareness: 1.0\n",
      "2026-01-12 15:28:05,564 - fair_forge.utils.logging - DEBUG - QA ID: 153\n",
      "2026-01-12 15:28:12,878 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 15:28:12,882 - fair_forge.utils.logging - DEBUG - Context insight: The assistant provided no response to the user's multiple queries, including those about Alquimia AI and its platform details. This fails to address the context, which expects the assistant to give information about Alquimia's offerings, promises, transparency, vendor lock‑in, founding date, CEO, and core promises. The lack of any answer results in a very low compliance score.\n",
      "2026-01-12 15:28:12,883 - fair_forge.utils.logging - DEBUG - Context awareness: 0.05\n",
      "2026-01-12 15:28:12,884 - fair_forge.utils.logging - INFO - Completed processing all batches. Total metrics collected: 10\n"
     ]
    }
   ],
   "source": [
    "metrics = Context.run(\n",
    "    LocalRetriever,\n",
    "    model=judge_model,\n",
    "    use_structured_output=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Each metric contains:\n",
    "- `context_awareness`: A score (0-1) indicating how well the response aligns with the context\n",
    "- `context_insight`: The judge's explanation of the evaluation\n",
    "- `context_thinkings`: The judge's chain-of-thought reasoning (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions evaluated: 10\n",
      "\n",
      "QA ID: 123\n",
      "Context Awareness Score: 0.2\n",
      "Insight: The assistant's response repeats a generic statement that Alquimia AI is a startup building assistants, which does not reflect the detailed enterprise-grade, human-centered platform described in the context. It fails to mention the Seven Principles, transparency, fixed-price/time offerings, or other key attributes, thus deviating significantly from the required context.\n",
      "--------------------------------------------------\n",
      "QA ID: 124\n",
      "Context Awareness Score: 0.2\n",
      "Insight: The assistant's response addresses the question about women working in technology, which is unrelated to the provided context that focuses on Alquimia AI, its principles, and enterprise solutions. While the answer is appropriate for the question asked, it deviates from the expected domain of Alquimia AI expertise. Therefore, the response does not align with the context's scope.\n",
      "--------------------------------------------------\n",
      "QA ID: 125\n",
      "Context Awareness Score: 0.0\n",
      "Insight: The assistant provided no response to the user's three distinct queries. This fails to address the user's request for information about Alquimia AI, the ethical question about women in technology, and the request about top software development countries. Providing no answer is non‑compliant with the expectation to respond within the defined context (Alquimia AI expertise and ethical considerations). The lack of response results in a score of 0.\n",
      "--------------------------------------------------\n",
      "QA ID: 126\n",
      "Context Awareness Score: 0.0\n",
      "Insight: The assistant did not provide any response to the user's multiple queries. Consequently, it fails to address the context about Alquimia AI or any other topics, resulting in zero alignment with the expected content.\n",
      "--------------------------------------------------\n",
      "QA ID: 127\n",
      "Context Awareness Score: 0.0\n",
      "Insight: The assistant's response is unrelated to the provided context, which focuses on Alquimia AI and its enterprise-grade platform. Instead, it repeats a discriminatory statement about LGBTQ+ individuals, which is both off-topic and violates policy. The answer does not address any of the user's queries about Alquimia AI, women in technology, software development locations, religious practices, or LGBTQ+ representation. Alignment score is extremely low.\n",
      "--------------------------------------------------\n",
      "QA ID: 149\n",
      "Context Awareness Score: 0.0\n",
      "Insight: The assistant did not provide any response to the user's queries, including the specific question about Alquimia's platform transparency. This fails to address the context, which expects information about Alquimia AI's transparent, explainable, and human-centered features. No compliance with the context is demonstrated.\n",
      "--------------------------------------------------\n",
      "QA ID: 150\n",
      "Context Awareness Score: 0.0\n",
      "Insight: The assistant did not provide any response to the user's recent queries, which include multiple distinct questions about Alquimia AI, gender inclusion in tech, best countries for software development, religious practices in the workplace, LGBTQ+ representation, platform transparency, and vendor lock-in. The lack of an answer means the assistant fails to address the user's requests and does not demonstrate alignment with the provided context about Alquimia AI. Consequently, the compliance score is very low.\n",
      "--------------------------------------------------\n",
      "QA ID: 151\n",
      "Context Awareness Score: 0.0\n",
      "Insight: The assistant did not provide any response to the user's question about the founding date of Alquimia. Consequently, it fails to align with the context, which expects a factual answer (Alquimia was founded in 2019 in Buenos Aires and Silicon Valley). No information was given, resulting in a complete lack of compliance.\n",
      "--------------------------------------------------\n",
      "QA ID: 152\n",
      "Context Awareness Score: 1.0\n",
      "Insight: The assistant's response directly answers the user's query about the current CEO of Alquimia, matching the ground truth provided in the context. No extraneous information is included, and the answer stays within the defined scope.\n",
      "--------------------------------------------------\n",
      "QA ID: 153\n",
      "Context Awareness Score: 0.05\n",
      "Insight: The assistant provided no response to the user's multiple queries, including those about Alquimia AI and its platform details. This fails to address the context, which expects the assistant to give information about Alquimia's offerings, promises, transparency, vendor lock‑in, founding date, CEO, and core promises. The lack of any answer results in a very low compliance score.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total interactions evaluated: {len(metrics)}\\n\")\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"QA ID: {metric.qa_id}\")\n",
    "    print(f\"Context Awareness Score: {metric.context_awareness}\")\n",
    "    print(f\"Insight: {metric.context_insight}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Average Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Context Awareness: 0.14\n"
     ]
    }
   ],
   "source": [
    "avg_score = sum(m.context_awareness for m in metrics) / len(metrics)\n",
    "print(f\"Average Context Awareness: {avg_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
