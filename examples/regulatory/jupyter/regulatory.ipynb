{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Regulatory Compliance Metric Example\n",
    "\n",
    "This notebook demonstrates how to use the **Regulatory** metric from Fair Forge to evaluate whether AI assistant responses comply with a regulatory corpus (e.g., company policies, legal frameworks).\n",
    "\n",
    "The metric uses:\n",
    "1. **Embedding-based retrieval** to find relevant regulatory chunks\n",
    "2. **Reranker model** to detect contradictions between responses and regulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install Fair Forge with the regulatory dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"alquimia-fair-forge[regulatory]\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from fair_forge.connectors import LocalCorpusConnector\n",
    "from fair_forge.core import Retriever\n",
    "from fair_forge.metrics.regulatory import Regulatory\n",
    "from fair_forge.schemas.common import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Create a Custom Retriever\n",
    "\n",
    "Load the conversation dataset from the local data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalRetriever(Retriever):\n",
    "    \"\"\"Load conversations from local JSON file.\"\"\"\n",
    "\n",
    "    def load_dataset(self) -> list[Dataset]:\n",
    "        data_path = Path(\"../data/dataset.json\")\n",
    "        with open(data_path, encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return [Dataset.model_validate(d) for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Configure the Corpus Connector\n",
    "\n",
    "Point the corpus connector to the directory containing regulatory markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = Path(\"../corpus\")\n",
    "corpus_connector = LocalCorpusConnector(corpus_dir)\n",
    "\n",
    "# Verify documents load correctly\n",
    "documents = corpus_connector.load_documents()\n",
    "print(f\"Loaded {len(documents)} regulatory document(s):\")\n",
    "for doc in documents:\n",
    "    print(f\"  - {doc.source}: {len(doc.text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Run the Regulatory Metric\n",
    "\n",
    "Configure and run the metric. Key parameters:\n",
    "- `embedding_model`: Model for semantic retrieval (default: Qwen3-Embedding-0.6B)\n",
    "- `reranker_model`: Model for contradiction detection (default: Qwen3-Reranker-0.6B)\n",
    "- `chunk_size`: Characters per chunk\n",
    "- `top_k`: Max chunks to retrieve per query\n",
    "- `similarity_threshold`: Minimum similarity for retrieval\n",
    "- `contradiction_threshold`: Score below which a chunk contradicts the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Regulatory.run(\n",
    "    LocalRetriever,\n",
    "    corpus_connector=corpus_connector,\n",
    "    embedding_model=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    reranker_model=\"Qwen/Qwen3-Reranker-0.6B\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    top_k=5,\n",
    "    similarity_threshold=0.3,\n",
    "    contradiction_threshold=0.6,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Each metric contains:\n",
    "- `compliance_score`: Score from 0-1 (ratio of supporting chunks)\n",
    "- `verdict`: COMPLIANT, NON_COMPLIANT, or IRRELEVANT\n",
    "- `supporting_chunks`: Number of chunks that support the response\n",
    "- `contradicting_chunks`: Number of chunks that contradict the response\n",
    "- `retrieved_chunks`: Detailed information about each retrieved chunk\n",
    "- `insight`: Human-readable explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total interactions evaluated: {len(metrics)}\\n\")\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"QA ID: {metric.qa_id}\")\n",
    "    print(f\"Query: {metric.query[:80]}...\" if len(metric.query) > 80 else f\"Query: {metric.query}\")\n",
    "    print(f\"Verdict: {metric.verdict}\")\n",
    "    print(f\"Compliance Score: {metric.compliance_score:.2f}\")\n",
    "    print(f\"Supporting: {metric.supporting_chunks}, Contradicting: {metric.contradicting_chunks}\")\n",
    "    print(f\"Insight: {metric.insight}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Examine Retrieved Chunks\n",
    "\n",
    "View the regulatory chunks that were matched for each interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    print(f\"\\n=== QA ID: {metric.qa_id} ===\")\n",
    "    print(f\"Response: {metric.assistant[:100]}...\")\n",
    "    print(f\"\\nRetrieved {len(metric.retrieved_chunks)} chunk(s):\")\n",
    "    \n",
    "    for chunk in metric.retrieved_chunks:\n",
    "        print(f\"\\n  [{chunk.verdict}] {chunk.source} (chunk #{chunk.chunk_index})\")\n",
    "        print(f\"  Similarity: {chunk.similarity:.4f}, Reranker: {chunk.reranker_score:.4f}\")\n",
    "        print(f\"  Preview: {chunk.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Calculate Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "compliant = sum(1 for m in metrics if m.verdict == \"COMPLIANT\")\n",
    "non_compliant = sum(1 for m in metrics if m.verdict == \"NON_COMPLIANT\")\n",
    "irrelevant = sum(1 for m in metrics if m.verdict == \"IRRELEVANT\")\n",
    "avg_score = sum(m.compliance_score for m in metrics) / len(metrics)\n",
    "\n",
    "print(\"=== Summary ===\")\n",
    "print(f\"Total interactions: {len(metrics)}\")\n",
    "print(f\"Compliant: {compliant} ({100*compliant/len(metrics):.1f}%)\")\n",
    "print(f\"Non-Compliant: {non_compliant} ({100*non_compliant/len(metrics):.1f}%)\")\n",
    "print(f\"Irrelevant: {irrelevant} ({100*irrelevant/len(metrics):.1f}%)\")\n",
    "print(f\"Average Compliance Score: {avg_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save the results to JSON for further analysis or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [metric.model_dump() for metric in metrics]\n",
    "\n",
    "output_path = Path(\"../data/regulatory_results.json\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
