{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Attribution Analysis with Fair Forge\n",
    "\n",
    "This notebook demonstrates how to use Fair Forge's explainability module to compute and visualize token attributions for language model responses.\n",
    "\n",
    "Token attribution helps answer: **\"Which parts of the input influenced the model's output the most?\"**\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install Fair Forge with explainability support:\n",
    "```bash\n",
    "pip install alquimia-fair-forge[explainability]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "#%pip install alquimia-fair-forge[explainability] transformers torch -q\n",
    "#!uv pip install --python {sys.executable} --force-reinstall \"$(ls ../../../dist/*.whl)[explainability]\" -q\n",
    "#%pip install transformers torch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Load Model and Tokenizer\n",
    "\n",
    "We'll use a small model for demonstration. The explainability module works with any HuggingFace causal language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 757479 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load a small model (Qwen3-0.6B for this example)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen3-0.6B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(repo_id)\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     12\u001b[0m     repo_id,\n\u001b[1;32m     13\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16  \u001b[38;5;66;03m# Important: use float16 for attribution methods\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:880\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    878\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m         )\n\u001b[0;32m--> 880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    882\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2110\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2107\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2108\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[1;32m   2111\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   2112\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   2113\u001b[0m     init_configuration,\n\u001b[1;32m   2114\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[1;32m   2115\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   2116\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   2117\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   2118\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   2119\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[1;32m   2120\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   2121\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2122\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2336\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2336\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[1;32m   2337\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2341\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/qwen2/tokenization_qwen2_fast.py:120\u001b[0m, in \u001b[0;36mQwen2TokenizerFast.__init__\u001b[0;34m(self, vocab_file, merges_file, tokenizer_file, unk_token, bos_token, eos_token, pad_token, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m unk_token \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    110\u001b[0m     AddedToken(unk_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(unk_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m unk_token\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m pad_token \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m     AddedToken(pad_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pad_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m pad_token\n\u001b[1;32m    118\u001b[0m )\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    121\u001b[0m     vocab_file,\n\u001b[1;32m    122\u001b[0m     merges_file,\n\u001b[1;32m    123\u001b[0m     tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[1;32m    124\u001b[0m     unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[1;32m    125\u001b[0m     bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[1;32m    126\u001b[0m     eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[1;32m    127\u001b[0m     pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    129\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[0;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 757479 column 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Disable autocast for compatibility with attribution methods\n",
    "torch.set_autocast_enabled(False)\n",
    "\n",
    "# Load a small model (Qwen3-0.6B for this example)\n",
    "repo_id = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    torch_dtype=torch.float16  # Important: use float16 for attribution methods\n",
    ")\n",
    "\n",
    "print(f\"Loaded model: {repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Fair Forge Explainability Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fair_forge.explainability import (\n",
    "    AttributionExplainer,\n",
    "    AttributionMethod,\n",
    "    Granularity,\n",
    "    compute_attributions,\n",
    ")\n",
    "\n",
    "# Create an explainer instance\n",
    "explainer = AttributionExplainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    default_method=AttributionMethod.LIME,  # LIME is a good default\n",
    "    default_granularity=Granularity.WORD,   # Word-level is most interpretable\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Explainer created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Attributions for a Single Response\n",
    "\n",
    "Let's compute attributions for a simple Q&A interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversation\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Answer concisely with 3 bullet points.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain Albert Einstein's theory of relativity\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# The model's actual response (target text to explain)\n",
    "target = \"\"\"\n",
    "- Einstein proposed that time and space are not absolute but relative, influencing how we perceive the universe.\n",
    "- He introduced the concept of spacetime curvature, which explains gravity as a distortion of spacetime.\n",
    "- His equations, like the general and special theories, have revolutionized our understanding of physics.\n",
    "\"\"\"\n",
    "\n",
    "# Compute attributions\n",
    "result = explainer.explain(\n",
    "    messages=messages,\n",
    "    target=target,\n",
    "    method=AttributionMethod.LIME,\n",
    ")\n",
    "\n",
    "print(f\"Computed {len(result.attributions)} attributions\")\n",
    "print(f\"Method: {result.method.value}\")\n",
    "print(f\"Granularity: {result.granularity.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Top Contributing Tokens/Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 most important tokens\n",
    "top_10 = result.get_top_k(10)\n",
    "\n",
    "print(\"Top 10 Most Important Words/Tokens:\")\n",
    "print(\"-\" * 40)\n",
    "for i, attr in enumerate(top_10, 1):\n",
    "    print(f\"{i:2}. '{attr.text:15}' | score: {attr.score:+.4f} | normalized: {attr.normalized_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Attributions\n",
    "\n",
    "The explainer can display an interactive visualization of the attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the attribution visualization\n",
    "explainer.visualize(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Different Attribution Methods\n",
    "\n",
    "Fair Forge supports multiple attribution methods. Let's compare a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simpler example for faster computation\n",
    "simple_messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "simple_target = \"The capital of France is Paris.\"\n",
    "\n",
    "# Compare different methods\n",
    "methods_to_compare = [\n",
    "    AttributionMethod.LIME,\n",
    "    AttributionMethod.OCCLUSION,\n",
    "]\n",
    "\n",
    "for method in methods_to_compare:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Method: {method.value.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    result = explainer.explain(\n",
    "        messages=simple_messages,\n",
    "        target=simple_target,\n",
    "        method=method,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTop 5 contributing words:\")\n",
    "    for attr in result.get_top_k(5):\n",
    "        print(f\"  '{attr.text}': {attr.score:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Processing\n",
    "\n",
    "Process multiple Q&A pairs at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple Q&A pairs\n",
    "qa_pairs = [\n",
    "    (\n",
    "        [{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n",
    "        \"Machine learning is a subset of AI that enables systems to learn from data.\"\n",
    "    ),\n",
    "    (\n",
    "        [{\"role\": \"user\", \"content\": \"What is deep learning?\"}],\n",
    "        \"Deep learning uses neural networks with many layers to process complex patterns.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Compute attributions for all pairs\n",
    "batch_results = explainer.explain_batch(qa_pairs)\n",
    "\n",
    "print(f\"Processed {len(batch_results)} items\")\n",
    "print(f\"Total compute time: {batch_results.total_compute_time_seconds:.2f}s\")\n",
    "\n",
    "# Show top words for each\n",
    "for i, result in enumerate(batch_results):\n",
    "    print(f\"\\nQ&A #{i+1}:\")\n",
    "    print(f\"  Top 3 words: {[attr.text for attr in result.get_top_k(3)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using the Convenience Function\n",
    "\n",
    "For one-off attributions, use the `compute_attributions` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick one-liner for attribution computation\n",
    "quick_result = compute_attributions(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
    "    target=\"The sky appears blue due to Rayleigh scattering of sunlight.\",\n",
    "    method=AttributionMethod.LIME,\n",
    "    granularity=Granularity.WORD,\n",
    ")\n",
    "\n",
    "print(\"Top 5 contributing words:\")\n",
    "for attr in quick_result.get_top_k(5):\n",
    "    print(f\"  '{attr.text}': {attr.score:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to dictionary format\n",
    "viz_data = quick_result.to_dict_for_visualization()\n",
    "print(\"Visualization data structure:\")\n",
    "print(f\"  tokens: {viz_data['tokens'][:5]}...\")\n",
    "print(f\"  scores: {viz_data['scores'][:5]}...\")\n",
    "\n",
    "# Export full result as JSON-compatible dict\n",
    "result_dict = quick_result.model_dump()\n",
    "print(f\"\\nFull result keys: {list(result_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Setup**: Loading a model and creating an `AttributionExplainer`\n",
    "2. **Single attributions**: Using `explainer.explain()` for individual Q&A pairs\n",
    "3. **Top-K analysis**: Finding the most important tokens with `get_top_k()`\n",
    "4. **Visualization**: Interactive display with `explainer.visualize()`\n",
    "5. **Method comparison**: Comparing LIME, Occlusion, and other methods\n",
    "6. **Batch processing**: Processing multiple items with `explain_batch()`\n",
    "7. **Convenience function**: Quick one-off attribution with `compute_attributions()`\n",
    "8. **Export**: Getting data for further analysis\n",
    "\n",
    "### Available Attribution Methods\n",
    "\n",
    "**Gradient-based** (faster, require differentiable models):\n",
    "- `SALIENCY`, `INTEGRATED_GRADIENTS`, `GRADIENT_SHAP`, `SMOOTH_GRAD`, `SQUARE_GRAD`, `VAR_GRAD`, `INPUT_X_GRADIENT`\n",
    "\n",
    "**Perturbation-based** (model-agnostic, more robust):\n",
    "- `LIME`, `KERNEL_SHAP`, `OCCLUSION`, `SOBOL`\n",
    "\n",
    "### Granularity Options\n",
    "- `TOKEN`: Individual tokens (finest granularity)\n",
    "- `WORD`: Word-level (recommended for interpretability)\n",
    "- `SENTENCE`: Sentence-level (coarsest granularity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
