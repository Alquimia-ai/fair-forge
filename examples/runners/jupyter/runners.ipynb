{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Fair Forge Runners Example\n",
    "\n",
    "This notebook demonstrates how to use the Fair Forge runners module to execute test datasets against AI systems.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The runners module provides:\n",
    "- **BaseRunner**: Abstract interface for implementing custom runners\n",
    "- **AlquimiaRunner**: Implementation for Alquimia AI agents\n",
    "- **Storage backends**: Local filesystem and LakeFS support for loading test datasets and saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Create `.env` file:\n",
    "```.env\n",
    "ALQUIMIA_API_KEY=...\n",
    "ALQUIMIA_URL=...\n",
    "AGENT_ID=...\n",
    "CHANNEL_ID=...\n",
    "\n",
    "```\n",
    "\n",
    "Install requiered dependencies:\n",
    "```bash\n",
    "uv venv\n",
    "source .venv/bin/activate\n",
    "uv pip install .[runners,cloud]\n",
    "uv run jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from fair_forge.runners import AlquimiaRunner\n",
    "from fair_forge.schemas import Batch, Dataset\n",
    "from fair_forge.storage import create_local_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mock-data",
   "metadata": {},
   "source": [
    "## Create Mock Test Data\n",
    "\n",
    "Let's create some mock test datasets to demonstrate the runner functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "create-mock-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mock dataset: test_session_22549597\n",
      "Number of test cases: 3\n"
     ]
    }
   ],
   "source": [
    "# Create mock batches (test cases)\n",
    "mock_batches = [\n",
    "    Batch(\n",
    "        qa_id=\"test_001\",\n",
    "        query=\"What is the capital of France?\",\n",
    "        assistant=\"\",  # Will be filled by the runner\n",
    "        ground_truth_assistant=\"The capital of France is Paris.\",\n",
    "        observation=\"Basic geography question\",\n",
    "        agentic={},\n",
    "        ground_truth_agentic={},\n",
    "    ),\n",
    "    Batch(\n",
    "        qa_id=\"test_002\",\n",
    "        query=\"Explain quantum computing in simple terms.\",\n",
    "        assistant=\"\",\n",
    "        ground_truth_assistant=\"Quantum computing uses quantum mechanics principles to process information...\",\n",
    "        observation=\"Technical explanation test\",\n",
    "        agentic={},\n",
    "        ground_truth_agentic={},\n",
    "    ),\n",
    "    Batch(\n",
    "        qa_id=\"test_003\",\n",
    "        query=\"Write a haiku about programming.\",\n",
    "        assistant=\"\",\n",
    "        ground_truth_assistant=\"Code flows like water\\nBugs hide in silent shadows\\nDebugger reveals\",\n",
    "        observation=\"Creative writing test\",\n",
    "        agentic={},\n",
    "        ground_truth_agentic={},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create mock dataset\n",
    "mock_dataset = Dataset(\n",
    "    session_id=f\"test_session_{uuid.uuid4().hex[:8]}\",\n",
    "    assistant_id=\"test_assistant_001\",\n",
    "    language=\"english\",\n",
    "    context=\"General knowledge and creative writing test suite\",\n",
    "    conversation=mock_batches,\n",
    ")\n",
    "\n",
    "print(f\"Created mock dataset: {mock_dataset.session_id}\")\n",
    "print(f\"Number of test cases: {len(mock_dataset.conversation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "storage-setup",
   "metadata": {},
   "source": [
    "## Storage Setup\n",
    "\n",
    "### Option 1: Local Storage\n",
    "\n",
    "Use local filesystem for storing test datasets and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "local-storage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-13 10:12:59.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.storage\u001b[0m:\u001b[36mcreate_local_storage\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mCreating local filesystem storage\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock dataset saved to local storage\n"
     ]
    }
   ],
   "source": [
    "# Create local storage instance\n",
    "local_storage = create_local_storage(\n",
    "    tests_dir=Path(\"./test_datasets\"),\n",
    "    results_dir=Path(\"./test_results\"),\n",
    "    enabled_suites=None,  # Load all test suites\n",
    ")\n",
    "\n",
    "# Save mock dataset to local storage for later loading\n",
    "test_datasets_dir = Path(\"./test_datasets\")\n",
    "test_datasets_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(test_datasets_dir / \"mock_test_suite.json\", \"w\") as f:\n",
    "    json.dump(mock_dataset.model_dump(), f, indent=2)\n",
    "\n",
    "print(\"Mock dataset saved to local storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lakefs-storage",
   "metadata": {},
   "source": [
    "### Option 2: LakeFS Storage (Optional)\n",
    "\n",
    "Use LakeFS for cloud-based storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lakefs-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and configure if using LakeFS\n",
    "# lakefs_storage = create_lakefs_storage(\n",
    "#     host=\"http://lakefs.example.com:8000\",\n",
    "#     username=\"admin\",\n",
    "#     password=\"your-password\",\n",
    "#     repo_id=\"fair-forge-tests\",\n",
    "#     enabled_suites=None,\n",
    "#     tests_prefix=\"tests/\",\n",
    "#     results_prefix=\"results/\",\n",
    "#     branch_name=\"main\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "runner-setup",
   "metadata": {},
   "source": [
    "## Runner Setup\n",
    "\n",
    "### AlquimiaRunner Configuration\n",
    "\n",
    "Configure the Alquimia runner to execute tests against your AI agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "runner-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runner configured successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Alquimia runner\n",
    "# NOTE: Set these environment variables or replace with your actual values\n",
    "runner = AlquimiaRunner(\n",
    "    base_url=os.getenv(\"ALQUIMIA_URL\", \"https://api.alquimia.ai\"),\n",
    "    api_key=os.getenv(\"ALQUIMIA_API_KEY\", \"your-api-key\"),\n",
    "    agent_id=os.getenv(\"AGENT_ID\", \"your-agent-id\"),\n",
    "    channel_id=os.getenv(\"CHANNEL_ID\", \"your-channel-id\"),\n",
    "    api_version=os.getenv(\"ALQUIMIA_VERSION\", \"\"),\n",
    ")\n",
    "\n",
    "print(\"Runner configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execute-tests",
   "metadata": {},
   "source": [
    "## Execute Tests\n",
    "\n",
    "### Run Single Batch\n",
    "\n",
    "Execute a single test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "run-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a single batch\n",
    "async def run_single_batch():\n",
    "    batch = mock_batches[0]\n",
    "    session_id = f\"test_session_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "    print(f\"Running batch: {batch.qa_id}\")\n",
    "    print(f\"Query: {batch.query}\")\n",
    "\n",
    "    updated_batch, success, exec_time = await runner.run_batch(batch, session_id)\n",
    "\n",
    "    print(f\"\\nSuccess: {success}\")\n",
    "    print(f\"Execution time: {exec_time:.2f}ms\")\n",
    "    print(f\"Response: {updated_batch.assistant}\")\n",
    "\n",
    "    return updated_batch\n",
    "\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "# result = await run_single_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-dataset",
   "metadata": {},
   "source": [
    "### Run Complete Dataset\n",
    "\n",
    "Execute all test cases in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "run-full-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete dataset\n",
    "async def run_complete_dataset():\n",
    "    print(f\"Running dataset: {mock_dataset.session_id}\")\n",
    "    print(f\"Total batches: {len(mock_dataset.conversation)}\\n\")\n",
    "\n",
    "    updated_dataset, summary = await runner.run_dataset(mock_dataset)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXECUTION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Session ID: {summary['session_id']}\")\n",
    "    print(f\"Total batches: {summary['total_batches']}\")\n",
    "    print(f\"Successes: {summary['successes']}\")\n",
    "    print(f\"Failures: {summary['failures']}\")\n",
    "    print(f\"Total execution time: {summary['total_execution_time_ms']:.2f}ms\")\n",
    "    print(f\"Average batch time: {summary['avg_batch_time_ms']:.2f}ms\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return updated_dataset, summary\n",
    "\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "# results, summary = await run_complete_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-results",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Save execution results to storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "save-to-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to local storage\n",
    "async def save_results(updated_dataset):\n",
    "    run_id = str(uuid.uuid4())\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    result_path = local_storage.save_results(\n",
    "        datasets=[updated_dataset],\n",
    "        run_id=run_id,\n",
    "        timestamp=timestamp,\n",
    "    )\n",
    "\n",
    "    print(f\"Results saved to: {result_path}\")\n",
    "    return result_path\n",
    "\n",
    "\n",
    "# Execute (uncomment to run after running dataset)\n",
    "# result_path = await save_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-datasets",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "Load test datasets from storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "load-from-storage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-13 10:17:31.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mLoading test datasets from test_datasets\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:31.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mFound 1 JSON test file(s)\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:31.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mLoading test dataset: mock_test_suite\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:31.051\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m76\u001b[0m - \u001b[32m\u001b[1mLoaded dataset from mock_test_suite\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:31.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mLoaded 1 dataset(s) with 3 total test case(s)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 dataset(s)\n",
      "  - test_session_22549597: 3 batches\n"
     ]
    }
   ],
   "source": [
    "# Load datasets from local storage\n",
    "loaded_datasets = local_storage.load_datasets()\n",
    "\n",
    "print(f\"Loaded {len(loaded_datasets)} dataset(s)\")\n",
    "for ds in loaded_datasets:\n",
    "    print(f\"  - {ds.session_id}: {len(ds.conversation)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-pipeline",
   "metadata": {},
   "source": [
    "## Complete Pipeline Example\n",
    "\n",
    "Put it all together in a complete pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complete-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-13 10:17:54.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mLoading test datasets from test_datasets\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:54.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mFound 1 JSON test file(s)\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:54.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mLoading test dataset: mock_test_suite\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:54.128\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m76\u001b[0m - \u001b[32m\u001b[1mLoaded dataset from mock_test_suite\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:54.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mLoaded 1 dataset(s) with 3 total test case(s)\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:54.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.runners.alquimia_runner\u001b[0m:\u001b[36mrun_dataset\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mRunning dataset: test_session_22549597 (3 batch(es))\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:54.130\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.runners.alquimia_runner\u001b[0m:\u001b[36mrun_dataset\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1m  Batch 1/3: test_001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading test datasets...\n",
      "Loaded 1 dataset(s)\n",
      "\n",
      "Step 2: Executing datasets...\n",
      "\n",
      "[1/1] Processing: test_session_22549597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 10:17:55,466 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/vo-community-assistant?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-13 10:17:55,875 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-b51a600b3a724608a6e69e95fb54925a?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-13 10:17:56.385\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.runners.alquimia_runner\u001b[0m:\u001b[36mrun_batch\u001b[0m:\u001b[36m111\u001b[0m - \u001b[34m\u001b[1m  ✓ Batch test_001 completed (2253.7ms)\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:56.385\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.runners.alquimia_runner\u001b[0m:\u001b[36mrun_dataset\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1m  Batch 2/3: test_002\u001b[0m\n",
      "2026-01-13 10:17:57,206 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/vo-community-assistant?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-13 10:17:57,410 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-40eac577a3564e0197d9a5610f576235?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-13 10:17:58.026\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.runners.alquimia_runner\u001b[0m:\u001b[36mrun_batch\u001b[0m:\u001b[36m111\u001b[0m - \u001b[34m\u001b[1m  ✓ Batch test_002 completed (1639.9ms)\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:58.027\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.runners.alquimia_runner\u001b[0m:\u001b[36mrun_dataset\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1m  Batch 3/3: test_003\u001b[0m\n",
      "2026-01-13 10:17:58,799 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/vo-community-assistant?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-13 10:17:59,151 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-fffc0d8575c24c2094a88ac2fd991cdc?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-13 10:17:59.562\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.runners.alquimia_runner\u001b[0m:\u001b[36mrun_batch\u001b[0m:\u001b[36m111\u001b[0m - \u001b[34m\u001b[1m  ✓ Batch test_003 completed (1533.5ms)\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:59.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.runners.alquimia_runner\u001b[0m:\u001b[36mrun_dataset\u001b[0m:\u001b[36m177\u001b[0m - \u001b[1m✓ Dataset test_session_22549597 completed: 3/3 passed (5433.6ms)\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:59.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36msave_results\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mSaving results locally: test_results/test_run_20260113_101759_da1b62d2-2379-4dca-b0ec-1b05091f1b1c.json\u001b[0m\n",
      "\u001b[32m2026-01-13 10:17:59.566\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mfair_forge.storage.local_storage\u001b[0m:\u001b[36msave_results\u001b[0m:\u001b[36m119\u001b[0m - \u001b[32m\u001b[1mResults saved locally to test_results/test_run_20260113_101759_da1b62d2-2379-4dca-b0ec-1b05091f1b1c.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Saving results...\n",
      "\n",
      "======================================================================\n",
      "OVERALL SUMMARY\n",
      "======================================================================\n",
      "Total datasets: 1\n",
      "Total test cases: 3\n",
      "Successes: 3\n",
      "Failures: 0\n",
      "Success rate: 100.0%\n",
      "Results saved to: test_results/test_run_20260113_101759_da1b62d2-2379-4dca-b0ec-1b05091f1b1c.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "async def complete_pipeline():\n",
    "    \"\"\"Complete test execution pipeline.\"\"\"\n",
    "\n",
    "    # 1. Load datasets from storage\n",
    "    print(\"Step 1: Loading test datasets...\")\n",
    "    datasets = local_storage.load_datasets()\n",
    "    print(f\"Loaded {len(datasets)} dataset(s)\\n\")\n",
    "\n",
    "    if not datasets:\n",
    "        print(\"No datasets found!\")\n",
    "        return\n",
    "\n",
    "    # 2. Execute all datasets\n",
    "    print(\"Step 2: Executing datasets...\")\n",
    "    executed_datasets = []\n",
    "    all_summaries = []\n",
    "\n",
    "    for i, dataset in enumerate(datasets, 1):\n",
    "        print(f\"\\n[{i}/{len(datasets)}] Processing: {dataset.session_id}\")\n",
    "        updated_dataset, summary = await runner.run_dataset(dataset)\n",
    "        executed_datasets.append(updated_dataset)\n",
    "        all_summaries.append(summary)\n",
    "\n",
    "    # 3. Save results\n",
    "    print(\"\\nStep 3: Saving results...\")\n",
    "    run_id = str(uuid.uuid4())\n",
    "    timestamp = datetime.now()\n",
    "    result_path = local_storage.save_results(\n",
    "        datasets=executed_datasets,\n",
    "        run_id=run_id,\n",
    "        timestamp=timestamp,\n",
    "    )\n",
    "\n",
    "    # 4. Print overall summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    total_batches = sum(s[\"total_batches\"] for s in all_summaries)\n",
    "    total_successes = sum(s[\"successes\"] for s in all_summaries)\n",
    "    total_failures = sum(s[\"failures\"] for s in all_summaries)\n",
    "    print(f\"Total datasets: {len(datasets)}\")\n",
    "    print(f\"Total test cases: {total_batches}\")\n",
    "    print(f\"Successes: {total_successes}\")\n",
    "    print(f\"Failures: {total_failures}\")\n",
    "    print(f\"Success rate: {(total_successes/total_batches*100):.1f}%\")\n",
    "    print(f\"Results saved to: {result_path}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# Execute (uncomment to run complete pipeline)\n",
    "await complete_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-runner",
   "metadata": {},
   "source": [
    "## Creating Custom Runners\n",
    "\n",
    "You can create custom runner implementations by extending `BaseRunner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-runner-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from fair_forge.schemas.runner import BaseRunner\n",
    "\n",
    "\n",
    "class MockRunner(BaseRunner):\n",
    "    \"\"\"Example mock runner for testing.\"\"\"\n",
    "\n",
    "    async def run_batch(self, batch: Batch, session_id: str, **kwargs: Any) -> tuple[Batch, bool, float]:\n",
    "        \"\"\"Return a mock response immediately.\"\"\"\n",
    "        import time\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Mock response\n",
    "        mock_response = f\"Mock response for: {batch.query}\"\n",
    "        updated_batch = batch.model_copy(update={\"assistant\": mock_response})\n",
    "\n",
    "        exec_time = (time.time() - start) * 1000\n",
    "        return updated_batch, True, exec_time\n",
    "\n",
    "    async def run_dataset(self, dataset: Dataset, **kwargs: Any) -> tuple[Dataset, dict[str, Any]]:\n",
    "        \"\"\"Run all batches in dataset.\"\"\"\n",
    "        import time\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        updated_batches = []\n",
    "        for batch in dataset.conversation:\n",
    "            updated_batch, _, _ = await self.run_batch(batch, dataset.session_id)\n",
    "            updated_batches.append(updated_batch)\n",
    "\n",
    "        updated_dataset = dataset.model_copy(update={\"conversation\": updated_batches})\n",
    "\n",
    "        summary = {\n",
    "            \"session_id\": dataset.session_id,\n",
    "            \"total_batches\": len(dataset.conversation),\n",
    "            \"successes\": len(dataset.conversation),\n",
    "            \"failures\": 0,\n",
    "            \"total_execution_time_ms\": (time.time() - start) * 1000,\n",
    "            \"avg_batch_time_ms\": 1.0,\n",
    "        }\n",
    "\n",
    "        return updated_dataset, summary\n",
    "\n",
    "\n",
    "# Test mock runner\n",
    "mock_runner = MockRunner()\n",
    "print(\"Custom MockRunner created successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
