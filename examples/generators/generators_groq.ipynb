{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Fair Forge Generators - Groq Example\n",
    "\n",
    "This notebook demonstrates how to use the Fair Forge generators module with **Groq Cloud** for ultra-fast synthetic test dataset generation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `GroqGenerator` uses LangChain to interact with Groq's inference API, which provides extremely fast inference for open-source LLMs.\n",
    "\n",
    "### Why Groq?\n",
    "- **Speed**: Up to 10x faster than traditional cloud providers\n",
    "- **Cost**: Competitive pricing for high-volume usage\n",
    "- **Models**: Access to popular open-source models (Llama 3, OSS GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. Get your free API key from [Groq Console](https://console.groq.com/)\n",
    "\n",
    "2. Set your Groq API key as an environment variable:\n",
    "\n",
    "```bash\n",
    "export GROQ_API_KEY=\"your-api-key\"\n",
    "```\n",
    "\n",
    "Or create a `.env` file:\n",
    "```.env\n",
    "GROQ_API_KEY=your-api-key\n",
    "```\n",
    "\n",
    "3. Install required dependencies:\n",
    "```bash\n",
    "uv venv\n",
    "source .venv/bin/activate\n",
    "uv pip install \".[generators-groq]\" python-dotenv\n",
    "uv run jupyter lab\n",
    "```\n",
    "\n",
    "**Note:** Use `.[generators-groq]` to get the correct LangChain dependencies. The base `.[generators]` group does not include `langchain-groq`.\n",
    "\n",
    "If you're already in Jupyter and install packages, **restart the kernel** for changes to take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from fair_forge.generators import (\n",
    "    create_groq_generator,\n",
    "    create_markdown_loader,\n",
    "    GroqGenerator,\n",
    ")\n",
    "from fair_forge.schemas import Dataset, Batch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-content-header",
   "metadata": {},
   "source": [
    "## Create Sample Content\n",
    "\n",
    "Let's create a sample markdown document for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sample-content",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample content saved to: ml_fundamentals.md\n"
     ]
    }
   ],
   "source": [
    "sample_content = \"\"\"# Machine Learning Fundamentals\n",
    "\n",
    "This guide covers the basics of machine learning for beginners.\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "Machine learning can be categorized into three main types:\n",
    "\n",
    "### Supervised Learning\n",
    "- Uses labeled training data\n",
    "- Predicts outcomes based on input features\n",
    "- Examples: Classification, Regression\n",
    "\n",
    "### Unsupervised Learning\n",
    "- Works with unlabeled data\n",
    "- Discovers hidden patterns and structures\n",
    "- Examples: Clustering, Dimensionality Reduction\n",
    "\n",
    "### Reinforcement Learning\n",
    "- Agent learns through interaction with environment\n",
    "- Maximizes cumulative reward\n",
    "- Examples: Game playing, Robotics\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "Key metrics for evaluating ML models:\n",
    "\n",
    "- **Accuracy**: Proportion of correct predictions\n",
    "- **Precision**: True positives among predicted positives\n",
    "- **Recall**: True positives among actual positives\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Split data into train/validation/test sets\n",
    "2. Use cross-validation for robust evaluation\n",
    "3. Monitor for overfitting\n",
    "4. Document your experiments\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "sample_file = Path(\"./ml_fundamentals.md\")\n",
    "sample_file.write_text(sample_content)\n",
    "print(f\"Sample content saved to: {sample_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loader-header",
   "metadata": {},
   "source": [
    "## Create Context Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "loader",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 10:33:46.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators\u001b[0m:\u001b[36mcreate_markdown_loader\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mCreating local markdown loader\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:46.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m138\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:46.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mCreated 7 chunks from ml_fundamentals.md\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 7 chunks:\n",
      "\n",
      "- machine_learning_fundamentals: 63 chars\n",
      "- types_of_machine_learning: 58 chars\n",
      "- supervised_learning: 111 chars\n",
      "- unsupervised_learning: 119 chars\n",
      "- reinforcement_learning: 116 chars\n",
      "- model_evaluation: 252 chars\n",
      "- best_practices: 147 chars\n"
     ]
    }
   ],
   "source": [
    "# Create markdown loader\n",
    "loader = create_markdown_loader(\n",
    "    max_chunk_size=2000,\n",
    "    header_levels=[1, 2, 3],\n",
    ")\n",
    "\n",
    "# Preview chunks\n",
    "chunks = loader.load(str(sample_file))\n",
    "print(f\"Created {len(chunks)} chunks:\\n\")\n",
    "for chunk in chunks:\n",
    "    print(f\"- {chunk.chunk_id}: {len(chunk.content)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-header",
   "metadata": {},
   "source": [
    "## Create Groq Generator\n",
    "\n",
    "The generator reads the API key from the `GROQ_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 10:33:47.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators\u001b[0m:\u001b[36mcreate_groq_generator\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mCreating Groq generator with model: llama-3.1-8b-instant\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:47.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.groq_generator\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mInitializing Groq generator with model: llama-3.1-8b-instant\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq generator created with model: llama-3.1-8b-instant\n"
     ]
    }
   ],
   "source": [
    "# Create Groq generator\n",
    "generator = create_groq_generator(\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.4,\n",
    "    max_tokens=2048,\n",
    "    use_structured_output=True,\n",
    ")\n",
    "\n",
    "print(f\"Groq generator created with model: {generator.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-header",
   "metadata": {},
   "source": [
    "## Generate Test Dataset\n",
    "\n",
    "Groq's fast inference makes generation very quick!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 10:33:47.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m196\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:47.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m138\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:47.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mCreated 7 chunks from ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:47.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:47.425\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk machine_learning_fundamentals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test dataset with Groq...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 10:33:48,076 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:48.100\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:48.100\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk types_of_machine_learning\u001b[0m\n",
      "2026-01-14 10:33:48,715 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:48.718\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 1 queries for chunk types_of_machine_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:48.719\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk supervised_learning\u001b[0m\n",
      "2026-01-14 10:33:49,537 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:49.541\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk supervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:49.542\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk unsupervised_learning\u001b[0m\n",
      "2026-01-14 10:33:50,151 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:50.155\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk unsupervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:50.156\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk reinforcement_learning\u001b[0m\n",
      "2026-01-14 10:33:50,765 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:50.769\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk reinforcement_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:50.770\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk model_evaluation\u001b[0m\n",
      "2026-01-14 10:33:51,277 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:51.281\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:51.282\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk best_practices\u001b[0m\n",
      "2026-01-14 10:33:51,892 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:51.897\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk best_practices\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:51.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1mGenerated 19 total queries across 7 chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset in 4.48 seconds:\n",
      "  Session ID: 8b5e0dc1-5e50-4676-bf0a-f3d53b0552fd\n",
      "  Total queries: 19\n",
      "\n",
      "Generated queries:\n",
      "  [machine_learning_fundamentals_q1] (medium/factual)\n",
      "    What are the fundamental concepts of machine learning?\n",
      "\n",
      "  [machine_learning_fundamentals_q2] (hard/application)\n",
      "    How can you apply machine learning to real-world problems?\n",
      "\n",
      "  [machine_learning_fundamentals_q3] (medium/comparative)\n",
      "    What are the differences between supervised and unsupervised learning?\n",
      "\n",
      "  [types_of_machine_learning_q1] (easy/factual)\n",
      "    What are the three main categories of machine learning?\n",
      "\n",
      "  [supervised_learning_q1] (medium/factual)\n",
      "    What type of problems does the model predict outcomes for?\n",
      "\n",
      "  [supervised_learning_q2] (hard/inferential)\n",
      "    How does the model use labeled training data to make predictions?\n",
      "\n",
      "  [supervised_learning_q3] (easy/application)\n",
      "    Can you give an example of a problem type where the model's prediction is useful?\n",
      "\n",
      "  [unsupervised_learning_q1] (medium/factual)\n",
      "    What are some examples of techniques used to discover hidden patterns in unlabeled data?\n",
      "\n",
      "  [unsupervised_learning_q2] (hard/comparative)\n",
      "    How does clustering differ from dimensionality reduction in the context of unlabeled data?\n",
      "\n",
      "  [unsupervised_learning_q3] (easy/application)\n",
      "    Can you describe a scenario where discovering hidden patterns in unlabeled data would be particularly useful?\n",
      "\n",
      "  [reinforcement_learning_q1] (medium/factual)\n",
      "    How does the agent learn through interaction with the environment?\n",
      "\n",
      "  [reinforcement_learning_q2] (easy/factual)\n",
      "    What are some examples of environments where an agent can maximize cumulative reward?\n",
      "\n",
      "  [reinforcement_learning_q3] (hard/inferential)\n",
      "    Can you describe how the agent's ability to maximize cumulative reward relates to its learning process?\n",
      "\n",
      "  [model_evaluation_q1] (medium/factual)\n",
      "    What are the four key metrics used to evaluate ML model performance?\n",
      "\n",
      "  [model_evaluation_q2] (hard/inferential)\n",
      "    How do accuracy and precision relate to each other in the context of ML model evaluation?\n",
      "\n",
      "  [model_evaluation_q3] (medium/comparative)\n",
      "    Compare and contrast the F1 score and recall metrics in terms of their calculation and application.\n",
      "\n",
      "  [best_practices_q1] (medium/factual)\n",
      "    What are the primary methods for splitting data in machine learning?\n",
      "\n",
      "  [best_practices_q2] (hard/inferential)\n",
      "    How can cross-validation help prevent overfitting in machine learning models?\n",
      "\n",
      "  [best_practices_q3] (easy/application)\n",
      "    What are the key considerations when documenting experiments in machine learning?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "async def generate_dataset():\n",
    "    print(\"Generating test dataset with Groq...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    dataset = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=3,\n",
    "        language=\"english\",\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"Generated dataset in {elapsed:.2f} seconds:\")\n",
    "    print(f\"  Session ID: {dataset.session_id}\")\n",
    "    print(f\"  Total queries: {len(dataset.conversation)}\\n\")\n",
    "    \n",
    "    print(\"Generated queries:\")\n",
    "    for batch in dataset.conversation:\n",
    "        difficulty = batch.agentic.get('difficulty', 'N/A')\n",
    "        query_type = batch.agentic.get('query_type', 'N/A')\n",
    "        print(f\"  [{batch.qa_id}] ({difficulty}/{query_type})\")\n",
    "        print(f\"    {batch.query}\\n\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Execute\n",
    "dataset = await generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seed-examples-header",
   "metadata": {},
   "source": [
    "## Generate with Seed Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seed-examples",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 10:33:51.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m196\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:51.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m138\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:51.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mCreated 7 chunks from ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:51.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:51.913\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk machine_learning_fundamentals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with seed examples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 10:33:52,813 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:52.818\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:52.821\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk types_of_machine_learning\u001b[0m\n",
      "2026-01-14 10:33:53,429 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:53.433\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 1 queries for chunk types_of_machine_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:53.434\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk supervised_learning\u001b[0m\n",
      "2026-01-14 10:33:53,940 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:53.944\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk supervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:53.945\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk unsupervised_learning\u001b[0m\n",
      "2026-01-14 10:33:54,555 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:54.557\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk unsupervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:54.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk reinforcement_learning\u001b[0m\n",
      "2026-01-14 10:33:55,066 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:55.070\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk reinforcement_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:55.071\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk model_evaluation\u001b[0m\n",
      "2026-01-14 10:33:55,681 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:55.684\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 1 queries for chunk model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:55.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk best_practices\u001b[0m\n",
      "2026-01-14 10:33:56,193 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:56.196\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk best_practices\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:56.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1mGenerated 12 total queries across 7 chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 12 queries:\n",
      "  - What are the fundamental concepts of machine learning that a beginner should understand?\n",
      "  - How can a machine learning model be designed to handle imbalanced datasets?\n",
      "  - What are the three main types of machine learning?\n",
      "  - What are the key differences between supervised and unsupervised learning?\n",
      "  - How can you balance the trade-off between precision and recall in a machine learning model?\n"
     ]
    }
   ],
   "source": [
    "async def generate_with_seeds():\n",
    "    seed_examples = [\n",
    "        \"What is the difference between supervised and unsupervised learning?\",\n",
    "        \"How do you prevent overfitting in a machine learning model?\",\n",
    "        \"When should you use precision vs recall as your primary metric?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"Generating with seed examples...\\n\")\n",
    "    \n",
    "    dataset = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=2,\n",
    "        seed_examples=seed_examples,\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(dataset.conversation)} queries:\")\n",
    "    for batch in dataset.conversation[:5]:\n",
    "        print(f\"  - {batch.query}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Execute\n",
    "dataset_with_seeds = await generate_with_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to: generated_tests_groq.json\n"
     ]
    }
   ],
   "source": [
    "# Save dataset to JSON\n",
    "output_file = Path(\"./generated_tests_groq.json\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(dataset.model_dump(), f, indent=2)\n",
    "\n",
    "print(f\"Dataset saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## Available Groq Models\n",
    "\n",
    "Check them [here](https://console.groq.com/docs/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## Speed Comparison\n",
    "\n",
    "Groq is known for its extremely fast inference. Here's a quick benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 10:33:56.617\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk machine_learning_fundamentals\u001b[0m\n",
      "2026-01-14 10:33:57,217 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:57.221\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:57.223\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk machine_learning_fundamentals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: 0.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 10:33:57,831 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:57.834\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-14 10:33:57.835\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m133\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk machine_learning_fundamentals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2: 0.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 10:33:58,343 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 10:33:58.348\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m164\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk machine_learning_fundamentals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3: 0.51s\n",
      "\n",
      "Average: 0.58s per chunk (3 queries)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "async def benchmark_generation():\n",
    "    \"\"\"Benchmark generation speed.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        start = time.time()\n",
    "        await generator.generate_queries(\n",
    "            chunk=chunks[0],\n",
    "            num_queries=3,\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed)\n",
    "        print(f\"Run {i+1}: {elapsed:.2f}s\")\n",
    "    \n",
    "    avg = sum(times) / len(times)\n",
    "    print(f\"\\nAverage: {avg:.2f}s per chunk (3 queries)\")\n",
    "\n",
    "# Execute\n",
    "await benchmark_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "# Clean up sample files\n",
    "if sample_file.exists():\n",
    "    sample_file.unlink()\n",
    "if output_file.exists():\n",
    "    output_file.unlink()\n",
    "print(\"Cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
