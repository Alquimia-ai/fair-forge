{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "introduction",
      "metadata": {},
      "source": [
        "# Fair Forge Generators - Groq Example\n",
        "\n",
        "This notebook demonstrates how to use the Fair Forge generators module with **Groq Cloud** for ultra-fast synthetic test dataset generation.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The `GroqGenerator` uses LangChain to interact with Groq's inference API, which provides extremely fast inference for models like Llama, Mixtral, and Gemma.\n",
        "\n",
        "### Why Groq?\n",
        "- **Speed**: Up to 10x faster than traditional cloud providers\n",
        "- **Cost**: Competitive pricing for high-volume usage\n",
        "- **Models**: Access to popular open-source models (Llama 3, Mixtral)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "1. Get your free API key from [Groq Console](https://console.groq.com/)\n",
        "\n",
        "2. Set your Groq API key as an environment variable:\n",
        "\n",
        "```bash\n",
        "export GROQ_API_KEY=\"your-api-key\"\n",
        "```\n",
        "\n",
        "Or create a `.env` file:\n",
        "```.env\n",
        "GROQ_API_KEY=your-api-key\n",
        "```\n",
        "\n",
        "3. Install required dependencies:\n",
        "```bash\n",
        "uv pip install .[generators]\n",
        "uv pip install langchain-groq python-dotenv\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports-header",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from fair_forge.generators import (\n",
        "    create_groq_generator,\n",
        "    create_markdown_loader,\n",
        "    GroqGenerator,\n",
        ")\n",
        "from fair_forge.schemas import Dataset, Batch\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"Imports loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sample-content-header",
      "metadata": {},
      "source": [
        "## Create Sample Content\n",
        "\n",
        "Let's create a sample markdown document for testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sample-content",
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_content = \"\"\"# Machine Learning Fundamentals\n",
        "\n",
        "This guide covers the basics of machine learning for beginners.\n",
        "\n",
        "## Types of Machine Learning\n",
        "\n",
        "Machine learning can be categorized into three main types:\n",
        "\n",
        "### Supervised Learning\n",
        "- Uses labeled training data\n",
        "- Predicts outcomes based on input features\n",
        "- Examples: Classification, Regression\n",
        "\n",
        "### Unsupervised Learning\n",
        "- Works with unlabeled data\n",
        "- Discovers hidden patterns and structures\n",
        "- Examples: Clustering, Dimensionality Reduction\n",
        "\n",
        "### Reinforcement Learning\n",
        "- Agent learns through interaction with environment\n",
        "- Maximizes cumulative reward\n",
        "- Examples: Game playing, Robotics\n",
        "\n",
        "## Model Evaluation\n",
        "\n",
        "Key metrics for evaluating ML models:\n",
        "\n",
        "- **Accuracy**: Proportion of correct predictions\n",
        "- **Precision**: True positives among predicted positives\n",
        "- **Recall**: True positives among actual positives\n",
        "- **F1 Score**: Harmonic mean of precision and recall\n",
        "\n",
        "## Best Practices\n",
        "\n",
        "1. Split data into train/validation/test sets\n",
        "2. Use cross-validation for robust evaluation\n",
        "3. Monitor for overfitting\n",
        "4. Document your experiments\n",
        "\"\"\"\n",
        "\n",
        "# Save to file\n",
        "sample_file = Path(\"./ml_fundamentals.md\")\n",
        "sample_file.write_text(sample_content)\n",
        "print(f\"Sample content saved to: {sample_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "loader-header",
      "metadata": {},
      "source": [
        "## Create Context Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "loader",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create markdown loader\n",
        "loader = create_markdown_loader(\n",
        "    max_chunk_size=2000,\n",
        "    header_levels=[1, 2, 3],\n",
        ")\n",
        "\n",
        "# Preview chunks\n",
        "chunks = loader.load(str(sample_file))\n",
        "print(f\"Created {len(chunks)} chunks:\\n\")\n",
        "for chunk in chunks:\n",
        "    print(f\"- {chunk.chunk_id}: {len(chunk.content)} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "generator-header",
      "metadata": {},
      "source": [
        "## Create Groq Generator\n",
        "\n",
        "The generator reads the API key from the `GROQ_API_KEY` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "generator",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Groq generator with Llama 3.1 70B (recommended for quality)\n",
        "generator = create_groq_generator(\n",
        "    model_name=\"llama-3.1-70b-versatile\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=2048,\n",
        "    use_structured_output=True,\n",
        ")\n",
        "\n",
        "print(f\"Groq generator created with model: {generator.model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "generate-header",
      "metadata": {},
      "source": [
        "## Generate Test Dataset\n",
        "\n",
        "Groq's fast inference makes generation very quick!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "generate",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "async def generate_dataset():\n",
        "    print(\"Generating test dataset with Groq...\\n\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    dataset = await generator.generate_dataset(\n",
        "        context_loader=loader,\n",
        "        source=str(sample_file),\n",
        "        assistant_id=\"ml-assistant\",\n",
        "        num_queries_per_chunk=3,\n",
        "        language=\"english\",\n",
        "    )\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"Generated dataset in {elapsed:.2f} seconds:\")\n",
        "    print(f\"  Session ID: {dataset.session_id}\")\n",
        "    print(f\"  Total queries: {len(dataset.conversation)}\\n\")\n",
        "    \n",
        "    print(\"Generated queries:\")\n",
        "    for batch in dataset.conversation:\n",
        "        difficulty = batch.agentic.get('difficulty', 'N/A')\n",
        "        query_type = batch.agentic.get('query_type', 'N/A')\n",
        "        print(f\"  [{batch.qa_id}] ({difficulty}/{query_type})\")\n",
        "        print(f\"    {batch.query}\\n\")\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# Execute\n",
        "dataset = await generate_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seed-examples-header",
      "metadata": {},
      "source": [
        "## Generate with Seed Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seed-examples",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def generate_with_seeds():\n",
        "    seed_examples = [\n",
        "        \"What is the difference between supervised and unsupervised learning?\",\n",
        "        \"How do you prevent overfitting in a machine learning model?\",\n",
        "        \"When should you use precision vs recall as your primary metric?\",\n",
        "    ]\n",
        "    \n",
        "    print(\"Generating with seed examples...\\n\")\n",
        "    \n",
        "    dataset = await generator.generate_dataset(\n",
        "        context_loader=loader,\n",
        "        source=str(sample_file),\n",
        "        assistant_id=\"ml-assistant\",\n",
        "        num_queries_per_chunk=2,\n",
        "        seed_examples=seed_examples,\n",
        "    )\n",
        "    \n",
        "    print(f\"Generated {len(dataset.conversation)} queries:\")\n",
        "    for batch in dataset.conversation[:5]:\n",
        "        print(f\"  - {batch.query}\")\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# Execute\n",
        "dataset_with_seeds = await generate_with_seeds()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-header",
      "metadata": {},
      "source": [
        "## Save Generated Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save dataset to JSON\n",
        "output_file = Path(\"./generated_tests_groq.json\")\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(dataset.model_dump(), f, indent=2)\n",
        "\n",
        "print(f\"Dataset saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "models-header",
      "metadata": {},
      "source": [
        "## Available Groq Models\n",
        "\n",
        "| Model | Context | Best For |\n",
        "|-------|---------|----------|\n",
        "| `llama-3.1-70b-versatile` | 128K | High quality, complex tasks |\n",
        "| `llama-3.1-8b-instant` | 128K | Fast, simple tasks |\n",
        "| `llama3-groq-70b-8192-tool-use-preview` | 8K | Tool use |\n",
        "| `mixtral-8x7b-32768` | 32K | Balanced performance |\n",
        "| `gemma2-9b-it` | 8K | Compact, efficient |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "try-different-model",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Use Llama 3.1 8B for faster generation\n",
        "# generator_fast = create_groq_generator(\n",
        "#     model_name=\"llama-3.1-8b-instant\",\n",
        "#     temperature=0.7,\n",
        "# )\n",
        "\n",
        "# Example: Use Mixtral for longer context\n",
        "# generator_mixtral = create_groq_generator(\n",
        "#     model_name=\"mixtral-8x7b-32768\",\n",
        "#     temperature=0.5,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison-header",
      "metadata": {},
      "source": [
        "## Speed Comparison\n",
        "\n",
        "Groq is known for its extremely fast inference. Here's a quick benchmark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "benchmark",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "async def benchmark_generation():\n",
        "    \"\"\"Benchmark generation speed.\"\"\"\n",
        "    times = []\n",
        "    \n",
        "    for i in range(3):\n",
        "        start = time.time()\n",
        "        await generator.generate_queries(\n",
        "            chunk=chunks[0],\n",
        "            num_queries=3,\n",
        "        )\n",
        "        elapsed = time.time() - start\n",
        "        times.append(elapsed)\n",
        "        print(f\"Run {i+1}: {elapsed:.2f}s\")\n",
        "    \n",
        "    avg = sum(times) / len(times)\n",
        "    print(f\"\\nAverage: {avg:.2f}s per chunk (3 queries)\")\n",
        "\n",
        "# Execute\n",
        "await benchmark_generation()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup-header",
      "metadata": {},
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleanup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up sample files\n",
        "if sample_file.exists():\n",
        "    sample_file.unlink()\n",
        "if output_file.exists():\n",
        "    output_file.unlink()\n",
        "print(\"Cleanup completed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
