{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Fair Forge Generators - Groq Example\n",
    "\n",
    "This notebook demonstrates how to use the Fair Forge generators module with **Groq Cloud** for ultra-fast synthetic test dataset generation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `GroqGenerator` uses LangChain to interact with Groq's inference API, which provides extremely fast inference for open-source LLMs.\n",
    "\n",
    "### Why Groq?\n",
    "- **Speed**: Up to 10x faster than traditional cloud providers\n",
    "- **Cost**: Competitive pricing for high-volume usage\n",
    "- **Models**: Access to popular open-source models (Llama 3, OSS GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. Get your free API key from [Groq Console](https://console.groq.com/)\n",
    "\n",
    "2. Set your Groq API key as an environment variable:\n",
    "\n",
    "```bash\n",
    "export GROQ_API_KEY=\"your-api-key\"\n",
    "```\n",
    "\n",
    "Or create a `.env` file:\n",
    "```.env\n",
    "GROQ_API_KEY=your-api-key\n",
    "```\n",
    "\n",
    "3. Install required dependencies:\n",
    "```bash\n",
    "uv venv\n",
    "source .venv/bin/activate\n",
    "uv pip install \".[generators-groq]\" python-dotenv\n",
    "uv run jupyter lab\n",
    "```\n",
    "\n",
    "**Note:** Use `.[generators-groq]` to get the correct LangChain dependencies. The base `.[generators]` group does not include `langchain-groq`.\n",
    "\n",
    "If you're already in Jupyter and install packages, **restart the kernel** for changes to take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from fair_forge.generators import (\n",
    "    create_groq_generator,\n",
    "    create_markdown_loader,\n",
    "    GroqGenerator,\n",
    "    # Strategies for chunk selection\n",
    "    SequentialStrategy,\n",
    "    RandomSamplingStrategy,\n",
    ")\n",
    "from fair_forge.schemas import Dataset, Batch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-content-header",
   "metadata": {},
   "source": [
    "## Create Sample Content\n",
    "\n",
    "Let's create a sample markdown document for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sample-content",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample content saved to: ml_fundamentals.md\n"
     ]
    }
   ],
   "source": [
    "sample_content = \"\"\"# Machine Learning Fundamentals\n",
    "\n",
    "This guide covers the basics of machine learning for beginners.\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "Machine learning can be categorized into three main types:\n",
    "\n",
    "### Supervised Learning\n",
    "- Uses labeled training data\n",
    "- Predicts outcomes based on input features\n",
    "- Examples: Classification, Regression\n",
    "\n",
    "### Unsupervised Learning\n",
    "- Works with unlabeled data\n",
    "- Discovers hidden patterns and structures\n",
    "- Examples: Clustering, Dimensionality Reduction\n",
    "\n",
    "### Reinforcement Learning\n",
    "- Agent learns through interaction with environment\n",
    "- Maximizes cumulative reward\n",
    "- Examples: Game playing, Robotics\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "Key metrics for evaluating ML models:\n",
    "\n",
    "- **Accuracy**: Proportion of correct predictions\n",
    "- **Precision**: True positives among predicted positives\n",
    "- **Recall**: True positives among actual positives\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Split data into train/validation/test sets\n",
    "2. Use cross-validation for robust evaluation\n",
    "3. Monitor for overfitting\n",
    "4. Document your experiments\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "sample_file = Path(\"./ml_fundamentals.md\")\n",
    "sample_file.write_text(sample_content)\n",
    "print(f\"Sample content saved to: {sample_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loader-header",
   "metadata": {},
   "source": [
    "## Create Context Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "loader",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 13:16:35.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators\u001b[0m:\u001b[36mcreate_markdown_loader\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mCreating local markdown loader\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:35.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:35.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:35.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 7 chunks:\n",
      "\n",
      "- ml_fundamentals_machine_learning_fundamentals: 63 chars\n",
      "- ml_fundamentals_types_of_machine_learning: 58 chars\n",
      "- ml_fundamentals_supervised_learning: 111 chars\n",
      "- ml_fundamentals_unsupervised_learning: 119 chars\n",
      "- ml_fundamentals_reinforcement_learning: 116 chars\n",
      "- ml_fundamentals_model_evaluation: 252 chars\n",
      "- ml_fundamentals_best_practices: 147 chars\n"
     ]
    }
   ],
   "source": [
    "# Create markdown loader\n",
    "loader = create_markdown_loader(\n",
    "    max_chunk_size=2000,\n",
    "    header_levels=[1, 2, 3],\n",
    ")\n",
    "\n",
    "# Preview chunks\n",
    "chunks = loader.load(str(sample_file))\n",
    "print(f\"Created {len(chunks)} chunks:\\n\")\n",
    "for chunk in chunks:\n",
    "    print(f\"- {chunk.chunk_id}: {len(chunk.content)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-header",
   "metadata": {},
   "source": [
    "## Create Groq Generator\n",
    "\n",
    "The generator reads the API key from the `GROQ_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 13:16:35.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators\u001b[0m:\u001b[36mcreate_groq_generator\u001b[0m:\u001b[36m130\u001b[0m - \u001b[1mCreating Groq generator with model: llama-3.1-8b-instant\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:35.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.groq_generator\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mInitializing Groq generator with model: llama-3.1-8b-instant\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq generator created with model: llama-3.1-8b-instant\n"
     ]
    }
   ],
   "source": [
    "# Create Groq generator\n",
    "generator = create_groq_generator(\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.4,\n",
    "    max_tokens=2048,\n",
    "    use_structured_output=True,\n",
    ")\n",
    "\n",
    "print(f\"Groq generator created with model: {generator.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-header",
   "metadata": {},
   "source": [
    "## Generate Test Dataset\n",
    "\n",
    "Groq's fast inference makes generation very quick!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 13:16:36.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:36.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:36.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:36.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:36.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:36.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mUsing chunk selection strategy: SequentialStrategy()\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:36.045\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test dataset with Groq...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 13:16:37,145 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:37.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:37.179\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "2026-01-14 13:16:37,753 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:37.757\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:37.758\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "2026-01-14 13:16:38,471 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:38.476\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:38.479\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "2026-01-14 13:16:39,085 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:39.090\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:39.091\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "2026-01-14 13:16:39,801 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:39.805\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:39.806\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "2026-01-14 13:16:40,417 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:40.420\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:40.422\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "2026-01-14 13:16:40,929 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:40.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:40.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m414\u001b[0m - \u001b[1mGenerated 21 batches for chunk group (7 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:40.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m428\u001b[0m - \u001b[1mGenerated 1 dataset(s) with 21 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 dataset(s) in 4.90 seconds:\n",
      "  Session ID: 4a30bc8f-fe9c-4977-b6b4-7d1e5afe5612\n",
      "  Total queries: 21\n",
      "\n",
      "Generated queries:\n",
      "  [ml_fundamentals_machine_learning_fundamentals_q1] (medium/factual)\n",
      "    What are the fundamental concepts of machine learning?\n",
      "\n",
      "  [ml_fundamentals_machine_learning_fundamentals_q2] (hard/application)\n",
      "    How can machine learning be applied in real-world scenarios?\n",
      "\n",
      "  [ml_fundamentals_machine_learning_fundamentals_q3] (medium/comparative)\n",
      "    What are the key differences between supervised and unsupervised learning?\n",
      "\n",
      "  [ml_fundamentals_types_of_machine_learning_q1] (easy/factual)\n",
      "    What are the three main types of machine learning?\n",
      "\n",
      "  [ml_fundamentals_types_of_machine_learning_q2] (medium/inferential)\n",
      "    How does the categorization of machine learning into three types impact its applications?\n",
      "\n",
      "  [ml_fundamentals_types_of_machine_learning_q3] (hard/comparative)\n",
      "    Compare and contrast supervised and unsupervised learning.\n",
      "\n",
      "  [ml_fundamentals_supervised_learning_q1] (medium/factual)\n",
      "    What type of tasks does this model typically handle?\n",
      "\n",
      "  [ml_fundamentals_supervised_learning_q2] (hard/inferential)\n",
      "    How does the model use labeled training data to improve its predictions?\n",
      "\n",
      "  [ml_fundamentals_supervised_learning_q3] (easy/application)\n",
      "    Can you give an example of a classification task that this model might be used for?\n",
      "\n",
      "  [ml_fundamentals_unsupervised_learning_q1] (medium/factual)\n",
      "    What are some common applications of clustering in data analysis?\n",
      "\n",
      "  [ml_fundamentals_unsupervised_learning_q2] (hard/inferential)\n",
      "    How does dimensionality reduction help in identifying hidden patterns in unlabeled data?\n",
      "\n",
      "  [ml_fundamentals_unsupervised_learning_q3] (hard/comparative)\n",
      "    Compare the effectiveness of clustering and dimensionality reduction in revealing underlying structures in data.\n",
      "\n",
      "  [ml_fundamentals_reinforcement_learning_q1] (medium/factual)\n",
      "    How does an agent learn through interaction with its environment?\n",
      "\n",
      "  [ml_fundamentals_reinforcement_learning_q2] (hard/application)\n",
      "    Explain how an agent maximizes cumulative reward in a game playing scenario.\n",
      "\n",
      "  [ml_fundamentals_reinforcement_learning_q3] (hard/comparative)\n",
      "    Compare the learning process of an agent in a robotics environment with one in a game playing environment.\n",
      "\n",
      "  [ml_fundamentals_model_evaluation_q1] (medium/factual)\n",
      "    What are the four key metrics used to evaluate ML model performance?\n",
      "\n",
      "  [ml_fundamentals_model_evaluation_q2] (hard/inferential)\n",
      "    How do accuracy and precision relate to each other in the context of ML model evaluation?\n",
      "\n",
      "  [ml_fundamentals_model_evaluation_q3] (hard/comparative)\n",
      "    Compare and contrast the F1 score and recall in terms of their impact on ML model evaluation.\n",
      "\n",
      "  [ml_fundamentals_best_practices_q1] (medium/factual)\n",
      "    How do you split data for robust evaluation?\n",
      "\n",
      "  [ml_fundamentals_best_practices_q2] (hard/factual)\n",
      "    What is the purpose of cross-validation in machine learning?\n",
      "\n",
      "  [ml_fundamentals_best_practices_q3] (medium/inferential)\n",
      "    What are potential risks of overfitting in machine learning models?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "async def generate_dataset():\n",
    "    print(\"Generating test dataset with Groq...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # generate_dataset now returns list[Dataset]\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=3,\n",
    "        language=\"english\",\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # With default SequentialStrategy, we get one dataset\n",
    "    dataset = datasets[0]\n",
    "    \n",
    "    print(f\"Generated {len(datasets)} dataset(s) in {elapsed:.2f} seconds:\")\n",
    "    print(f\"  Session ID: {dataset.session_id}\")\n",
    "    print(f\"  Total queries: {len(dataset.conversation)}\\n\")\n",
    "    \n",
    "    print(\"Generated queries:\")\n",
    "    for batch in dataset.conversation:\n",
    "        difficulty = batch.agentic.get('difficulty', 'N/A')\n",
    "        query_type = batch.agentic.get('query_type', 'N/A')\n",
    "        print(f\"  [{batch.qa_id}] ({difficulty}/{query_type})\")\n",
    "        print(f\"    {batch.query}\\n\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute\n",
    "datasets = await generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eacdeb-cda1-4c08-947e-5d717376aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to JSON\n",
    "dataset = datasets[0]  # Get the first (and only) dataset\n",
    "output_file = Path(\"./generated_tests_groq.json\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(dataset.model_dump(), f, indent=2)\n",
    "\n",
    "print(f\"Dataset saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seed-examples-header",
   "metadata": {},
   "source": [
    "## Generate with Seed Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seed-examples",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 13:16:40.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:40.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:40.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:40.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:40.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:40.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mUsing chunk selection strategy: SequentialStrategy()\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:40.954\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with seed examples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 13:16:41,441 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:41.444\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:41.446\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "2026-01-14 13:16:41,952 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:41.956\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:41.957\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "2026-01-14 13:16:42,464 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:42.468\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:42.469\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "2026-01-14 13:16:42,976 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:42.980\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:42.981\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "2026-01-14 13:16:43,488 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:43.494\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:43.500\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "2026-01-14 13:16:44,074 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:44.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 1 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:44.080\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "2026-01-14 13:16:44,615 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:44.619\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:44.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m414\u001b[0m - \u001b[1mGenerated 13 batches for chunk group (7 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:44.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m428\u001b[0m - \u001b[1mGenerated 1 dataset(s) with 13 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 13 queries:\n",
      "  - What are the fundamental concepts covered in this guide on machine learning?\n",
      "  - How does the guide on machine learning distinguish between different types of learning algorithms?\n",
      "  - What are the three main types of machine learning?\n",
      "  - How does the type of machine learning impact the choice of algorithm?\n",
      "  - What are the primary applications of supervised learning?\n"
     ]
    }
   ],
   "source": [
    "async def generate_with_seeds():\n",
    "    seed_examples = [\n",
    "        \"What is the difference between supervised and unsupervised learning?\",\n",
    "        \"How do you prevent overfitting in a machine learning model?\",\n",
    "        \"When should you use precision vs recall as your primary metric?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"Generating with seed examples...\\n\")\n",
    "    \n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=2,\n",
    "        seed_examples=seed_examples,\n",
    "    )\n",
    "    \n",
    "    dataset = datasets[0]\n",
    "    print(f\"Generated {len(dataset.conversation)} queries:\")\n",
    "    for batch in dataset.conversation[:5]:\n",
    "        print(f\"  - {batch.query}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute\n",
    "datasets_with_seeds = await generate_with_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colth80fwj",
   "metadata": {},
   "source": [
    "## Chunk Selection Strategies\n",
    "\n",
    "Strategies control how chunks are selected and grouped during generation. By default, all chunks are processed sequentially into a single dataset.\n",
    "\n",
    "### RandomSamplingStrategy\n",
    "\n",
    "Randomly samples chunks multiple times to create diverse test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0mdtcv8js",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 13:16:44.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:44.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:44.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:44.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:44.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:44.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mUsing chunk selection strategy: RandomSamplingStrategy(num_samples=2, chunks_per_sample=3, seed=42)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:44.640\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: RandomSamplingStrategy(num_samples=2, chunks_per_sample=3, seed=42)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 13:16:45,126 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:45.129\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:45.130\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "2026-01-14 13:16:45,639 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:45.642\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:45.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "2026-01-14 13:16:46,151 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:46.155\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:46.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m414\u001b[0m - \u001b[1mGenerated 6 batches for chunk group (3 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:46.157\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "2026-01-14 13:16:46,726 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:46.729\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:46.731\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "2026-01-14 13:16:47,174 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:47.177\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:47.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "2026-01-14 13:16:47,692 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:47.695\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m246\u001b[0m - \u001b[34m\u001b[1mGenerated 1 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:47.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m414\u001b[0m - \u001b[1mGenerated 5 batches for chunk group (3 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:47.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m428\u001b[0m - \u001b[1mGenerated 2 dataset(s) with 11 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 datasets:\n",
      "\n",
      "Dataset 1:\n",
      "  Session: d7367f1c...\n",
      "  Queries: 6\n",
      "  Chunks: {'ml_fundamentals_best_practices', 'ml_fundamentals_machine_learning_fundamentals', 'ml_fundamentals_model_evaluation'}\n",
      "\n",
      "Dataset 2:\n",
      "  Session: 79b60cfb...\n",
      "  Queries: 5\n",
      "  Chunks: {'ml_fundamentals_types_of_machine_learning', 'ml_fundamentals_supervised_learning', 'ml_fundamentals_model_evaluation'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def generate_with_random_sampling():\n",
    "    \"\"\"Generate multiple datasets using random chunk sampling.\"\"\"\n",
    "    \n",
    "    # Create a strategy that samples 3 random chunks, 2 times\n",
    "    strategy = RandomSamplingStrategy(\n",
    "        num_samples=2,       # Create 2 datasets\n",
    "        chunks_per_sample=3, # Each with 3 random chunks\n",
    "        seed=42,             # For reproducibility\n",
    "    )\n",
    "    \n",
    "    print(f\"Strategy: {strategy}\\n\")\n",
    "    \n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=2,\n",
    "        selection_strategy=strategy,\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(datasets)} datasets:\\n\")\n",
    "    for i, ds in enumerate(datasets):\n",
    "        print(f\"Dataset {i+1}:\")\n",
    "        print(f\"  Session: {ds.session_id[:8]}...\")\n",
    "        print(f\"  Queries: {len(ds.conversation)}\")\n",
    "        # Show chunk IDs from the queries\n",
    "        chunk_ids = set(b.agentic.get('chunk_id', 'N/A') for b in ds.conversation)\n",
    "        print(f\"  Chunks: {chunk_ids}\\n\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute\n",
    "random_datasets = await generate_with_random_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9zhyn4d3nt",
   "metadata": {},
   "source": [
    "## Conversation Mode\n",
    "\n",
    "Instead of generating independent queries, conversation mode creates coherent multi-turn conversations where each question builds on the previous ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "kvqjf7dotkq",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 13:16:47.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:47.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:47.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:47.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:47.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:47.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mUsing chunk selection strategy: SequentialStrategy()\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:47.715\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m275\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating conversations (each turn builds on the previous)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 13:16:48,404 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:48.407\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:48.409\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m275\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "2026-01-14 13:16:49,121 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:49.125\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:49.126\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m275\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "2026-01-14 13:16:49,837 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:49.841\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:49.842\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m275\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "2026-01-14 13:16:50,553 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:50.557\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:50.559\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m275\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "2026-01-14 13:16:51,373 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:51.377\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:51.378\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m275\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "2026-01-14 13:16:52,074 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:52.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:52.079\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m275\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "2026-01-14 13:16:52,807 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:52.810\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:52.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m414\u001b[0m - \u001b[1mGenerated 21 batches for chunk group (7 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:52.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m428\u001b[0m - \u001b[1mGenerated 1 dataset(s) with 21 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 21 conversation turns:\n",
      "\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_machine_learning_fundamentals ---\n",
      "  Turn 1: What is machine learning?\n",
      "  Turn 2: Can you give an example of how machine learning is used?\n",
      "         (builds on: ml_fundamentals_machine_learning_fundamentals_t1)\n",
      "  Turn 3: How does machine learning differ from traditional programming?\n",
      "         (builds on: ml_fundamentals_machine_learning_fundamentals_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_types_of_machine_learning ---\n",
      "  Turn 1: What are the main types of machine learning?\n",
      "  Turn 2: Can you explain supervised learning in more detail?\n",
      "         (builds on: ml_fundamentals_types_of_machine_learning_t1)\n",
      "  Turn 3: How is supervised learning used in real-world applications?\n",
      "         (builds on: ml_fundamentals_types_of_machine_learning_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_supervised_learning ---\n",
      "  Turn 1: What is the main purpose of using labeled training data?\n",
      "  Turn 2: How does the model predict outcomes based on input features?\n",
      "         (builds on: ml_fundamentals_supervised_learning_t1)\n",
      "  Turn 3: Can you provide an example of how this works in practice, such as classification or regression?\n",
      "         (builds on: ml_fundamentals_supervised_learning_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_unsupervised_learning ---\n",
      "  Turn 1: What are some common applications of unsupervised learning?\n",
      "  Turn 2: Can you give an example of how clustering is used in real-world scenarios?\n",
      "         (builds on: ml_fundamentals_unsupervised_learning_t1)\n",
      "  Turn 3: How does dimensionality reduction help in identifying hidden patterns in data?\n",
      "         (builds on: ml_fundamentals_unsupervised_learning_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_reinforcement_learning ---\n",
      "  Turn 1: What is the main goal of an agent in an environment?\n",
      "  Turn 2: How does an agent maximize its reward in a game playing scenario?\n",
      "         (builds on: ml_fundamentals_reinforcement_learning_t1)\n",
      "  Turn 3: Can you provide an example of how an agent applies its learning to achieve a cumulative reward in robotics?\n",
      "         (builds on: ml_fundamentals_reinforcement_learning_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_model_evaluation ---\n",
      "  Turn 1: What are the key metrics used to evaluate machine learning models?\n",
      "  Turn 2: Can you explain what accuracy means in the context of ML model evaluation?\n",
      "         (builds on: ml_fundamentals_model_evaluation_t1)\n",
      "  Turn 3: How does the F1 score relate to precision and recall in ML model evaluation?\n",
      "         (builds on: ml_fundamentals_model_evaluation_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_best_practices ---\n",
      "  Turn 1: What are the key steps in preparing a dataset for machine learning?\n",
      "  Turn 2: What is cross-validation and why is it used in machine learning?\n",
      "         (builds on: ml_fundamentals_best_practices_t1)\n",
      "  Turn 3: How can I monitor for overfitting in my machine learning model?\n",
      "         (builds on: ml_fundamentals_best_practices_t2)\n"
     ]
    }
   ],
   "source": [
    "async def generate_conversations():\n",
    "    \"\"\"Generate coherent multi-turn conversations.\"\"\"\n",
    "    \n",
    "    print(\"Generating conversations (each turn builds on the previous)...\\n\")\n",
    "    \n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=3,  # 3-turn conversations\n",
    "        conversation_mode=True,   # Enable conversation mode\n",
    "    )\n",
    "    \n",
    "    dataset = datasets[0]\n",
    "    print(f\"Generated {len(dataset.conversation)} conversation turns:\\n\")\n",
    "    \n",
    "    # Group by chunk to show conversation flow\n",
    "    current_chunk = None\n",
    "    for batch in dataset.conversation:\n",
    "        chunk_id = batch.agentic.get('chunk_id', 'N/A')\n",
    "        turn_num = batch.agentic.get('turn_number', 0)\n",
    "        builds_on = batch.agentic.get('builds_on', None)\n",
    "        \n",
    "        if chunk_id != current_chunk:\n",
    "            print(f\"\\n--- Conversation for chunk: {chunk_id} ---\")\n",
    "            current_chunk = chunk_id\n",
    "        \n",
    "        print(f\"  Turn {turn_num}: {batch.query}\")\n",
    "        if builds_on:\n",
    "            print(f\"         (builds on: {builds_on})\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute\n",
    "conversation_datasets = await generate_conversations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8almrdtvdrq",
   "metadata": {},
   "source": [
    "### Combined: Random Sampling + Conversation Mode\n",
    "\n",
    "You can combine strategies with conversation mode to create diverse conversation-based test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7qpbt4t01so",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-14 13:16:52.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:52.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:52.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:52.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:52.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:52.829\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mUsing chunk selection strategy: RandomSamplingStrategy(num_samples=2, chunks_per_sample=2, seed=42)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:52.830\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m275\u001b[0m - \u001b[34m\u001b[1mGenerating 2-turn conversation for chunk ml_fundamentals_model_evaluation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2 datasets with 2 random chunks each (conversation mode)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 13:16:53,318 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:53.323\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[34m\u001b[1mGenerated 2 turns for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:53.324\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m275\u001b[0m - \u001b[34m\u001b[1mGenerating 2-turn conversation for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "2026-01-14 13:16:53,933 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:53.938\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[34m\u001b[1mGenerated 2 turns for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:53.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m414\u001b[0m - \u001b[1mGenerated 4 batches for chunk group (2 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:53.941\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m275\u001b[0m - \u001b[34m\u001b[1mGenerating 2-turn conversation for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "2026-01-14 13:16:54,651 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:54.656\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[34m\u001b[1mGenerated 2 turns for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:54.657\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m275\u001b[0m - \u001b[34m\u001b[1mGenerating 2-turn conversation for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "2026-01-14 13:16:55,264 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-14 13:16:55.268\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[34m\u001b[1mGenerated 2 turns for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:55.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m414\u001b[0m - \u001b[1mGenerated 4 batches for chunk group (2 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-14 13:16:55.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.langchain_generator\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m428\u001b[0m - \u001b[1mGenerated 2 dataset(s) with 8 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 (4 turns):\n",
      "  [ml_fundamentals] T1: What are the key metrics used to evaluate ML model...\n",
      "  [ml_fundamentals] T2: Can you explain the difference between accuracy an...\n",
      "  [ml_fundamentals] T1: What is machine learning?...\n",
      "  [ml_fundamentals] T2: Can you give an example of how machine learning is...\n",
      "\n",
      "Dataset 2 (4 turns):\n",
      "  [ml_fundamentals] T1: What is machine learning?...\n",
      "  [ml_fundamentals] T2: How does it differ from traditional programming?...\n",
      "  [ml_fundamentals] T1: What metrics are commonly used to evaluate ML mode...\n",
      "  [ml_fundamentals] T2: Can you explain the difference between precision a...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def generate_random_conversations():\n",
    "    \"\"\"Combine random sampling with conversation mode.\"\"\"\n",
    "    \n",
    "    strategy = RandomSamplingStrategy(\n",
    "        num_samples=2,\n",
    "        chunks_per_sample=2,\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    print(\"Generating 2 datasets with 2 random chunks each (conversation mode)...\\n\")\n",
    "    \n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=2,  # 2-turn conversations\n",
    "        selection_strategy=strategy,\n",
    "        conversation_mode=True,\n",
    "    )\n",
    "    \n",
    "    for i, ds in enumerate(datasets):\n",
    "        print(f\"Dataset {i+1} ({len(ds.conversation)} turns):\")\n",
    "        for batch in ds.conversation[:4]:  # Show first 4 turns\n",
    "            chunk = batch.agentic.get('chunk_id', 'N/A')[:15]\n",
    "            turn = batch.agentic.get('turn_number', 0)\n",
    "            print(f\"  [{chunk}] T{turn}: {batch.query[:50]}...\")\n",
    "        print()\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute\n",
    "combined_datasets = await generate_random_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to: combined_dataset_groq.json\n"
     ]
    }
   ],
   "source": [
    "# Save dataset to JSON\n",
    "combined_dataset = combined_datasets[0] \n",
    "output_file = Path(\"./combined_dataset_groq.json\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(combined_dataset.model_dump(), f, indent=2)\n",
    "\n",
    "print(f\"Dataset saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## Available Groq Models\n",
    "\n",
    "Check them [here](https://console.groq.com/docs/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "# Clean up sample files\n",
    "if sample_file.exists():\n",
    "    sample_file.unlink()\n",
    "if output_file.exists():\n",
    "    output_file.unlink()\n",
    "print(\"Cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
