{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "introduction",
      "metadata": {},
      "source": [
        "# Generators - Alquimia Example\n",
        "\n",
        "This notebook demonstrates how to use the Fair Forge generators module with **Alquimia** to create synthetic test datasets from context documents.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The generators module provides tools for creating test datasets:\n",
        "- **AlquimiaGenerator**: Wraps Alquimia's agent API for query generation\n",
        "- **LocalMarkdownLoader**: Loads and chunks markdown files with hybrid splitting\n",
        "- **Selection Strategies**: Control how chunks are sampled (Sequential, RandomSampling)\n",
        "- **Conversation Mode**: Generate coherent multi-turn conversations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "installation",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "First, install Fair Forge with Alquimia support and required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "!uv pip install --python {sys.executable} --force-reinstall \"$(ls ../../dist/*.whl)[generators-alquimia]\" -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Import the required modules and configure your Alquimia credentials.\n",
        "\n",
        "**Note:** The AlquimiaGenerator requires an agent configured in your Alquimia workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
        "\n",
        "from fair_forge.generators import (\n",
        "    create_alquimia_generator,\n",
        "    create_markdown_loader,\n",
        "    RandomSamplingStrategy,\n",
        ")\n",
        "from fair_forge.schemas import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "credentials",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "\n",
        "ALQUIMIA_API_KEY = getpass.getpass(\"Enter your Alquimia API key: \")\n",
        "ALQUIMIA_URL = input(\"Enter Alquimia URL (default: https://api.alquimia.ai): \") or \"https://api.alquimia.ai\"\n",
        "ALQUIMIA_AGENT_ID = input(\"Enter your Agent ID: \")\n",
        "ALQUIMIA_CHANNEL_ID = input(\"Enter your Channel ID: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "context-loader-header",
      "metadata": {},
      "source": [
        "## Create Context Loader\n",
        "\n",
        "The context loader reads source documents and splits them into chunks for query generation.\n",
        "\n",
        "The `LocalMarkdownLoader` uses a hybrid chunking strategy:\n",
        "1. **Primary**: Split by markdown headers (H1, H2, H3)\n",
        "2. **Fallback**: Split by character count for long sections without headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "context-loader",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create context loader with default settings\n",
        "loader = create_markdown_loader(\n",
        "    max_chunk_size=2000,\n",
        "    min_chunk_size=200,\n",
        "    overlap=100,\n",
        "    header_levels=[1, 2, 3],\n",
        ")\n",
        "\n",
        "print(\"Context loader created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sample-content-header",
      "metadata": {},
      "source": [
        "## Create Sample Content\n",
        "\n",
        "Let's create a sample markdown file to demonstrate the generator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sample-content",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample markdown content\n",
        "sample_content = \"\"\"# Fair Forge Documentation\n",
        "\n",
        "Fair Forge is a performance-measurement library for evaluating AI models and assistants.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "The library provides comprehensive metrics for:\n",
        "- **Fairness**: Measure bias across different demographic groups\n",
        "- **Toxicity**: Detect harmful or offensive language\n",
        "- **Conversational Quality**: Evaluate dialogue coherence and relevance\n",
        "- **Context Adherence**: Check if responses align with provided context\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "To get started with Fair Forge, install the package using pip and create a retriever to load your test datasets.\n",
        "\n",
        "### Basic Usage\n",
        "\n",
        "Here's a simple example of running the toxicity metric:\n",
        "\n",
        "```python\n",
        "from fair_forge.metrics import Toxicity\n",
        "results = Toxicity.run(MyRetriever)\n",
        "```\n",
        "\n",
        "## Architecture\n",
        "\n",
        "Fair Forge follows a modular architecture with the following components:\n",
        "\n",
        "1. **Core**: Base classes and interfaces\n",
        "2. **Metrics**: Individual metric implementations\n",
        "3. **Runners**: Test execution against AI systems\n",
        "4. **Storage**: Backend for test datasets and results\n",
        "\n",
        "Each component can be extended to support custom implementations.\n",
        "\"\"\"\n",
        "\n",
        "# Save to file\n",
        "sample_file = Path(\"./sample_docs.md\")\n",
        "sample_file.write_text(sample_content)\n",
        "print(f\"Sample content saved to: {sample_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-chunks-header",
      "metadata": {},
      "source": [
        "## Load and Chunk Content\n",
        "\n",
        "Let's see how the loader chunks the markdown content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-chunks",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and chunk the markdown file\n",
        "chunks = loader.load(str(sample_file))\n",
        "\n",
        "print(f\"Created {len(chunks)} chunks:\\n\")\n",
        "for i, chunk in enumerate(chunks, 1):\n",
        "    print(f\"Chunk {i}: {chunk.chunk_id}\")\n",
        "    print(f\"  Header: {chunk.metadata.get('header', 'N/A')}\")\n",
        "    print(f\"  Method: {chunk.metadata.get('chunking_method', 'N/A')}\")\n",
        "    print(f\"  Length: {len(chunk.content)} chars\")\n",
        "    print(f\"  Preview: {chunk.content[:80]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "generator-header",
      "metadata": {},
      "source": [
        "## Create Alquimia Generator\n",
        "\n",
        "The `AlquimiaGenerator` wraps the Alquimia client as a LangChain-compatible model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "generator-setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Alquimia generator\n",
        "generator = create_alquimia_generator(\n",
        "    base_url=ALQUIMIA_URL,\n",
        "    api_key=ALQUIMIA_API_KEY,\n",
        "    agent_id=ALQUIMIA_AGENT_ID,\n",
        "    channel_id=ALQUIMIA_CHANNEL_ID,\n",
        "    use_structured_output=True,\n",
        ")\n",
        "\n",
        "print(\"Generator created successfully\")\n",
        "print(f\"  Base URL: {generator.base_url}\")\n",
        "print(f\"  Agent ID: {generator.agent_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "generate-queries-header",
      "metadata": {},
      "source": [
        "## Generate Queries from Single Chunk\n",
        "\n",
        "Let's generate queries for a single chunk first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "generate-queries",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate queries for a single chunk\n",
        "chunk = chunks[0]\n",
        "print(f\"Generating queries for chunk: {chunk.chunk_id}\")\n",
        "print(f\"Content preview: {chunk.content[:100]}...\\n\")\n",
        "\n",
        "queries = await generator.generate_queries(\n",
        "    chunk=chunk,\n",
        "    num_queries=3,\n",
        ")\n",
        "\n",
        "print(f\"\\nGenerated {len(queries)} queries:\\n\")\n",
        "for i, q in enumerate(queries, 1):\n",
        "    print(f\"{i}. {q.query}\")\n",
        "    print(f\"   Difficulty: {q.difficulty}\")\n",
        "    print(f\"   Type: {q.query_type}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "generate-dataset-header",
      "metadata": {},
      "source": [
        "## Generate Complete Dataset\n",
        "\n",
        "Generate a complete test dataset from all chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "generate-dataset",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate complete dataset\n",
        "print(\"Generating complete dataset from markdown file...\\n\")\n",
        "\n",
        "datasets = await generator.generate_dataset(\n",
        "    context_loader=loader,\n",
        "    source=str(sample_file),\n",
        "    assistant_id=\"test-assistant\",\n",
        "    num_queries_per_chunk=3,\n",
        "    language=\"english\",\n",
        ")\n",
        "\n",
        "# With default SequentialStrategy, we get one dataset\n",
        "dataset = datasets[0]\n",
        "\n",
        "print(f\"Generated {len(datasets)} dataset(s):\")\n",
        "print(f\"  Session ID: {dataset.session_id}\")\n",
        "print(f\"  Assistant ID: {dataset.assistant_id}\")\n",
        "print(f\"  Language: {dataset.language}\")\n",
        "print(f\"  Total queries: {len(dataset.conversation)}\")\n",
        "print(f\"  Context length: {len(dataset.context)} chars\\n\")\n",
        "\n",
        "print(\"Sample queries:\")\n",
        "for batch in dataset.conversation[:5]:\n",
        "    print(f\"  - [{batch.qa_id}] {batch.query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seed-examples-header",
      "metadata": {},
      "source": [
        "## Generate with Seed Examples\n",
        "\n",
        "Guide the query generation style using seed examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seed-examples",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate with seed examples for style guidance\n",
        "seed_examples = [\n",
        "    \"What are the main components of Fair Forge's architecture?\",\n",
        "    \"How can I measure bias in my AI assistant's responses?\",\n",
        "    \"What steps are needed to integrate Fair Forge with an existing pipeline?\",\n",
        "]\n",
        "\n",
        "print(\"Generating with seed examples...\")\n",
        "print(f\"Seed examples provided: {len(seed_examples)}\\n\")\n",
        "\n",
        "datasets_with_seeds = await generator.generate_dataset(\n",
        "    context_loader=loader,\n",
        "    source=str(sample_file),\n",
        "    assistant_id=\"test-assistant\",\n",
        "    num_queries_per_chunk=2,\n",
        "    language=\"english\",\n",
        "    seed_examples=seed_examples,\n",
        ")\n",
        "\n",
        "dataset_seeds = datasets_with_seeds[0]\n",
        "print(f\"Generated {len(dataset_seeds.conversation)} queries with seed examples:\")\n",
        "for batch in dataset_seeds.conversation[:3]:\n",
        "    print(f\"  - {batch.query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "strategies-header",
      "metadata": {},
      "source": [
        "## Chunk Selection Strategies\n",
        "\n",
        "Strategies control how chunks are selected and grouped during generation.\n",
        "\n",
        "### RandomSamplingStrategy\n",
        "\n",
        "Randomly samples chunks multiple times to create diverse test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "random-sampling",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate multiple datasets using random chunk sampling\n",
        "strategy = RandomSamplingStrategy(\n",
        "    num_samples=2,       # Create 2 datasets\n",
        "    chunks_per_sample=2, # Each with 2 random chunks\n",
        "    seed=42,             # For reproducibility\n",
        ")\n",
        "\n",
        "print(f\"Strategy: {strategy}\\n\")\n",
        "\n",
        "random_datasets = await generator.generate_dataset(\n",
        "    context_loader=loader,\n",
        "    source=str(sample_file),\n",
        "    assistant_id=\"test-assistant\",\n",
        "    num_queries_per_chunk=2,\n",
        "    selection_strategy=strategy,\n",
        ")\n",
        "\n",
        "print(f\"Generated {len(random_datasets)} datasets:\\n\")\n",
        "for i, ds in enumerate(random_datasets):\n",
        "    print(f\"Dataset {i+1}: {len(ds.conversation)} queries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conversation-header",
      "metadata": {},
      "source": [
        "## Conversation Mode\n",
        "\n",
        "Generate coherent multi-turn conversations where each question builds on the previous:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "conversation-mode",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate coherent multi-turn conversations\n",
        "print(\"Generating conversations (each turn builds on the previous)...\\n\")\n",
        "\n",
        "conversation_datasets = await generator.generate_dataset(\n",
        "    context_loader=loader,\n",
        "    source=str(sample_file),\n",
        "    assistant_id=\"test-assistant\",\n",
        "    num_queries_per_chunk=3,  # 3-turn conversations\n",
        "    conversation_mode=True,   # Enable conversation mode\n",
        ")\n",
        "\n",
        "dataset_conv = conversation_datasets[0]\n",
        "print(f\"Generated {len(dataset_conv.conversation)} conversation turns:\\n\")\n",
        "\n",
        "# Group by chunk to show conversation flow\n",
        "current_chunk = None\n",
        "for batch in dataset_conv.conversation:\n",
        "    chunk_id = batch.agentic.get('chunk_id', 'N/A')\n",
        "    turn_num = batch.agentic.get('turn_number', 0)\n",
        "    \n",
        "    if chunk_id != current_chunk:\n",
        "        print(f\"\\n--- Conversation for: {chunk_id} ---\")\n",
        "        current_chunk = chunk_id\n",
        "    \n",
        "    print(f\"  Turn {turn_num}: {batch.query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-dataset-header",
      "metadata": {},
      "source": [
        "## Save Generated Dataset\n",
        "\n",
        "Save the generated dataset to JSON for use with runners and metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save-dataset",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save dataset to JSON\n",
        "output_file = Path(\"./generated_tests_alquimia.json\")\n",
        "\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(dataset.model_dump(), f, indent=2)\n",
        "\n",
        "print(f\"Dataset saved to: {output_file}\")\n",
        "print(f\"Total queries: {len(dataset.conversation)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "custom-prompt-note",
      "metadata": {},
      "source": [
        "## Note on Custom System Prompts\n",
        "\n",
        "**Important:** The AlquimiaGenerator does not support custom system prompts in the same way as direct LangChain models, because the agent's system prompt is configured in the Alquimia workspace.\n",
        "\n",
        "Instead, you can:\n",
        "1. Use **seed examples** to guide the style of generated queries\n",
        "2. Configure the agent's system prompt directly in your Alquimia workspace to accept `context`, `num_queries`, and `seed_examples` as template variables\n",
        "\n",
        "For full control over the system prompt, use a LangChain model directly with `BaseGenerator` (see the OpenAI or Groq example notebooks)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup-header",
      "metadata": {},
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleanup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up sample files\n",
        "if sample_file.exists():\n",
        "    sample_file.unlink()\n",
        "if output_file.exists():\n",
        "    output_file.unlink()\n",
        "print(\"Sample files cleaned up\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
