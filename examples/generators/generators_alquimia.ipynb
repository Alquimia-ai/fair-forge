{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Fair Forge Generators - Alquimia Example\n",
    "\n",
    "This notebook demonstrates how to use the Fair Forge generators module with **Alquimia** to create synthetic test datasets from context documents.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The generators module provides:\n",
    "- **BaseGenerator**: Base class that accepts any LangChain-compatible chat model\n",
    "- **AlquimiaGenerator**: Adapter that wraps Alquimia's agent API as a LangChain model\n",
    "- **AlquimiaChatModel**: LangChain-compatible adapter for Alquimia agents\n",
    "- **BaseContextLoader**: Abstract interface for loading and chunking context documents\n",
    "- **LocalMarkdownLoader**: Implementation for loading local markdown files with hybrid chunking\n",
    "\n",
    "## How AlquimiaGenerator Works\n",
    "\n",
    "The AlquimiaGenerator wraps the Alquimia client as a LangChain-compatible model. The `context`, `seed_examples`, and `num_queries` are extracted from the system prompt and passed as extra data kwargs to the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": "## Setup\n\nCreate `.env` file:\n```.env\nALQUIMIA_API_KEY=your-api-key\nALQUIMIA_URL=https://api.alquimia.ai\nALQUIMIA_AGENT_ID=your-agent-id\nALQUIMIA_CHANNEL_ID=your-channel-id\n```\n\nInstall required dependencies:\n```bash\nuv venv\nsource .venv/bin/activate\n\n# Option 1: Install fair-forge with Alquimia support\nuv pip install \"alquimia-fair-forge[generators-alquimia]\" python-dotenv\n\n# Option 2: Install separately\n# uv pip install alquimia-fair-forge alquimia-client python-dotenv\n\nuv run jupyter lab\n```\n\n**Note:** The AlquimiaGenerator requires an agent configured in your Alquimia workspace. The context, seed examples, and num_queries are passed to the agent as extra data that gets injected into the agent's system prompt."
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from fair_forge.generators import (\n",
    "    create_alquimia_generator,\n",
    "    create_markdown_loader,\n",
    "    AlquimiaGenerator,\n",
    "    AlquimiaChatModel,\n",
    "    BaseGenerator,\n",
    "    LocalMarkdownLoader,\n",
    "    # Strategies for chunk selection\n",
    "    SequentialStrategy,\n",
    "    RandomSamplingStrategy,\n",
    ")\n",
    "from fair_forge.schemas import Dataset, Batch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-loader-header",
   "metadata": {},
   "source": [
    "## Step 1: Create Context Loader\n",
    "\n",
    "The context loader reads source documents and splits them into chunks for query generation.\n",
    "\n",
    "The `LocalMarkdownLoader` uses a hybrid chunking strategy:\n",
    "1. **Primary**: Split by markdown headers (H1, H2, H3)\n",
    "2. **Fallback**: Split by character count for long sections without headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "context-loader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create context loader with default settings\n",
    "loader = create_markdown_loader(\n",
    "    max_chunk_size=2000,   # Maximum characters per chunk\n",
    "    min_chunk_size=200,    # Minimum characters per chunk\n",
    "    overlap=100,           # Overlap between size-based chunks\n",
    "    header_levels=[1, 2, 3],  # Split on H1, H2, H3 headers\n",
    ")\n",
    "\n",
    "print(\"Context loader created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-content-header",
   "metadata": {},
   "source": [
    "## Step 2: Create Sample Markdown Content\n",
    "\n",
    "Let's create a sample markdown file to demonstrate the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample markdown content\n",
    "sample_content = \"\"\"# Fair Forge Documentation\n",
    "\n",
    "Fair Forge is a performance-measurement library for evaluating AI models and assistants.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "The library provides comprehensive metrics for:\n",
    "- **Fairness**: Measure bias across different demographic groups\n",
    "- **Toxicity**: Detect harmful or offensive language\n",
    "- **Conversational Quality**: Evaluate dialogue coherence and relevance\n",
    "- **Context Adherence**: Check if responses align with provided context\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To get started with Fair Forge, install the package using pip:\n",
    "\n",
    "```bash\n",
    "pip install fair-forge\n",
    "```\n",
    "\n",
    "Then create a retriever to load your test datasets and run metrics.\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "Here's a simple example of running the toxicity metric:\n",
    "\n",
    "```python\n",
    "from fair_forge.metrics import Toxicity\n",
    "\n",
    "results = Toxicity.run(MyRetriever)\n",
    "```\n",
    "\n",
    "## Architecture\n",
    "\n",
    "Fair Forge follows a modular architecture with the following components:\n",
    "\n",
    "1. **Core**: Base classes and interfaces\n",
    "2. **Metrics**: Individual metric implementations\n",
    "3. **Runners**: Test execution against AI systems\n",
    "4. **Storage**: Backend for test datasets and results\n",
    "\n",
    "Each component can be extended to support custom implementations.\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "sample_file = Path(\"./sample_docs.md\")\n",
    "sample_file.write_text(sample_content)\n",
    "print(f\"Sample content saved to: {sample_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-chunks-header",
   "metadata": {},
   "source": [
    "## Step 3: Load and Chunk Content\n",
    "\n",
    "Let's see how the loader chunks the markdown content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-chunks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk the markdown file\n",
    "chunks = loader.load(str(sample_file))\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk.chunk_id}\")\n",
    "    print(f\"  Header: {chunk.metadata.get('header', 'N/A')}\")\n",
    "    print(f\"  Method: {chunk.metadata.get('chunking_method', 'N/A')}\")\n",
    "    print(f\"  Length: {len(chunk.content)} chars\")\n",
    "    print(f\"  Preview: {chunk.content[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-header",
   "metadata": {},
   "source": [
    "## Step 4: Create Alquimia Generator\n",
    "\n",
    "The `AlquimiaGenerator` wraps the Alquimia client as a LangChain-compatible model, allowing it to be used with the `BaseGenerator` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generator-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Alquimia generator using factory function\n",
    "# NOTE: Set these environment variables or replace with your actual values\n",
    "generator = create_alquimia_generator(\n",
    "    base_url=os.getenv(\"ALQUIMIA_URL\", \"https://api.alquimia.ai\"),\n",
    "    api_key=os.getenv(\"ALQUIMIA_API_KEY\", \"your-api-key\"),\n",
    "    agent_id=os.getenv(\"ALQUIMIA_AGENT_ID\", \"your-agent-id\"),\n",
    "    channel_id=os.getenv(\"ALQUIMIA_CHANNEL_ID\", \"your-channel-id\"),\n",
    "    use_structured_output=True,\n",
    ")\n",
    "\n",
    "print(\"Generator created successfully\")\n",
    "print(f\"  Base URL: {generator.base_url}\")\n",
    "print(f\"  Agent ID: {generator.agent_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-header",
   "metadata": {},
   "source": [
    "### Alternative: Direct Instantiation\n",
    "\n",
    "You can also create the generator directly, or use the `AlquimiaChatModel` with `BaseGenerator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct instantiation\n",
    "# generator = AlquimiaGenerator(\n",
    "#     base_url=\"https://api.alquimia.ai\",\n",
    "#     api_key=\"your-api-key\",\n",
    "#     agent_id=\"your-agent-id\",\n",
    "#     channel_id=\"your-channel-id\",\n",
    "# )\n",
    "\n",
    "# Or use AlquimiaChatModel directly with BaseGenerator\n",
    "# model = AlquimiaChatModel(\n",
    "#     base_url=\"https://api.alquimia.ai\",\n",
    "#     api_key=\"your-api-key\",\n",
    "#     agent_id=\"your-agent-id\",\n",
    "#     channel_id=\"your-channel-id\",\n",
    "# )\n",
    "# generator = BaseGenerator(model=model)\n",
    "\n",
    "print(\"See code comments for alternative approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-queries-header",
   "metadata": {},
   "source": [
    "## Step 5: Generate Queries from Single Chunk\n",
    "\n",
    "Let's generate queries for a single chunk first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate queries for a single chunk\n",
    "async def generate_from_chunk():\n",
    "    chunk = chunks[0]  # Use first chunk\n",
    "    print(f\"Generating queries for chunk: {chunk.chunk_id}\")\n",
    "    print(f\"Content preview: {chunk.content[:100]}...\\n\")\n",
    "    \n",
    "    queries = await generator.generate_queries(\n",
    "        chunk=chunk,\n",
    "        num_queries=3,\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(queries)} queries:\\n\")\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        print(f\"{i}. {q.query}\")\n",
    "        print(f\"   Difficulty: {q.difficulty}\")\n",
    "        print(f\"   Type: {q.query_type}\\n\")\n",
    "    \n",
    "    return queries\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "# queries = await generate_from_chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-dataset-header",
   "metadata": {},
   "source": [
    "## Step 6: Generate Complete Dataset\n",
    "\n",
    "Generate a complete test dataset from all chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complete dataset\n",
    "async def generate_full_dataset():\n",
    "    print(\"Generating complete dataset from markdown file...\\n\")\n",
    "    \n",
    "    # generate_dataset returns list[Dataset]\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"test-assistant\",\n",
    "        num_queries_per_chunk=3,\n",
    "        language=\"english\",\n",
    "    )\n",
    "    \n",
    "    # With default SequentialStrategy, we get one dataset\n",
    "    dataset = datasets[0]\n",
    "    \n",
    "    print(f\"Generated {len(datasets)} dataset(s):\")\n",
    "    print(f\"  Session ID: {dataset.session_id}\")\n",
    "    print(f\"  Assistant ID: {dataset.assistant_id}\")\n",
    "    print(f\"  Language: {dataset.language}\")\n",
    "    print(f\"  Total queries: {len(dataset.conversation)}\")\n",
    "    print(f\"  Context length: {len(dataset.context)} chars\\n\")\n",
    "    \n",
    "    print(\"Sample queries:\")\n",
    "    for batch in dataset.conversation[:5]:\n",
    "        print(f\"  - [{batch.qa_id}] {batch.query}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "# datasets = await generate_full_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seed-examples-header",
   "metadata": {},
   "source": [
    "## Step 7: Generate with Seed Examples\n",
    "\n",
    "Guide the query generation style using seed examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seed-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with seed examples for style guidance\n",
    "async def generate_with_seeds():\n",
    "    seed_examples = [\n",
    "        \"What are the main components of Fair Forge's architecture?\",\n",
    "        \"How can I measure bias in my AI assistant's responses?\",\n",
    "        \"What steps are needed to integrate Fair Forge with an existing pipeline?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"Generating with seed examples...\")\n",
    "    print(f\"Seed examples provided: {len(seed_examples)}\\n\")\n",
    "    \n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"test-assistant\",\n",
    "        num_queries_per_chunk=2,\n",
    "        language=\"english\",\n",
    "        seed_examples=seed_examples,\n",
    "    )\n",
    "    \n",
    "    dataset = datasets[0]\n",
    "    print(f\"Generated {len(dataset.conversation)} queries\")\n",
    "    return datasets\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "# datasets_with_seeds = await generate_with_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategies-header",
   "metadata": {},
   "source": [
    "## Chunk Selection Strategies\n",
    "\n",
    "Strategies control how chunks are selected and grouped during generation.\n",
    "\n",
    "### RandomSamplingStrategy\n",
    "\n",
    "Randomly samples chunks multiple times to create diverse test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_with_random_sampling():\n",
    "    \"\"\"Generate multiple datasets using random chunk sampling.\"\"\"\n",
    "    \n",
    "    strategy = RandomSamplingStrategy(\n",
    "        num_samples=2,       # Create 2 datasets\n",
    "        chunks_per_sample=2, # Each with 2 random chunks\n",
    "        seed=42,             # For reproducibility\n",
    "    )\n",
    "    \n",
    "    print(f\"Strategy: {strategy}\\n\")\n",
    "    \n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"test-assistant\",\n",
    "        num_queries_per_chunk=2,\n",
    "        selection_strategy=strategy,\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(datasets)} datasets:\\n\")\n",
    "    for i, ds in enumerate(datasets):\n",
    "        print(f\"Dataset {i+1}: {len(ds.conversation)} queries\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "# random_datasets = await generate_with_random_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation-header",
   "metadata": {},
   "source": [
    "## Conversation Mode\n",
    "\n",
    "Generate coherent multi-turn conversations where each question builds on the previous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conversation-mode",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_conversations():\n",
    "    \"\"\"Generate coherent multi-turn conversations.\"\"\"\n",
    "    \n",
    "    print(\"Generating conversations (each turn builds on the previous)...\\n\")\n",
    "    \n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"test-assistant\",\n",
    "        num_queries_per_chunk=3,  # 3-turn conversations\n",
    "        conversation_mode=True,   # Enable conversation mode\n",
    "    )\n",
    "    \n",
    "    dataset = datasets[0]\n",
    "    print(f\"Generated {len(dataset.conversation)} conversation turns:\\n\")\n",
    "    \n",
    "    # Group by chunk to show conversation flow\n",
    "    current_chunk = None\n",
    "    for batch in dataset.conversation:\n",
    "        chunk_id = batch.agentic.get('chunk_id', 'N/A')\n",
    "        turn_num = batch.agentic.get('turn_number', 0)\n",
    "        \n",
    "        if chunk_id != current_chunk:\n",
    "            print(f\"\\n--- Conversation for: {chunk_id} ---\")\n",
    "            current_chunk = chunk_id\n",
    "        \n",
    "        print(f\"  Turn {turn_num}: {batch.query}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "# conversation_datasets = await generate_conversations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-prompt-header",
   "metadata": {},
   "source": [
    "## Note on Custom System Prompts\n",
    "\n",
    "**Important:** The AlquimiaGenerator does not support custom system prompts in the same way as direct LangChain models, because the agent's system prompt is configured in the Alquimia workspace.\n",
    "\n",
    "Instead, you can:\n",
    "1. Use **seed examples** to guide the style of generated queries\n",
    "2. Configure the agent's system prompt directly in your Alquimia workspace to accept `context`, `num_queries`, and `seed_examples` as template variables\n",
    "\n",
    "For full control over the system prompt, use a LangChain model directly with `BaseGenerator` (see the OpenAI or Groq example notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom system prompts, use LangChain models directly with BaseGenerator:\n",
    "# \n",
    "# from langchain_openai import ChatOpenAI\n",
    "# \n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# generator = BaseGenerator(model=model)\n",
    "# \n",
    "# custom_prompt = \"\"\"You are a QA specialist creating test questions...\n",
    "# Context: {context}\n",
    "# {seed_examples_section}\n",
    "# Generate exactly {num_queries} technical questions.\n",
    "# \"\"\"\n",
    "# \n",
    "# datasets = await generator.generate_dataset(\n",
    "#     context_loader=loader,\n",
    "#     source=str(sample_file),\n",
    "#     assistant_id=\"test-assistant\",\n",
    "#     num_queries_per_chunk=2,\n",
    "#     custom_system_prompt=custom_prompt,\n",
    "# )\n",
    "\n",
    "print(\"See generators_openai.ipynb or generators_groq.ipynb for custom prompt examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-dataset-header",
   "metadata": {},
   "source": [
    "## Step 8: Save Generated Dataset\n",
    "\n",
    "Save the generated dataset to JSON for use with runners and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generated dataset to JSON\n",
    "async def save_dataset(dataset: Dataset, output_path: str):\n",
    "    output_file = Path(output_path)\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(dataset.model_dump(), f, indent=2)\n",
    "    \n",
    "    print(f\"Dataset saved to: {output_file}\")\n",
    "    print(f\"Total queries: {len(dataset.conversation)}\")\n",
    "    return output_file\n",
    "\n",
    "# Example usage (uncomment after generating dataset)\n",
    "# await save_dataset(dataset, \"./generated_tests.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integration-header",
   "metadata": {},
   "source": [
    "## Step 9: Integration with Runners\n",
    "\n",
    "Use the generated dataset with Fair Forge runners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example integration with runners\n",
    "# from fair_forge.runners import AlquimiaRunner\n",
    "# from fair_forge.storage import create_local_storage\n",
    "\n",
    "async def run_generated_tests(dataset: Dataset):\n",
    "    \"\"\"\n",
    "    Example of running generated tests against an AI assistant.\n",
    "    \n",
    "    Uncomment and configure to use.\n",
    "    \"\"\"\n",
    "    # # Configure runner\n",
    "    # runner = AlquimiaRunner(\n",
    "    #     base_url=os.getenv(\"ALQUIMIA_URL\"),\n",
    "    #     api_key=os.getenv(\"ALQUIMIA_API_KEY\"),\n",
    "    #     agent_id=os.getenv(\"AGENT_ID\"),\n",
    "    #     channel_id=os.getenv(\"CHANNEL_ID\"),\n",
    "    # )\n",
    "    # \n",
    "    # # Run dataset\n",
    "    # updated_dataset, summary = await runner.run_dataset(dataset)\n",
    "    # \n",
    "    # print(f\"Completed: {summary['successes']}/{summary['total_batches']} passed\")\n",
    "    # return updated_dataset\n",
    "    pass\n",
    "\n",
    "print(\"Integration example ready (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-loader-header",
   "metadata": {},
   "source": [
    "## Creating Custom Context Loaders\n",
    "\n",
    "You can create custom context loaders by extending `BaseContextLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-loader",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fair_forge.schemas.generators import BaseContextLoader, Chunk\n",
    "\n",
    "class JsonContextLoader(BaseContextLoader):\n",
    "    \"\"\"Example custom loader for JSON documents.\"\"\"\n",
    "    \n",
    "    def load(self, source: str) -> list[Chunk]:\n",
    "        \"\"\"Load and chunk a JSON file.\"\"\"\n",
    "        import json\n",
    "        from pathlib import Path\n",
    "        \n",
    "        path = Path(source)\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        chunks = []\n",
    "        # Example: each top-level key becomes a chunk\n",
    "        for i, (key, value) in enumerate(data.items()):\n",
    "            content = f\"{key}: {json.dumps(value, indent=2)}\"\n",
    "            chunks.append(Chunk(\n",
    "                content=content,\n",
    "                chunk_id=f\"json_{key}\",\n",
    "                metadata={\"key\": key, \"source\": str(path)},\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "print(\"Custom JsonContextLoader defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up sample files\n",
    "if sample_file.exists():\n",
    "    sample_file.unlink()\n",
    "    print(\"Sample files cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}