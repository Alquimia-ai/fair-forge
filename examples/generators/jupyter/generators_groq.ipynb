{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Fair Forge Generators - Groq Example\n",
    "\n",
    "This notebook demonstrates how to use the Fair Forge generators module with **Groq Cloud** for ultra-fast synthetic test dataset generation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `BaseGenerator` class accepts any LangChain-compatible chat model, including Groq's `ChatGroq`. This provides extremely fast inference for open-source LLMs.\n",
    "\n",
    "### Why Groq?\n",
    "- **Speed**: Up to 10x faster than traditional cloud providers\n",
    "- **Cost**: Competitive pricing for high-volume usage\n",
    "- **Models**: Access to popular open-source models (Llama 3, Mixtral, Gemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ad310-3913-41b9-94a5-ee50d835d0ea",
   "metadata": {},
   "source": [
    "## Installation\n",
    "First, install Fair Forge with Alquimia support and required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10ef3f-93dc-4298-a293-0b7b2b4fcd9e",
   "metadata": {},
   "outputs": [],
   "source": "!pip install \"alquimia-fair-forge[generators]\" langchain-groq -q"
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from fair_forge.generators import (\n",
    "    BaseGenerator,\n",
    "    # Strategies for chunk selection\n",
    "    RandomSamplingStrategy,\n",
    "    create_markdown_loader,\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-content-header",
   "metadata": {},
   "source": [
    "## Create Sample Content\n",
    "\n",
    "Let's create a sample markdown document for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sample-content",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample content saved to: ml_fundamentals.md\n"
     ]
    }
   ],
   "source": [
    "sample_content = \"\"\"# Machine Learning Fundamentals\n",
    "\n",
    "This guide covers the basics of machine learning for beginners.\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "Machine learning can be categorized into three main types:\n",
    "\n",
    "### Supervised Learning\n",
    "- Uses labeled training data\n",
    "- Predicts outcomes based on input features\n",
    "- Examples: Classification, Regression\n",
    "\n",
    "### Unsupervised Learning\n",
    "- Works with unlabeled data\n",
    "- Discovers hidden patterns and structures\n",
    "- Examples: Clustering, Dimensionality Reduction\n",
    "\n",
    "### Reinforcement Learning\n",
    "- Agent learns through interaction with environment\n",
    "- Maximizes cumulative reward\n",
    "- Examples: Game playing, Robotics\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "Key metrics for evaluating ML models:\n",
    "\n",
    "- **Accuracy**: Proportion of correct predictions\n",
    "- **Precision**: True positives among predicted positives\n",
    "- **Recall**: True positives among actual positives\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Split data into train/validation/test sets\n",
    "2. Use cross-validation for robust evaluation\n",
    "3. Monitor for overfitting\n",
    "4. Document your experiments\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "sample_file = Path(\"./ml_fundamentals.md\")\n",
    "sample_file.write_text(sample_content)\n",
    "print(f\"Sample content saved to: {sample_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loader-header",
   "metadata": {},
   "source": [
    "## Create Context Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "loader",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:10:17.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators\u001b[0m:\u001b[36mcreate_markdown_loader\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mCreating local markdown loader\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:17.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:17.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:17.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 7 chunks:\n",
      "\n",
      "- ml_fundamentals_machine_learning_fundamentals: 63 chars\n",
      "- ml_fundamentals_types_of_machine_learning: 58 chars\n",
      "- ml_fundamentals_supervised_learning: 111 chars\n",
      "- ml_fundamentals_unsupervised_learning: 119 chars\n",
      "- ml_fundamentals_reinforcement_learning: 116 chars\n",
      "- ml_fundamentals_model_evaluation: 252 chars\n",
      "- ml_fundamentals_best_practices: 147 chars\n"
     ]
    }
   ],
   "source": [
    "# Create markdown loader\n",
    "loader = create_markdown_loader(\n",
    "    max_chunk_size=2000,\n",
    "    header_levels=[1, 2, 3],\n",
    ")\n",
    "\n",
    "# Preview chunks\n",
    "chunks = loader.load(str(sample_file))\n",
    "print(f\"Created {len(chunks)} chunks:\\n\")\n",
    "for chunk in chunks:\n",
    "    print(f\"- {chunk.chunk_id}: {len(chunk.content)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-header",
   "metadata": {},
   "source": [
    "## Create Generator with Groq Model\n",
    "\n",
    "The `BaseGenerator` accepts any LangChain-compatible chat model. Here we use `ChatGroq` from `langchain-groq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator created with model: llama-3.1-8b-instant\n"
     ]
    }
   ],
   "source": [
    "# Create Groq model using LangChain\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  # Fast model for demos\n",
    "    temperature=0.4,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "# Create generator with the model\n",
    "generator = BaseGenerator(\n",
    "    model=model,\n",
    "    use_structured_output=True,\n",
    ")\n",
    "\n",
    "print(f\"Generator created with model: {model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-header",
   "metadata": {},
   "source": [
    "## Generate Test Dataset\n",
    "\n",
    "Groq's fast inference makes generation very quick!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:10:18.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:18.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:18.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:18.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:18.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m527\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:18.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m531\u001b[0m - \u001b[1mUsing chunk selection strategy: SequentialStrategy()\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:18.216\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test dataset with Groq...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 17:10:19,305 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:19.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:19.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "2026-01-15 17:10:20,042 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:20.046\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:20.047\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "2026-01-15 17:10:20,656 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:20.659\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 1 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:20.661\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "2026-01-15 17:10:21,289 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:21.292\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:21.293\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "2026-01-15 17:10:21,992 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:21.995\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:21.996\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "2026-01-15 17:10:22,606 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:22.609\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:22.610\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "2026-01-15 17:10:23,336 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:23.339\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:23.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 19 batches for chunk group (7 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:23.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1mGenerated 1 dataset(s) with 19 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 dataset(s) in 5.13 seconds:\n",
      "  Session ID: e51451eb-f4f5-4917-885b-efadb4ecfe0d\n",
      "  Total queries: 19\n",
      "\n",
      "Generated queries:\n",
      "  [ml_fundamentals_machine_learning_fundamentals_q1] (medium/factual)\n",
      "    What are the fundamental concepts of machine learning?\n",
      "\n",
      "  [ml_fundamentals_machine_learning_fundamentals_q2] (hard/application)\n",
      "    How can you apply machine learning in real-world scenarios?\n",
      "\n",
      "  [ml_fundamentals_machine_learning_fundamentals_q3] (medium/comparative)\n",
      "    What are the key differences between supervised and unsupervised learning?\n",
      "\n",
      "  [ml_fundamentals_types_of_machine_learning_q1] (easy/factual)\n",
      "    What are the three main types of machine learning?\n",
      "\n",
      "  [ml_fundamentals_types_of_machine_learning_q2] (medium/application)\n",
      "    How can machine learning be applied in real-world scenarios?\n",
      "\n",
      "  [ml_fundamentals_types_of_machine_learning_q3] (hard/comparative)\n",
      "    What are the key differences between supervised and unsupervised machine learning?\n",
      "\n",
      "  [ml_fundamentals_supervised_learning_q1] (N/A/factual)\n",
      "    What type of tasks does the model perform?\n",
      "\n",
      "  [ml_fundamentals_unsupervised_learning_q1] (medium/application)\n",
      "    What are some common applications of clustering in data analysis?\n",
      "\n",
      "  [ml_fundamentals_unsupervised_learning_q2] (hard/inferential)\n",
      "    How does dimensionality reduction help in identifying hidden patterns in data?\n",
      "\n",
      "  [ml_fundamentals_unsupervised_learning_q3] (medium/comparative)\n",
      "    What are some key differences between clustering and dimensionality reduction?\n",
      "\n",
      "  [ml_fundamentals_reinforcement_learning_q1] (medium/factual)\n",
      "    How does an agent learn through interaction with its environment?\n",
      "\n",
      "  [ml_fundamentals_reinforcement_learning_q2] (N/A/factual)\n",
      "    What are some examples of environments where an agent can maximize cumulative reward?\n",
      "\n",
      "  [ml_fundamentals_reinforcement_learning_q3] (hard/application)\n",
      "    How can we design an environment to test an agent's ability to apply its learned knowledge?\n",
      "\n",
      "  [ml_fundamentals_model_evaluation_q1] (easy/factual)\n",
      "    What are the key metrics used to evaluate ML models?\n",
      "\n",
      "  [ml_fundamentals_model_evaluation_q2] (medium/inferential)\n",
      "    How do you interpret the F1 Score in the context of ML model evaluation?\n",
      "\n",
      "  [ml_fundamentals_model_evaluation_q3] (hard/comparative)\n",
      "    Compare and contrast the concepts of precision and recall in ML model evaluation.\n",
      "\n",
      "  [ml_fundamentals_best_practices_q1] (medium/factual)\n",
      "    What are the typical steps involved in splitting data for machine learning?\n",
      "\n",
      "  [ml_fundamentals_best_practices_q2] (hard/inferential)\n",
      "    How does cross-validation help in evaluating model performance?\n",
      "\n",
      "  [ml_fundamentals_best_practices_q3] (medium/comparative)\n",
      "    What are some common strategies for preventing overfitting in machine learning models?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def generate_dataset():\n",
    "    print(\"Generating test dataset with Groq...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # generate_dataset returns list[Dataset]\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=3,\n",
    "        language=\"english\",\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # With default SequentialStrategy, we get one dataset\n",
    "    dataset = datasets[0]\n",
    "\n",
    "    print(f\"Generated {len(datasets)} dataset(s) in {elapsed:.2f} seconds:\")\n",
    "    print(f\"  Session ID: {dataset.session_id}\")\n",
    "    print(f\"  Total queries: {len(dataset.conversation)}\\n\")\n",
    "\n",
    "    print(\"Generated queries:\")\n",
    "    for batch in dataset.conversation:\n",
    "        difficulty = batch.agentic.get(\"difficulty\", \"N/A\")\n",
    "        query_type = batch.agentic.get(\"query_type\", \"N/A\")\n",
    "        print(f\"  [{batch.qa_id}] ({difficulty}/{query_type})\")\n",
    "        print(f\"    {batch.query}\\n\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Execute\n",
    "datasets = await generate_dataset()\n",
    "dataset = datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seed-examples-header",
   "metadata": {},
   "source": [
    "## Generate with Seed Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seed-examples",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:10:23.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:23.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:23.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:23.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:23.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m527\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:23.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m531\u001b[0m - \u001b[1mUsing chunk selection strategy: SequentialStrategy()\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:23.355\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with seed examples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 17:10:23,829 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:23.831\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:23.832\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "2026-01-15 17:10:24,367 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:24.371\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:24.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "2026-01-15 17:10:24,957 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:24.960\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 1 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:24.961\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "2026-01-15 17:10:25,469 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:25.473\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:25.474\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "2026-01-15 17:10:25,982 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:25.985\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:25.987\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "2026-01-15 17:10:26,493 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:26.497\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:26.498\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "2026-01-15 17:10:27,005 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:27.008\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:27.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 13 batches for chunk group (7 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:27.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1mGenerated 1 dataset(s) with 13 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 13 queries:\n",
      "  - What are the primary types of machine learning algorithms for beginners?\n",
      "  - How can you balance model complexity and overfitting in a machine learning model?\n",
      "  - What are the three main types of machine learning?\n",
      "  - How do the different types of machine learning impact model performance?\n",
      "  - What are the key differences between supervised and unsupervised learning?\n"
     ]
    }
   ],
   "source": [
    "async def generate_with_seeds():\n",
    "    seed_examples = [\n",
    "        \"What is the difference between supervised and unsupervised learning?\",\n",
    "        \"How do you prevent overfitting in a machine learning model?\",\n",
    "        \"When should you use precision vs recall as your primary metric?\",\n",
    "    ]\n",
    "\n",
    "    print(\"Generating with seed examples...\\n\")\n",
    "\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=2,\n",
    "        seed_examples=seed_examples,\n",
    "    )\n",
    "\n",
    "    dataset = datasets[0]\n",
    "    print(f\"Generated {len(dataset.conversation)} queries:\")\n",
    "    for batch in dataset.conversation[:5]:\n",
    "        print(f\"  - {batch.query}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Execute\n",
    "datasets_with_seeds = await generate_with_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategies-header",
   "metadata": {},
   "source": [
    "## Chunk Selection Strategies\n",
    "\n",
    "Strategies control how chunks are selected and grouped during generation. By default, all chunks are processed sequentially into a single dataset.\n",
    "\n",
    "### RandomSamplingStrategy\n",
    "\n",
    "Randomly samples chunks multiple times to create diverse test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "random-sampling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:10:27.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:27.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:27.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:27.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:27.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m527\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:27.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m531\u001b[0m - \u001b[1mUsing chunk selection strategy: RandomSamplingStrategy(num_samples=2, chunks_per_sample=3, seed=42)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:27.031\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: RandomSamplingStrategy(num_samples=2, chunks_per_sample=3, seed=42)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 17:10:27,517 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:27.521\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:27.522\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "2026-01-15 17:10:27,949 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:27.952\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:27.953\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "2026-01-15 17:10:28,525 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:28.528\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:28.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 6 batches for chunk group (3 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:28.530\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "2026-01-15 17:10:29,263 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:29.266\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:29.267\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "2026-01-15 17:10:29,869 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:29.872\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:29.873\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m384\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "2026-01-15 17:10:30,384 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:30.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m415\u001b[0m - \u001b[34m\u001b[1mGenerated 1 queries for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:30.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 5 batches for chunk group (3 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:30.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1mGenerated 2 dataset(s) with 11 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 datasets:\n",
      "\n",
      "Dataset 1:\n",
      "  Session: c2356905...\n",
      "  Queries: 6\n",
      "  Chunks: {'ml_fundamentals_model_evaluation', 'ml_fundamentals_machine_learning_fundamentals', 'ml_fundamentals_best_practices'}\n",
      "\n",
      "Dataset 2:\n",
      "  Session: 6541f97c...\n",
      "  Queries: 5\n",
      "  Chunks: {'ml_fundamentals_model_evaluation', 'ml_fundamentals_supervised_learning', 'ml_fundamentals_types_of_machine_learning'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def generate_with_random_sampling():\n",
    "    \"\"\"Generate multiple datasets using random chunk sampling.\"\"\"\n",
    "\n",
    "    # Create a strategy that samples 3 random chunks, 2 times\n",
    "    strategy = RandomSamplingStrategy(\n",
    "        num_samples=2,  # Create 2 datasets\n",
    "        chunks_per_sample=3,  # Each with 3 random chunks\n",
    "        seed=42,  # For reproducibility\n",
    "    )\n",
    "\n",
    "    print(f\"Strategy: {strategy}\\n\")\n",
    "\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=2,\n",
    "        selection_strategy=strategy,\n",
    "    )\n",
    "\n",
    "    print(f\"Generated {len(datasets)} datasets:\\n\")\n",
    "    for i, ds in enumerate(datasets):\n",
    "        print(f\"Dataset {i+1}:\")\n",
    "        print(f\"  Session: {ds.session_id[:8]}...\")\n",
    "        print(f\"  Queries: {len(ds.conversation)}\")\n",
    "        # Show chunk IDs from the queries\n",
    "        chunk_ids = set(b.agentic.get(\"chunk_id\", \"N/A\") for b in ds.conversation)\n",
    "        print(f\"  Chunks: {chunk_ids}\\n\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Execute\n",
    "random_datasets = await generate_with_random_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation-header",
   "metadata": {},
   "source": [
    "## Conversation Mode\n",
    "\n",
    "Instead of generating independent queries, conversation mode creates coherent multi-turn conversations where each question builds on the previous ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conversation-mode",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:10:30.401\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:30.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:30.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:30.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:30.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m527\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:30.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m531\u001b[0m - \u001b[1mUsing chunk selection strategy: SequentialStrategy()\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:30.406\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating conversations (each turn builds on the previous)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 17:10:30,987 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:30.991\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:30.992\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "2026-01-15 17:10:31,582 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:31.587\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_types_of_machine_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:31.588\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "2026-01-15 17:10:32,535 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:32.539\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_supervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:32.540\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "2026-01-15 17:10:33,320 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:33.323\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_unsupervised_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:33.324\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "2026-01-15 17:10:33,966 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:33.969\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_reinforcement_learning\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:33.969\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "2026-01-15 17:10:34,583 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:34.587\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:34.588\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "2026-01-15 17:10:35,299 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:35.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mGenerated 3 turns for chunk ml_fundamentals_best_practices\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:35.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 21 batches for chunk group (7 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:35.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1mGenerated 1 dataset(s) with 21 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 21 conversation turns:\n",
      "\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_machine_learning_fundamentals ---\n",
      "  Turn 1: What is machine learning?\n",
      "  Turn 2: Can you give an example of how machine learning is used in real life?\n",
      "         (builds on: ml_fundamentals_machine_learning_fundamentals_t1)\n",
      "  Turn 3: How does machine learning differ from traditional programming?\n",
      "         (builds on: ml_fundamentals_machine_learning_fundamentals_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_types_of_machine_learning ---\n",
      "  Turn 1: What are the main categories of machine learning?\n",
      "  Turn 2: Can you explain supervised learning in more detail?\n",
      "         (builds on: ml_fundamentals_types_of_machine_learning_t1)\n",
      "  Turn 3: How does supervised learning compare to unsupervised learning in terms of data requirements?\n",
      "         (builds on: ml_fundamentals_types_of_machine_learning_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_supervised_learning ---\n",
      "  Turn 1: What is the main purpose of using labeled training data?\n",
      "  Turn 2: How does labeled training data help in predicting outcomes?\n",
      "         (builds on: ml_fundamentals_supervised_learning_t1)\n",
      "  Turn 3: Can you provide an example of how labeled training data is used in classification and regression tasks?\n",
      "         (builds on: ml_fundamentals_supervised_learning_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_unsupervised_learning ---\n",
      "  Turn 1: What are some common applications of unsupervised learning?\n",
      "  Turn 2: Can you give an example of how clustering works in unsupervised learning?\n",
      "         (builds on: ml_fundamentals_unsupervised_learning_t1)\n",
      "  Turn 3: How does dimensionality reduction relate to clustering in unsupervised learning?\n",
      "         (builds on: ml_fundamentals_unsupervised_learning_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_reinforcement_learning ---\n",
      "  Turn 1: What is learning?\n",
      "  Turn 2: How does an agent learn through interaction with the environment?\n",
      "         (builds on: ml_fundamentals_reinforcement_learning_t1)\n",
      "  Turn 3: Can you give an example of an agent maximizing cumulative reward?\n",
      "         (builds on: ml_fundamentals_reinforcement_learning_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_model_evaluation ---\n",
      "  Turn 1: What are the key metrics used to evaluate machine learning models?\n",
      "  Turn 2: Can you explain the difference between accuracy and precision?\n",
      "         (builds on: ml_fundamentals_model_evaluation_t1)\n",
      "  Turn 3: How do recall and F1 score relate to each other in the context of model evaluation?\n",
      "         (builds on: ml_fundamentals_model_evaluation_t2)\n",
      "\n",
      "--- Conversation for chunk: ml_fundamentals_best_practices ---\n",
      "  Turn 1: What are the general steps for splitting data into train and test sets?\n",
      "  Turn 2: How can we use cross-validation to evaluate our model's performance?\n",
      "         (builds on: ml_fundamentals_best_practices_t1)\n",
      "  Turn 3: What are some common techniques to monitor for overfitting in machine learning?\n",
      "         (builds on: ml_fundamentals_best_practices_t2)\n"
     ]
    }
   ],
   "source": [
    "async def generate_conversations():\n",
    "    \"\"\"Generate coherent multi-turn conversations.\"\"\"\n",
    "\n",
    "    print(\"Generating conversations (each turn builds on the previous)...\\n\")\n",
    "\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=3,  # 3-turn conversations\n",
    "        conversation_mode=True,  # Enable conversation mode\n",
    "    )\n",
    "\n",
    "    dataset = datasets[0]\n",
    "    print(f\"Generated {len(dataset.conversation)} conversation turns:\\n\")\n",
    "\n",
    "    # Group by chunk to show conversation flow\n",
    "    current_chunk = None\n",
    "    for batch in dataset.conversation:\n",
    "        chunk_id = batch.agentic.get(\"chunk_id\", \"N/A\")\n",
    "        turn_num = batch.agentic.get(\"turn_number\", 0)\n",
    "        builds_on = batch.agentic.get(\"builds_on\", None)\n",
    "\n",
    "        if chunk_id != current_chunk:\n",
    "            print(f\"\\n--- Conversation for chunk: {chunk_id} ---\")\n",
    "            current_chunk = chunk_id\n",
    "\n",
    "        print(f\"  Turn {turn_num}: {batch.query}\")\n",
    "        if builds_on:\n",
    "            print(f\"         (builds on: {builds_on})\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Execute\n",
    "conversation_datasets = await generate_conversations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-header",
   "metadata": {},
   "source": [
    "### Combined: Random Sampling + Conversation Mode\n",
    "\n",
    "You can combine strategies with conversation mode to create diverse conversation-based test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "combined",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:10:35.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1mLoading context from: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:35.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:35.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: ml_fundamentals.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:35.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 7 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:35.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m527\u001b[0m - \u001b[1mLoaded 7 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:35.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m531\u001b[0m - \u001b[1mUsing chunk selection strategy: RandomSamplingStrategy(num_samples=2, chunks_per_sample=2, seed=42)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:35.314\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mGenerating 2-turn conversation for chunk ml_fundamentals_model_evaluation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2 datasets with 2 random chunks each (conversation mode)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 17:10:36,016 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:36.019\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mGenerated 2 turns for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:36.020\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mGenerating 2-turn conversation for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "2026-01-15 17:10:36,733 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:36.736\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mGenerated 2 turns for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:36.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 4 batches for chunk group (2 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:36.739\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mGenerating 2-turn conversation for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "2026-01-15 17:10:37,346 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:37.349\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mGenerated 2 turns for chunk ml_fundamentals_machine_learning_fundamentals\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:37.350\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mGenerating 2-turn conversation for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "2026-01-15 17:10:37,961 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:10:37.963\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m482\u001b[0m - \u001b[34m\u001b[1mGenerated 2 turns for chunk ml_fundamentals_model_evaluation\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:37.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 4 batches for chunk group (2 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:10:37.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1mGenerated 2 dataset(s) with 8 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 (4 turns):\n",
      "  [ml_fundamentals] T1: What are the key metrics used to evaluate machine ...\n",
      "  [ml_fundamentals] T2: Can you explain the difference between precision a...\n",
      "  [ml_fundamentals] T1: What is machine learning?...\n",
      "  [ml_fundamentals] T2: How does it differ from traditional programming?...\n",
      "\n",
      "Dataset 2 (4 turns):\n",
      "  [ml_fundamentals] T1: What is machine learning?...\n",
      "  [ml_fundamentals] T2: How does it differ from traditional programming?...\n",
      "  [ml_fundamentals] T1: What are the key metrics used to evaluate ML model...\n",
      "  [ml_fundamentals] T2: Can you explain the difference between precision a...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def generate_random_conversations():\n",
    "    \"\"\"Combine random sampling with conversation mode.\"\"\"\n",
    "\n",
    "    strategy = RandomSamplingStrategy(\n",
    "        num_samples=2,\n",
    "        chunks_per_sample=2,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    print(\"Generating 2 datasets with 2 random chunks each (conversation mode)...\\n\")\n",
    "\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"ml-assistant\",\n",
    "        num_queries_per_chunk=2,  # 2-turn conversations\n",
    "        selection_strategy=strategy,\n",
    "        conversation_mode=True,\n",
    "    )\n",
    "\n",
    "    for i, ds in enumerate(datasets):\n",
    "        print(f\"Dataset {i+1} ({len(ds.conversation)} turns):\")\n",
    "        for batch in ds.conversation[:4]:  # Show first 4 turns\n",
    "            chunk = batch.agentic.get(\"chunk_id\", \"N/A\")[:15]\n",
    "            turn = batch.agentic.get(\"turn_number\", 0)\n",
    "            print(f\"  [{chunk}] T{turn}: {batch.query[:50]}...\")\n",
    "        print()\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Execute\n",
    "combined_datasets = await generate_random_conversations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to: generated_tests_groq.json\n"
     ]
    }
   ],
   "source": [
    "# Save dataset to JSON\n",
    "output_file = Path(\"./generated_tests_groq.json\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(dataset.model_dump(), f, indent=2)\n",
    "\n",
    "print(f\"Dataset saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## Available Groq Models\n",
    "\n",
    "Check [Groq Console](https://console.groq.com/docs/models) for the latest available models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup completed\n"
     ]
    }
   ],
   "source": [
    "# Clean up sample files\n",
    "if sample_file.exists():\n",
    "    sample_file.unlink()\n",
    "if output_file.exists():\n",
    "    output_file.unlink()\n",
    "print(\"Cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}