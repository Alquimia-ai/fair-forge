{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Fair Forge Generators - Alquimia Example\n",
    "\n",
    "This notebook demonstrates how to use the Fair Forge generators module with **Alquimia** to create synthetic test datasets from context documents.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The generators module provides:\n",
    "- **BaseGenerator**: Base class that accepts any LangChain-compatible chat model\n",
    "- **AlquimiaGenerator**: Adapter that wraps Alquimia's agent API as a LangChain model\n",
    "- **AlquimiaChatModel**: LangChain-compatible adapter for Alquimia agents\n",
    "- **BaseContextLoader**: Abstract interface for loading and chunking context documents\n",
    "- **LocalMarkdownLoader**: Implementation for loading local markdown files with hybrid chunking\n",
    "\n",
    "## How AlquimiaGenerator Works\n",
    "\n",
    "The AlquimiaGenerator wraps the Alquimia client as a LangChain-compatible model. The `context`, `seed_examples`, and `num_queries` are extracted from the system prompt and passed as extra data kwargs to the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa31e5-6667-4f26-b99b-386e3d61374e",
   "metadata": {},
   "source": [
    "## Installation\n",
    "First, install Fair Forge with Alquimia support and required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282543e-cb57-4572-80e5-96ec61f8a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!uv pip install --python {sys.executable} --force-reinstall \"$(ls ../../dist/*.whl)[generators-alquimia]\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f91e6-f3dc-44b7-88c9-45c39fbabbb5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the required modules and configure your Alquimia credentials.\n",
    "\n",
    "**Note:** The AlquimiaGenerator requires an agent configured in your Alquimia workspace. The context, seed examples, and num_queries are passed to the agent as extra data that gets injected into the agent's system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642e3b9-d1c8-4333-8692-1e5a670ea721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from fair_forge.generators import (\n",
    "    RandomSamplingStrategy,\n",
    "    create_alquimia_generator,\n",
    "    create_markdown_loader,\n",
    ")\n",
    "from fair_forge.schemas import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010344d-c540-402e-945c-92489713ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "ALQUIMIA_API_KEY = getpass.getpass(\"Enter your Alquimia API key: \")\n",
    "ALQUIMIA_URL = input(\"Enter Alquimia URL (default: https://api.alquimia.ai): \") or \"https://api.alquimia.ai\"\n",
    "ALQUIMIA_AGENT_ID = input(\"Enter your Agent ID: \")\n",
    "ALQUIMIA_CHANNEL_ID = input(\"Enter your Channel ID: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-loader-header",
   "metadata": {},
   "source": [
    "## Step 1: Create Context Loader\n",
    "\n",
    "The context loader reads source documents and splits them into chunks for query generation.\n",
    "\n",
    "The `LocalMarkdownLoader` uses a hybrid chunking strategy:\n",
    "1. **Primary**: Split by markdown headers (H1, H2, H3)\n",
    "2. **Fallback**: Split by character count for long sections without headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "context-loader",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:38:15.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators\u001b[0m:\u001b[36mcreate_markdown_loader\u001b[0m:\u001b[36m138\u001b[0m - \u001b[1mCreating local markdown loader\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context loader created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create context loader with default settings\n",
    "loader = create_markdown_loader(\n",
    "    max_chunk_size=2000,  # Maximum characters per chunk\n",
    "    min_chunk_size=200,  # Minimum characters per chunk\n",
    "    overlap=100,  # Overlap between size-based chunks\n",
    "    header_levels=[1, 2, 3],  # Split on H1, H2, H3 headers\n",
    ")\n",
    "\n",
    "print(\"Context loader created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-content-header",
   "metadata": {},
   "source": [
    "## Step 2: Create Sample Markdown Content\n",
    "\n",
    "Let's create a sample markdown file to demonstrate the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sample-content",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample content saved to: sample_docs.md\n"
     ]
    }
   ],
   "source": [
    "# Create sample markdown content\n",
    "sample_content = \"\"\"# Fair Forge Documentation\n",
    "\n",
    "Fair Forge is a performance-measurement library for evaluating AI models and assistants.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "The library provides comprehensive metrics for:\n",
    "- **Fairness**: Measure bias across different demographic groups\n",
    "- **Toxicity**: Detect harmful or offensive language\n",
    "- **Conversational Quality**: Evaluate dialogue coherence and relevance\n",
    "- **Context Adherence**: Check if responses align with provided context\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To get started with Fair Forge, install the package using pip:\n",
    "\n",
    "```bash\n",
    "pip install alquimia-fair-forge\n",
    "```\n",
    "\n",
    "Then create a retriever to load your test datasets and run metrics.\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "Here's a simple example of running the toxicity metric:\n",
    "\n",
    "```python\n",
    "from fair_forge.metrics import Toxicity\n",
    "\n",
    "results = Toxicity.run(MyRetriever)\n",
    "```\n",
    "\n",
    "## Architecture\n",
    "\n",
    "Fair Forge follows a modular architecture with the following components:\n",
    "\n",
    "1. **Core**: Base classes and interfaces\n",
    "2. **Metrics**: Individual metric implementations\n",
    "3. **Runners**: Test execution against AI systems\n",
    "4. **Storage**: Backend for test datasets and results\n",
    "\n",
    "Each component can be extended to support custom implementations.\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "sample_file = Path(\"./sample_docs.md\")\n",
    "sample_file.write_text(sample_content)\n",
    "print(f\"Sample content saved to: {sample_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-chunks-header",
   "metadata": {},
   "source": [
    "## Step 3: Load and Chunk Content\n",
    "\n",
    "Let's see how the loader chunks the markdown content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load-chunks",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:38:16.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:16.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: sample_docs.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:16.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 5 total chunks from 1 file(s)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 chunks:\n",
      "\n",
      "Chunk 1: sample_docs_fair_forge_documentation\n",
      "  Header: Fair Forge Documentation\n",
      "  Method: header\n",
      "  Length: 88 chars\n",
      "  Preview: Fair Forge is a performance-measurement library for evaluating AI models and ass...\n",
      "\n",
      "Chunk 2: sample_docs_key_features\n",
      "  Header: Key Features\n",
      "  Method: header\n",
      "  Length: 309 chars\n",
      "  Preview: The library provides comprehensive metrics for:\n",
      "- **Fairness**: Measure bias acr...\n",
      "\n",
      "Chunk 3: sample_docs_getting_started\n",
      "  Header: Getting Started\n",
      "  Method: header\n",
      "  Length: 167 chars\n",
      "  Preview: To get started with Fair Forge, install the package using pip:\n",
      "\n",
      "```bash\n",
      "pip inst...\n",
      "\n",
      "Chunk 4: sample_docs_basic_usage\n",
      "  Header: Basic Usage\n",
      "  Method: header\n",
      "  Length: 147 chars\n",
      "  Preview: Here's a simple example of running the toxicity metric:\n",
      "\n",
      "```python\n",
      "from fair_for...\n",
      "\n",
      "Chunk 5: sample_docs_architecture\n",
      "  Header: Architecture\n",
      "  Method: header\n",
      "  Length: 335 chars\n",
      "  Preview: Fair Forge follows a modular architecture with the following components:\n",
      "\n",
      "1. **C...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and chunk the markdown file\n",
    "chunks = loader.load(str(sample_file))\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk.chunk_id}\")\n",
    "    print(f\"  Header: {chunk.metadata.get('header', 'N/A')}\")\n",
    "    print(f\"  Method: {chunk.metadata.get('chunking_method', 'N/A')}\")\n",
    "    print(f\"  Length: {len(chunk.content)} chars\")\n",
    "    print(f\"  Preview: {chunk.content[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-header",
   "metadata": {},
   "source": [
    "## Step 4: Create Alquimia Generator\n",
    "\n",
    "The `AlquimiaGenerator` wraps the Alquimia client as a LangChain-compatible model, allowing it to be used with the `BaseGenerator` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "generator-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:38:16.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators\u001b[0m:\u001b[36mcreate_alquimia_generator\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mCreating Alquimia generator\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator created successfully\n",
      "  Base URL: https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com\n",
      "  Agent ID: test-generator\n"
     ]
    }
   ],
   "source": [
    "# Create Alquimia generator using factory function\n",
    "# NOTE: Set these environment variables or replace with your actual values\n",
    "generator = create_alquimia_generator(\n",
    "    base_url=os.getenv(\"ALQUIMIA_URL\", \"https://api.alquimia.ai\"),\n",
    "    api_key=os.getenv(\"ALQUIMIA_API_KEY\", \"your-api-key\"),\n",
    "    agent_id=os.getenv(\"ALQUIMIA_AGENT_ID\", \"your-agent-id\"),\n",
    "    channel_id=os.getenv(\"ALQUIMIA_CHANNEL_ID\", \"your-channel-id\"),\n",
    ")\n",
    "\n",
    "print(\"Generator created successfully\")\n",
    "print(f\"  Base URL: {generator.base_url}\")\n",
    "print(f\"  Agent ID: {generator.agent_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-queries-header",
   "metadata": {},
   "source": [
    "## Step 5: Generate Queries from Single Chunk\n",
    "\n",
    "Let's generate queries for a single chunk first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "generate-queries",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:38:17.306\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk sample_docs_fair_forge_documentation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating queries for chunk: sample_docs_fair_forge_documentation\n",
      "Content preview: Fair Forge is a performance-measurement library for evaluating AI models and assistants....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 17:38:18,328 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:38:18,525 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-f9df89f3acc94af28814c3519ad0f867?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:38:18.739\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk sample_docs_fair_forge_documentation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 queries:\n",
      "\n",
      "1. What is the primary purpose of Fair Forge, a performance-measurement library?\n",
      "   Difficulty: easy\n",
      "   Type: factual\n",
      "\n",
      "2. How might Fair Forge be used to evaluate the performance of an AI model that has been trained on a dataset of customer service chat logs?\n",
      "   Difficulty: medium\n",
      "   Type: analytical\n",
      "\n",
      "3. Compare and contrast Fair Forge with other performance-measurement libraries used for AI model evaluation. What advantages does Fair Forge offer over its competitors?\n",
      "   Difficulty: hard\n",
      "   Type: comparative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate queries for a single chunk\n",
    "async def generate_from_chunk():\n",
    "    chunk = chunks[0]  # Use first chunk\n",
    "    print(f\"Generating queries for chunk: {chunk.chunk_id}\")\n",
    "    print(f\"Content preview: {chunk.content[:100]}...\\n\")\n",
    "\n",
    "    queries = await generator.generate_queries(\n",
    "        chunk=chunk,\n",
    "        num_queries=3,\n",
    "    )\n",
    "\n",
    "    print(f\"Generated {len(queries)} queries:\\n\")\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        print(f\"{i}. {q.query}\")\n",
    "        print(f\"   Difficulty: {q.difficulty}\")\n",
    "        print(f\"   Type: {q.query_type}\\n\")\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "queries = await generate_from_chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-dataset-header",
   "metadata": {},
   "source": [
    "## Step 6: Generate Complete Dataset\n",
    "\n",
    "Generate a complete test dataset from all chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "generate-dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:38:18.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1mLoading context from: sample_docs.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:18.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:18.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: sample_docs.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:18.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 5 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:18.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m527\u001b[0m - \u001b[1mLoaded 5 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:18.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m531\u001b[0m - \u001b[1mUsing chunk selection strategy: SequentialStrategy()\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:18.755\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk sample_docs_fair_forge_documentation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating complete dataset from markdown file...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 17:38:19,761 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:38:19,966 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-d16f602bf06f4bedb33922b80710a41d?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:38:20.275\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk sample_docs_fair_forge_documentation\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:20.276\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk sample_docs_key_features\u001b[0m\n",
      "2026-01-15 17:38:21,093 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:38:21,297 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-74423dfbeb6645d6818dca0b437edc48?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:38:21.606\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk sample_docs_key_features\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:21.607\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk sample_docs_getting_started\u001b[0m\n",
      "2026-01-15 17:38:22,629 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:38:22,834 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-bd5d6443cf9d480f909069878e8ff3a6?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:38:22.836\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk sample_docs_getting_started\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:22.837\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk sample_docs_basic_usage\u001b[0m\n",
      "2026-01-15 17:38:23,564 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:38:23,755 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-39101645508f474d95c6e2b40010cfe0?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:38:24.268\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk sample_docs_basic_usage\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:24.269\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 3 queries for chunk sample_docs_architecture\u001b[0m\n",
      "2026-01-15 17:38:25,186 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:38:25,394 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-6e3adbcdda504b2c9dff560c4355cdfc?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:38:25.702\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 3 queries for chunk sample_docs_architecture\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:25.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 15 batches for chunk group (5 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:38:25.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1mGenerated 1 dataset(s) with 15 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 dataset(s):\n",
      "  Session ID: 63ced9b6-0b65-4e87-8014-17d6dbe721cd\n",
      "  Assistant ID: test-assistant\n",
      "  Language: english\n",
      "  Total queries: 15\n",
      "  Context length: 1054 chars\n",
      "\n",
      "Sample queries:\n",
      "  - [sample_docs_fair_forge_documentation_q1] Explain how Fair Forge can be used to evaluate the performance of AI models.\n",
      "  - [sample_docs_fair_forge_documentation_q2] What are some potential advantages of using Fair Forge over other performance-measurement libraries?\n",
      "  - [sample_docs_fair_forge_documentation_q3] Describe a scenario where Fair Forge would be particularly useful for evaluating an AI assistant.\n",
      "  - [sample_docs_key_features_q1] What are some of the key metrics provided by the library for evaluating AI performance?\n",
      "  - [sample_docs_key_features_q2] How might the library's fairness metric be used to identify biases in a chatbot's responses to users from different age groups?\n"
     ]
    }
   ],
   "source": [
    "# Generate complete dataset\n",
    "async def generate_full_dataset():\n",
    "    print(\"Generating complete dataset from markdown file...\\n\")\n",
    "\n",
    "    # generate_dataset returns list[Dataset]\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"test-assistant\",\n",
    "        num_queries_per_chunk=3,\n",
    "        language=\"english\",\n",
    "    )\n",
    "\n",
    "    # With default SequentialStrategy, we get one dataset\n",
    "    dataset = datasets[0]\n",
    "\n",
    "    print(f\"Generated {len(datasets)} dataset(s):\")\n",
    "    print(f\"  Session ID: {dataset.session_id}\")\n",
    "    print(f\"  Assistant ID: {dataset.assistant_id}\")\n",
    "    print(f\"  Language: {dataset.language}\")\n",
    "    print(f\"  Total queries: {len(dataset.conversation)}\")\n",
    "    print(f\"  Context length: {len(dataset.context)} chars\\n\")\n",
    "\n",
    "    print(\"Sample queries:\")\n",
    "    for batch in dataset.conversation[:5]:\n",
    "        print(f\"  - [{batch.qa_id}] {batch.query}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "datasets = await generate_full_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seed-examples-header",
   "metadata": {},
   "source": [
    "## Step 7: Generate with Seed Examples\n",
    "\n",
    "Guide the query generation style using seed examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "seed-examples",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:42:43.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1mLoading context from: sample_docs.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:43.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:43.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: sample_docs.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:43.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 5 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:43.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m527\u001b[0m - \u001b[1mLoaded 5 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:43.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m531\u001b[0m - \u001b[1mUsing chunk selection strategy: SequentialStrategy()\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:43.780\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk sample_docs_fair_forge_documentation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with seed examples...\n",
      "Seed examples provided: 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 17:42:44,679 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:42:44,884 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-8a813f3f1e204cfcb1246fd452640279?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:42:45.089\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk sample_docs_fair_forge_documentation\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:45.090\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk sample_docs_key_features\u001b[0m\n",
      "2026-01-15 17:42:46,112 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:42:46,318 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-cf4887e7f62b423182e7382698a97fa8?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:42:46.522\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk sample_docs_key_features\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:46.524\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk sample_docs_getting_started\u001b[0m\n",
      "2026-01-15 17:42:47,465 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:42:47,748 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-e30c3941ec534721bd59fcfc9c0cb55f?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:42:47.957\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk sample_docs_getting_started\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:47.958\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk sample_docs_basic_usage\u001b[0m\n",
      "2026-01-15 17:42:48,981 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:42:49,184 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-3b9b04c9be9943d39120c3f30c0910b5?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:42:49.391\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk sample_docs_basic_usage\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:49.392\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk sample_docs_architecture\u001b[0m\n",
      "2026-01-15 17:42:50,413 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:42:50,615 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-71cb038d3ec348309542ac4a99db3325?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:42:50.931\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk sample_docs_architecture\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:50.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 10 batches for chunk group (5 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:42:50.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1mGenerated 1 dataset(s) with 10 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 queries\n"
     ]
    }
   ],
   "source": [
    "# Generate with seed examples for style guidance\n",
    "async def generate_with_seeds():\n",
    "    seed_examples = [\n",
    "        \"What are the main components of Fair Forge's architecture?\",\n",
    "        \"How can I measure bias in my AI assistant's responses?\",\n",
    "        \"What steps are needed to integrate Fair Forge with an existing pipeline?\",\n",
    "    ]\n",
    "\n",
    "    print(\"Generating with seed examples...\")\n",
    "    print(f\"Seed examples provided: {len(seed_examples)}\\n\")\n",
    "\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"test-assistant\",\n",
    "        num_queries_per_chunk=2,\n",
    "        language=\"english\",\n",
    "        seed_examples=seed_examples,\n",
    "    )\n",
    "\n",
    "    dataset = datasets[0]\n",
    "    print(f\"Generated {len(dataset.conversation)} queries\")\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "datasets_with_seeds = await generate_with_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategies-header",
   "metadata": {},
   "source": [
    "## Chunk Selection Strategies\n",
    "\n",
    "Strategies control how chunks are selected and grouped during generation.\n",
    "\n",
    "### RandomSamplingStrategy\n",
    "\n",
    "Randomly samples chunks multiple times to create diverse test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "random-sampling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:39:14.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1mLoading context from: sample_docs.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:14.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:14.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: sample_docs.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:14.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 5 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:14.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m527\u001b[0m - \u001b[1mLoaded 5 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:14.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m531\u001b[0m - \u001b[1mUsing chunk selection strategy: RandomSamplingStrategy(num_samples=2, chunks_per_sample=2, seed=42)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:14.175\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk sample_docs_fair_forge_documentation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: RandomSamplingStrategy(num_samples=2, chunks_per_sample=2, seed=42)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 17:39:15,195 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:39:15,375 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-d8fcf3509e084a2aab91508210c3d904?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:39:15.556\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk sample_docs_fair_forge_documentation\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:15.557\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk sample_docs_architecture\u001b[0m\n",
      "2026-01-15 17:39:16,455 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:39:16,643 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-dfe366d6dae7478cbe4429858c73b2dd?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:39:17.057\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk sample_docs_architecture\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:17.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 4 batches for chunk group (2 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:17.059\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk sample_docs_getting_started\u001b[0m\n",
      "2026-01-15 17:39:17,959 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:39:18,141 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-c77056c360954eb69f0a9d3e9a7cdd04?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:39:18.421\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk sample_docs_getting_started\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:18.422\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m493\u001b[0m - \u001b[34m\u001b[1mGenerating 2 queries for chunk sample_docs_key_features\u001b[0m\n",
      "2026-01-15 17:39:19,079 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:39:19,258 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-c179edcec6864f358e26080285b980ee?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:39:19.669\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_queries\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mGenerated 2 queries for chunk sample_docs_key_features\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:19.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 4 batches for chunk group (2 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:39:19.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1mGenerated 2 dataset(s) with 8 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 datasets:\n",
      "\n",
      "Dataset 1: 4 queries\n",
      "Dataset 2: 4 queries\n"
     ]
    }
   ],
   "source": [
    "async def generate_with_random_sampling():\n",
    "    \"\"\"Generate multiple datasets using random chunk sampling.\"\"\"\n",
    "\n",
    "    strategy = RandomSamplingStrategy(\n",
    "        num_samples=2,  # Create 2 datasets\n",
    "        chunks_per_sample=2,  # Each with 2 random chunks\n",
    "        seed=42,  # For reproducibility\n",
    "    )\n",
    "\n",
    "    print(f\"Strategy: {strategy}\\n\")\n",
    "\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"test-assistant\",\n",
    "        num_queries_per_chunk=2,\n",
    "        selection_strategy=strategy,\n",
    "    )\n",
    "\n",
    "    print(f\"Generated {len(datasets)} datasets:\\n\")\n",
    "    for i, ds in enumerate(datasets):\n",
    "        print(f\"Dataset {i+1}: {len(ds.conversation)} queries\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "random_datasets = await generate_with_random_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation-header",
   "metadata": {},
   "source": [
    "## Conversation Mode\n",
    "\n",
    "Generate coherent multi-turn conversations where each question builds on the previous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "conversation-mode",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 17:40:01.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1mLoading context from: sample_docs.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:01.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mLoading 1 markdown file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:01.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36m_load_single_file\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mLoading markdown file: sample_docs.md\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:01.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.generators.context_loaders.local_markdown\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mCreated 5 total chunks from 1 file(s)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:01.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m527\u001b[0m - \u001b[1mLoaded 5 chunks from source\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:01.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m531\u001b[0m - \u001b[1mUsing chunk selection strategy: SequentialStrategy()\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:01.978\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m549\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk sample_docs_fair_forge_documentation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating conversations (each turn builds on the previous)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 17:40:03,293 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:40:03,492 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-4d60dbf5d38348ed9a083cec55aa7c24?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:40:03.804\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m575\u001b[0m - \u001b[34m\u001b[1mGenerated 1 turns for chunk sample_docs_fair_forge_documentation\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:03.805\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m549\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk sample_docs_key_features\u001b[0m\n",
      "2026-01-15 17:40:04,643 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:40:04,929 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-3cbddd3e98d645eba667c5487f4af667?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:40:05.339\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m575\u001b[0m - \u001b[34m\u001b[1mGenerated 1 turns for chunk sample_docs_key_features\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:05.340\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m549\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk sample_docs_getting_started\u001b[0m\n",
      "2026-01-15 17:40:06,155 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:40:06,362 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-abafdf8e28b544e98bd636844129bc98?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:40:06.671\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m575\u001b[0m - \u001b[34m\u001b[1mGenerated 1 turns for chunk sample_docs_getting_started\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:06.672\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m549\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk sample_docs_basic_usage\u001b[0m\n",
      "2026-01-15 17:40:07,591 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:40:08,002 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-bfe24dd0ee294198a61356aa4984774c?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:40:08.207\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m575\u001b[0m - \u001b[34m\u001b[1mGenerated 1 turns for chunk sample_docs_basic_usage\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:08.208\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m549\u001b[0m - \u001b[34m\u001b[1mGenerating 3-turn conversation for chunk sample_docs_architecture\u001b[0m\n",
      "2026-01-15 17:40:08,945 - httpx - INFO - HTTP Request: POST https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/infer/chat/test-generator?chat_history=50&agentspace=_default \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 17:40:09,230 - httpx - INFO - HTTP Request: GET https://alquimia-hermes-alquimia-runtime.apps.rosa.alquimia.zvb4.p3.openshiftapps.com/event/stream/task-1dfd7c1e03c54746968977bd5c8c80c5?response_only=true \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-01-15 17:40:09.641\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfair_forge.generators.alquimia_generator\u001b[0m:\u001b[36mgenerate_conversation\u001b[0m:\u001b[36m575\u001b[0m - \u001b[34m\u001b[1mGenerated 1 turns for chunk sample_docs_architecture\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:09.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m586\u001b[0m - \u001b[1mGenerated 5 batches for chunk group (5 chunks)\u001b[0m\n",
      "\u001b[32m2026-01-15 17:40:09.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfair_forge.schemas.generators\u001b[0m:\u001b[36mgenerate_dataset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1mGenerated 1 dataset(s) with 5 total batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 conversation turns:\n",
      "\n",
      "\n",
      "--- Conversation for: sample_docs_fair_forge_documentation ---\n",
      "  Turn 1: {\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"What is the primary purpose of Fair Forge?\",\n",
      "            \"difficulty\": \"easy\",\n",
      "            \"query_type\": \"factual\"\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"How does Fair Forge contribute to the evaluation of AI models and assistants?\",\n",
      "            \"difficulty\": \"medium\",\n",
      "            \"query_type\": \"inferential\"\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"Design a scenario where Fair Forge could be used to compare the performance of two AI-powered virtual assistants.\",\n",
      "            \"difficulty\": \"hard\",\n",
      "            \"query_type\": \"analytical\"\n",
      "        }\n",
      "    ],\n",
      "    \"chunk_summary\": \"Fair Forge is a performance-measurement library used for evaluating AI models and assistants, enabling accurate and comprehensive assessments of their capabilities.\"\n",
      "}\n",
      "\n",
      "--- Conversation for: sample_docs_key_features ---\n",
      "  Turn 1: {\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"What metrics does the library provide for evaluating AI systems?\",\n",
      "            \"difficulty\": \"easy\",\n",
      "            \"query_type\": \"factual\"\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"How might a library like this help developers identify and mitigate bias in their models?\",\n",
      "            \"difficulty\": \"medium\",\n",
      "            \"query_type\": \"inferential\"\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"Compare the library's approach to evaluating conversational quality with other existing methods. What advantages does it offer?\",\n",
      "            \"difficulty\": \"hard\",\n",
      "            \"query_type\": \"comparative\"\n",
      "        }\n",
      "    ],\n",
      "    \"chunk_summary\": \"The library provides metrics for evaluating AI systems, including fairness, toxicity, conversational quality, and context adherence, to help developers assess and improve their models.\"\n",
      "}\n",
      "\n",
      "--- Conversation for: sample_docs_getting_started ---\n",
      "  Turn 1: {\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"What is the command to install Fair Forge using pip?\",\n",
      "            \"difficulty\": \"easy\",\n",
      "            \"query_type\": \"factual\"\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"Describe the steps to get started with Fair Forge. How does it help in loading test datasets?\",\n",
      "            \"difficulty\": \"medium\",\n",
      "            \"query_type\": \"comparative\"\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"Explain how to use the retriever in Fair Forge to run metrics after loading test datasets. What are the benefits of this approach?\",\n",
      "            \"difficulty\": \"hard\",\n",
      "            \"query_type\": \"analytical\"\n",
      "        }\n",
      "    ],\n",
      "    \"chunk_summary\": \"To get started with Fair Forge, you need to install the package using pip and then create a retriever to load your test datasets and run metrics.\"\n",
      "}\n",
      "\n",
      "--- Conversation for: sample_docs_basic_usage ---\n",
      "  Turn 1: {\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"What is the purpose of the Toxicity metric in the provided code snippet?\",\n",
      "            \"difficulty\": \"easy\",\n",
      "            \"query_type\": \"factual\"\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"Explain the role of the MyRetriever object in the context of the Toxicity metric.\",\n",
      "            \"difficulty\": \"medium\",\n",
      "            \"query_type\": \"analytical\"\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"Suppose you want to modify the code to use a different retriever. How would you do it, and what changes would you need to make?\",\n",
      "            \"difficulty\": \"hard\",\n",
      "            \"query_type\": \"comparative\"\n",
      "        }\n",
      "    ],\n",
      "    \"chunk_summary\": \"The provided code snippet demonstrates the usage of the Toxicity metric from the fair_forge.metrics module to evaluate the toxicity of a retriever's results.\"\n",
      "}\n",
      "\n",
      "--- Conversation for: sample_docs_architecture ---\n",
      "  Turn 1: {\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"What are the primary components of Fair Forge's modular architecture?\",\n",
      "            \"difficulty\": \"easy\",\n",
      "            \"query_type\": \"factual\"\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"How does the Core component of Fair Forge's architecture facilitate customization?\",\n",
      "            \"difficulty\": \"medium\",\n",
      "            \"query_type\": \"inferential\"\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"Compare and contrast the roles of Metrics and Runners in Fair Forge's architecture.\",\n",
      "            \"difficulty\": \"hard\",\n",
      "            \"query_type\": \"comparative\"\n",
      "        }\n",
      "    ],\n",
      "    \"chunk_summary\": \"Fair Forge's modular architecture consists of Core, Metrics, Runners, and Storage components, allowing for customization and extension of each module to support custom implementations.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "async def generate_conversations():\n",
    "    \"\"\"Generate coherent multi-turn conversations.\"\"\"\n",
    "\n",
    "    print(\"Generating conversations (each turn builds on the previous)...\\n\")\n",
    "\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"test-assistant\",\n",
    "        num_queries_per_chunk=3,  # 3-turn conversations\n",
    "        conversation_mode=True,  # Enable conversation mode\n",
    "    )\n",
    "\n",
    "    dataset = datasets[0]\n",
    "    print(f\"Generated {len(dataset.conversation)} conversation turns:\\n\")\n",
    "\n",
    "    # Group by chunk to show conversation flow\n",
    "    current_chunk = None\n",
    "    for batch in dataset.conversation:\n",
    "        chunk_id = batch.agentic.get(\"chunk_id\", \"N/A\")\n",
    "        turn_num = batch.agentic.get(\"turn_number\", 0)\n",
    "\n",
    "        if chunk_id != current_chunk:\n",
    "            print(f\"\\n--- Conversation for: {chunk_id} ---\")\n",
    "            current_chunk = chunk_id\n",
    "\n",
    "        print(f\"  Turn {turn_num}: {batch.query}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "conversation_datasets = await generate_conversations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-prompt-header",
   "metadata": {},
   "source": [
    "## Note on Custom System Prompts\n",
    "\n",
    "**Important:** The AlquimiaGenerator does not support custom system prompts in the same way as direct LangChain models, because the agent's system prompt is configured in the Alquimia workspace.\n",
    "\n",
    "Instead, you can:\n",
    "1. Use **seed examples** to guide the style of generated queries\n",
    "2. Configure the agent's system prompt directly in your Alquimia workspace to accept `context`, `num_queries`, and `seed_examples` as template variables\n",
    "\n",
    "For full control over the system prompt, use a LangChain model directly with `BaseGenerator` (see Groq example notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-dataset-header",
   "metadata": {},
   "source": [
    "## Step 8: Save Generated Dataset\n",
    "\n",
    "Save the generated dataset to JSON for use with runners and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "save-dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to: generated_tests.json\n",
      "Total queries: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('generated_tests.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save generated dataset to JSON\n",
    "async def save_dataset(dataset: Dataset, output_path: str):\n",
    "    output_file = Path(output_path)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(dataset.model_dump(), f, indent=2)\n",
    "\n",
    "    print(f\"Dataset saved to: {output_file}\")\n",
    "    print(f\"Total queries: {len(dataset.conversation)}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "# Example usage (uncomment after generating dataset)\n",
    "await save_dataset(datasets[0], \"./generated_tests.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integration-header",
   "metadata": {},
   "source": [
    "## Step 9: Integration with Runners\n",
    "\n",
    "Use the generated dataset with Fair Forge runners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "integration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration example ready (uncomment to use)\n"
     ]
    }
   ],
   "source": [
    "# Example integration with runners\n",
    "# from fair_forge.runners import AlquimiaRunner\n",
    "# from fair_forge.storage import create_local_storage\n",
    "\n",
    "\n",
    "async def run_generated_tests(dataset: Dataset):\n",
    "    \"\"\"\n",
    "    Example of running generated tests against an AI assistant.\n",
    "\n",
    "    Uncomment and configure to use.\n",
    "    \"\"\"\n",
    "    # # Configure runner\n",
    "    # runner = AlquimiaRunner(\n",
    "    #     base_url=os.getenv(\"ALQUIMIA_URL\"),\n",
    "    #     api_key=os.getenv(\"ALQUIMIA_API_KEY\"),\n",
    "    #     agent_id=os.getenv(\"AGENT_ID\"),\n",
    "    #     channel_id=os.getenv(\"CHANNEL_ID\"),\n",
    "    # )\n",
    "    #\n",
    "    # # Run dataset\n",
    "    # updated_dataset, summary = await runner.run_dataset(dataset)\n",
    "    #\n",
    "    # print(f\"Completed: {summary['successes']}/{summary['total_batches']} passed\")\n",
    "    # return updated_dataset\n",
    "\n",
    "\n",
    "print(\"Integration example ready (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-loader-header",
   "metadata": {},
   "source": [
    "## Creating Custom Context Loaders\n",
    "\n",
    "You can create custom context loaders by extending `BaseContextLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "custom-loader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom JsonContextLoader defined\n"
     ]
    }
   ],
   "source": [
    "from fair_forge.schemas.generators import BaseContextLoader, Chunk\n",
    "\n",
    "\n",
    "class JsonContextLoader(BaseContextLoader):\n",
    "    \"\"\"Example custom loader for JSON documents.\"\"\"\n",
    "\n",
    "    def load(self, source: str) -> list[Chunk]:\n",
    "        \"\"\"Load and chunk a JSON file.\"\"\"\n",
    "        import json\n",
    "        from pathlib import Path\n",
    "\n",
    "        path = Path(source)\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        chunks = []\n",
    "        # Example: each top-level key becomes a chunk\n",
    "        for i, (key, value) in enumerate(data.items()):\n",
    "            content = f\"{key}: {json.dumps(value, indent=2)}\"\n",
    "            chunks.append(\n",
    "                Chunk(\n",
    "                    content=content,\n",
    "                    chunk_id=f\"json_{key}\",\n",
    "                    metadata={\"key\": key, \"source\": str(path)},\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "print(\"Custom JsonContextLoader defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample files cleaned up\n"
     ]
    }
   ],
   "source": [
    "# Clean up sample files\n",
    "if sample_file.exists():\n",
    "    sample_file.unlink()\n",
    "    print(\"Sample files cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
