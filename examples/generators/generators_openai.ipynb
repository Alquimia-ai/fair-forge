{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Fair Forge Generators - OpenAI Example\n",
    "\n",
    "This notebook demonstrates how to use the Fair Forge generators module with **OpenAI** models to create synthetic test datasets.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `BaseGenerator` class accepts any LangChain-compatible chat model, including OpenAI's `ChatOpenAI`. This provides a flexible interface for generating test queries from context documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": "## Setup\n\nSet your OpenAI API key as an environment variable:\n\n```bash\nexport OPENAI_API_KEY=\"your-api-key\"\n```\n\nOr create a `.env` file:\n```.env\nOPENAI_API_KEY=your-api-key\n```\n\nInstall required dependencies:\n```bash\nuv venv\nsource .venv/bin/activate\n\n# Install fair-forge (core package)\nuv pip install alquimia-fair-forge\n\n# Install OpenAI LangChain integration\nuv pip install langchain-openai python-dotenv\n\nuv run jupyter lab\n```\n\nIf you're already in Jupyter and install packages, **restart the kernel** for changes to take effect."
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from fair_forge.generators import (\n",
    "    BaseGenerator,\n",
    "    create_markdown_loader,\n",
    "    # Strategies for chunk selection\n",
    "    SequentialStrategy,\n",
    "    RandomSamplingStrategy,\n",
    ")\n",
    "from fair_forge.schemas import Dataset, Batch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-content-header",
   "metadata": {},
   "source": [
    "## Create Sample Content\n",
    "\n",
    "Let's create a sample markdown document to generate test queries from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_content = \"\"\"# AI Assistant Guidelines\n",
    "\n",
    "This document outlines best practices for building AI assistants.\n",
    "\n",
    "## Core Principles\n",
    "\n",
    "AI assistants should follow these core principles:\n",
    "\n",
    "1. **Helpfulness**: Always aim to provide useful and accurate information\n",
    "2. **Safety**: Never provide harmful or dangerous advice\n",
    "3. **Honesty**: Be transparent about limitations and uncertainties\n",
    "4. **Privacy**: Respect user privacy and data protection\n",
    "\n",
    "## Response Quality\n",
    "\n",
    "High-quality responses should:\n",
    "- Be clear and concise\n",
    "- Address the user's actual question\n",
    "- Provide relevant context when needed\n",
    "- Cite sources for factual claims\n",
    "\n",
    "## Error Handling\n",
    "\n",
    "When the assistant encounters errors or edge cases:\n",
    "- Acknowledge the limitation clearly\n",
    "- Suggest alternative approaches\n",
    "- Never make up information\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "sample_file = Path(\"./sample_guidelines.md\")\n",
    "sample_file.write_text(sample_content)\n",
    "print(f\"Sample content saved to: {sample_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loader-header",
   "metadata": {},
   "source": [
    "## Create Context Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create markdown loader with hybrid chunking\n",
    "loader = create_markdown_loader(\n",
    "    max_chunk_size=2000,\n",
    "    header_levels=[1, 2, 3],\n",
    ")\n",
    "\n",
    "# Preview the chunks\n",
    "chunks = loader.load(str(sample_file))\n",
    "print(f\"Created {len(chunks)} chunks:\\n\")\n",
    "for chunk in chunks:\n",
    "    print(f\"- {chunk.chunk_id}: {len(chunk.content)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-header",
   "metadata": {},
   "source": [
    "## Create Generator with OpenAI Model\n",
    "\n",
    "The `BaseGenerator` accepts any LangChain-compatible chat model. Here we use `ChatOpenAI` from `langchain-openai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OpenAI model using LangChain\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo\n",
    "    temperature=0.7,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "# Create generator with the model\n",
    "generator = BaseGenerator(\n",
    "    model=model,\n",
    "    use_structured_output=True,  # Use OpenAI's native structured output\n",
    ")\n",
    "\n",
    "print(f\"Generator created with model: {model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-header",
   "metadata": {},
   "source": [
    "## Generate Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complete dataset\n",
    "async def generate_dataset():\n",
    "    print(\"Generating test dataset with OpenAI...\\n\")\n",
    "    \n",
    "    # generate_dataset returns list[Dataset]\n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"my-assistant\",\n",
    "        num_queries_per_chunk=3,\n",
    "        language=\"english\",\n",
    "    )\n",
    "    \n",
    "    # With default SequentialStrategy, we get one dataset\n",
    "    dataset = datasets[0]\n",
    "    \n",
    "    print(f\"Generated {len(datasets)} dataset(s):\")\n",
    "    print(f\"  Session ID: {dataset.session_id}\")\n",
    "    print(f\"  Total queries: {len(dataset.conversation)}\\n\")\n",
    "    \n",
    "    print(\"Generated queries:\")\n",
    "    for batch in dataset.conversation:\n",
    "        difficulty = batch.agentic.get('difficulty', 'N/A')\n",
    "        query_type = batch.agentic.get('query_type', 'N/A')\n",
    "        print(f\"  [{batch.qa_id}] ({difficulty}/{query_type})\")\n",
    "        print(f\"    {batch.query}\\n\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute\n",
    "datasets = await generate_dataset()\n",
    "dataset = datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seed-examples-header",
   "metadata": {},
   "source": [
    "## Generate with Seed Examples\n",
    "\n",
    "Guide the style of generated questions using seed examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seed-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_with_seeds():\n",
    "    seed_examples = [\n",
    "        \"What should an AI assistant do when it encounters a harmful request?\",\n",
    "        \"How can an assistant ensure its responses respect user privacy?\",\n",
    "        \"What are the key characteristics of a high-quality AI response?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"Generating with seed examples...\\n\")\n",
    "    \n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"my-assistant\",\n",
    "        num_queries_per_chunk=2,\n",
    "        seed_examples=seed_examples,\n",
    "    )\n",
    "    \n",
    "    dataset = datasets[0]\n",
    "    print(f\"Generated {len(dataset.conversation)} queries with seed examples\")\n",
    "    for batch in dataset.conversation[:3]:\n",
    "        print(f\"  - {batch.query}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute\n",
    "datasets_with_seeds = await generate_with_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategies-header",
   "metadata": {},
   "source": [
    "## Chunk Selection Strategies\n",
    "\n",
    "Strategies control how chunks are selected and grouped during generation.\n",
    "\n",
    "### RandomSamplingStrategy\n",
    "\n",
    "Randomly samples chunks multiple times to create diverse test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_with_random_sampling():\n",
    "    \"\"\"Generate multiple datasets using random chunk sampling.\"\"\"\n",
    "    \n",
    "    strategy = RandomSamplingStrategy(\n",
    "        num_samples=2,       # Create 2 datasets\n",
    "        chunks_per_sample=2, # Each with 2 random chunks\n",
    "        seed=42,             # For reproducibility\n",
    "    )\n",
    "    \n",
    "    print(f\"Strategy: {strategy}\\n\")\n",
    "    \n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"my-assistant\",\n",
    "        num_queries_per_chunk=2,\n",
    "        selection_strategy=strategy,\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(datasets)} datasets:\\n\")\n",
    "    for i, ds in enumerate(datasets):\n",
    "        print(f\"Dataset {i+1}: {len(ds.conversation)} queries\")\n",
    "        chunk_ids = set(b.agentic.get('chunk_id', 'N/A') for b in ds.conversation)\n",
    "        print(f\"  Chunks: {chunk_ids}\\n\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute\n",
    "random_datasets = await generate_with_random_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation-header",
   "metadata": {},
   "source": [
    "## Conversation Mode\n",
    "\n",
    "Generate coherent multi-turn conversations where each question builds on the previous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conversation-mode",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_conversations():\n",
    "    \"\"\"Generate coherent multi-turn conversations.\"\"\"\n",
    "    \n",
    "    print(\"Generating conversations (each turn builds on the previous)...\\n\")\n",
    "    \n",
    "    datasets = await generator.generate_dataset(\n",
    "        context_loader=loader,\n",
    "        source=str(sample_file),\n",
    "        assistant_id=\"my-assistant\",\n",
    "        num_queries_per_chunk=3,  # 3-turn conversations\n",
    "        conversation_mode=True,   # Enable conversation mode\n",
    "    )\n",
    "    \n",
    "    dataset = datasets[0]\n",
    "    print(f\"Generated {len(dataset.conversation)} conversation turns:\\n\")\n",
    "    \n",
    "    # Group by chunk to show conversation flow\n",
    "    current_chunk = None\n",
    "    for batch in dataset.conversation:\n",
    "        chunk_id = batch.agentic.get('chunk_id', 'N/A')\n",
    "        turn_num = batch.agentic.get('turn_number', 0)\n",
    "        \n",
    "        if chunk_id != current_chunk:\n",
    "            print(f\"\\n--- Conversation for: {chunk_id} ---\")\n",
    "            current_chunk = chunk_id\n",
    "        \n",
    "        print(f\"  Turn {turn_num}: {batch.query}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execute\n",
    "conversation_datasets = await generate_conversations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to JSON\n",
    "output_file = Path(\"./generated_tests_openai.json\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(dataset.model_dump(), f, indent=2)\n",
    "\n",
    "print(f\"Dataset saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## Available OpenAI Models\n",
    "\n",
    "| Model | Description | Best For |\n",
    "|-------|-------------|----------|\n",
    "| `gpt-4o` | Most capable, multimodal | Complex reasoning |\n",
    "| `gpt-4o-mini` | Fast and cost-effective | General tasks |\n",
    "| `gpt-4-turbo` | Previous flagship | Longer context |\n",
    "| `gpt-3.5-turbo` | Fast and cheap | Simple tasks |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "try-different-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Use GPT-4o for higher quality generation\n",
    "# model_gpt4 = ChatOpenAI(\n",
    "#     model=\"gpt-4o\",\n",
    "#     temperature=0.5,  # Lower temperature for more focused output\n",
    "# )\n",
    "# generator_gpt4 = BaseGenerator(model=model_gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up sample files\n",
    "if sample_file.exists():\n",
    "    sample_file.unlink()\n",
    "if output_file.exists():\n",
    "    output_file.unlink()\n",
    "print(\"Cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}