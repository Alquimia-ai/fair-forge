{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# BestOf Metric Example\n",
    "\n",
    "This notebook demonstrates how to use the **BestOf** metric from Fair Forge to run tournament-style comparisons between multiple AI assistants.\n",
    "\n",
    "The metric:\n",
    "- Pairs up assistants in elimination rounds\n",
    "- Uses an LLM judge to evaluate each matchup\n",
    "- Advances winners until a final champion is determined\n",
    "- Handles ties (both advance) and byes (odd number of contestants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install Fair Forge and the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!uv pip install --python {sys.executable} --force-reinstall \"$(ls ../../dist/*.whl)[bestof]\" langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the required modules and configure your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from fair_forge.metrics.best_of import BestOf\n",
    "from fair_forge import Retriever\n",
    "from fair_forge.schemas import Dataset\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Create a Multi-Assistant Retriever\n",
    "\n",
    "The BestOf metric requires datasets with multiple `assistant_id` values to compare. We'll use a custom dataset with 4 assistants of varying quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestOfRetriever(Retriever):\n",
    "    \"\"\"Retriever that loads the multi-assistant dataset for BestOf comparison.\"\"\"\n",
    "\n",
    "    def load_dataset(self) -> list[Dataset]:\n",
    "        dataset_path = Path(\"dataset_bestof.json\")\n",
    "        datasets = []\n",
    "        with open(dataset_path) as infile:\n",
    "            for dataset in json.load(infile):\n",
    "                datasets.append(Dataset.model_validate(dataset))\n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Preview the Dataset\n",
    "\n",
    "Let's see which assistants we'll be comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BestOfRetriever()\n",
    "datasets = retriever.load_dataset()\n",
    "\n",
    "print(f\"Number of assistants: {len(datasets)}\")\n",
    "print(f\"\\nAssistants in tournament:\")\n",
    "for ds in datasets:\n",
    "    print(f\"  - {ds.assistant_id}: {len(ds.conversation)} conversations\")\n",
    "    \n",
    "print(f\"\\nSample question: {datasets[0].conversation[0].query}\")\n",
    "print(f\"\\nResponses from each assistant:\")\n",
    "for ds in datasets:\n",
    "    print(f\"\\n{ds.assistant_id}:\")\n",
    "    print(f\"  {ds.conversation[0].assistant[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Initialize the Judge Model\n",
    "\n",
    "The BestOf metric uses an LLM as a judge to compare pairs of assistants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    api_key=GROQ_API_KEY,\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Run the Tournament\n",
    "\n",
    "The BestOf metric will run elimination rounds until one assistant is crowned the winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = BestOf.run(\n",
    "    BestOfRetriever,\n",
    "    model=judge_model,\n",
    "    use_structured_output=True,\n",
    "    criteria=\"Overall response quality, helpfulness, and clarity\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Analyze Tournament Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament = metrics[0]\n",
    "\n",
    "print(f\"Tournament Winner: {tournament.bestof_winner_id}\")\n",
    "print(f\"\\nTotal Contests: {len(tournament.bestof_contests)}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "rounds = {}\n",
    "for contest in tournament.bestof_contests:\n",
    "    if contest.round not in rounds:\n",
    "        rounds[contest.round] = []\n",
    "    rounds[contest.round].append(contest)\n",
    "\n",
    "for round_num in sorted(rounds.keys()):\n",
    "    print(f\"\\nRound {round_num}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for contest in rounds[round_num]:\n",
    "        print(f\"  {contest.left_id} vs {contest.right_id}\")\n",
    "        print(f\"    Winner: {contest.winner_id}\")\n",
    "        print(f\"    Confidence: {contest.confidence}\")\n",
    "        print(f\"    Verdict: {contest.verdict}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Visualize the Tournament Bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "num_rounds = max(c.round for c in tournament.bestof_contests)\n",
    "contestants = sorted({ds.assistant_id for ds in datasets})\n",
    "\n",
    "round_x = {}\n",
    "for r in range(1, num_rounds + 2):\n",
    "    round_x[r] = r * 3\n",
    "\n",
    "y_positions = {}\n",
    "for i, contestant in enumerate(contestants):\n",
    "    y_positions[contestant] = (i + 0.5) * 2\n",
    "\n",
    "for i, contestant in enumerate(contestants):\n",
    "    ax.text(0.5, y_positions[contestant], contestant, \n",
    "            ha='right', va='center', fontsize=10, fontweight='bold')\n",
    "    ax.plot([0.6, 1], [y_positions[contestant], y_positions[contestant]], 'k-', linewidth=1)\n",
    "\n",
    "colors = {'winner': 'green', 'loser': 'red', 'tie': 'orange'}\n",
    "\n",
    "for contest in tournament.bestof_contests:\n",
    "    x = round_x[contest.round]\n",
    "    y1 = y_positions[contest.left_id]\n",
    "    y2 = y_positions[contest.right_id]\n",
    "    y_mid = (y1 + y2) / 2\n",
    "    \n",
    "    left_color = colors['winner'] if contest.winner_id == contest.left_id else colors['loser']\n",
    "    right_color = colors['winner'] if contest.winner_id == contest.right_id else colors['loser']\n",
    "    \n",
    "    ax.plot([x-1, x], [y1, y1], color=left_color, linewidth=2)\n",
    "    ax.plot([x-1, x], [y2, y2], color=right_color, linewidth=2)\n",
    "    ax.plot([x, x], [y1, y2], 'k-', linewidth=1)\n",
    "    \n",
    "    ax.annotate(f'R{contest.round}', (x-0.5, y_mid), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.text(round_x[num_rounds] + 1.5, sum(y_positions.values())/len(y_positions), \n",
    "        f\"Winner:\\n{tournament.bestof_winner_id}\", \n",
    "        ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='gold', alpha=0.8))\n",
    "\n",
    "ax.set_xlim(-0.5, round_x[num_rounds] + 3)\n",
    "ax.set_ylim(0, max(y_positions.values()) + 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('Tournament Bracket', fontsize=14, fontweight='bold')\n",
    "\n",
    "winner_patch = mpatches.Patch(color='green', label='Winner')\n",
    "loser_patch = mpatches.Patch(color='red', label='Loser')\n",
    "ax.legend(handles=[winner_patch, loser_patch], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Review Judge Reasoning\n",
    "\n",
    "Let's examine the judge's reasoning for each matchup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contest in tournament.bestof_contests:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Round {contest.round}: {contest.left_id} vs {contest.right_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nWinner: {contest.winner_id}\")\n",
    "    print(f\"Confidence: {contest.confidence}\")\n",
    "    print(f\"\\nVerdict: {contest.verdict}\")\n",
    "    print(f\"\\nReasoning: {contest.reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "### Tournament Structure\n",
    "\n",
    "- **Round 1**: Initial pairings (4 assistants → 2 winners)\n",
    "- **Round 2**: Finals (2 assistants → 1 champion)\n",
    "\n",
    "### Contest Output\n",
    "\n",
    "Each contest provides:\n",
    "- `winner_id`: The assistant that won this matchup\n",
    "- `confidence`: Judge's confidence in the decision\n",
    "- `verdict`: Brief summary of why this assistant won\n",
    "- `reasoning`: Detailed analysis of both assistants' performance\n",
    "\n",
    "### Special Cases\n",
    "\n",
    "- **Ties**: Both assistants advance to the next round\n",
    "- **Byes**: With odd contestants, one gets a free pass to the next round\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Model Selection**: Compare multiple LLMs to find the best for your use case\n",
    "- **A/B Testing**: Evaluate different prompt strategies or configurations\n",
    "- **Quality Benchmarking**: Establish baseline quality across assistant versions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
