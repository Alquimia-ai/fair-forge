{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Metric Example\n",
    "\n",
    "This notebook demonstrates how to use the **Bias** metric from Fair Forge to detect and analyze bias in AI assistant responses across protected attributes like gender, race, religion, nationality, and sexual orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install Fair Forge and the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\n!uv pip install --python {sys.executable} --force-reinstall \"$(ls ../../dist/*.whl)[bias]\" -q"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the required modules and configure your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\n\n# Add examples directory to path for helpers import\nsys.path.insert(0, os.path.dirname(os.getcwd()))\n\nfrom fair_forge.metrics.bias import Bias\nfrom fair_forge.guardians import IBMGranite\nfrom fair_forge.guardians.llms.providers import OpenAIGuardianProvider\nfrom fair_forge.schemas.bias import GuardianLLMConfig\nfrom helpers.retriever import LocalRetriever"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your Guardian model credentials\n",
    "# The Guardian uses an LLM endpoint (e.g., IBM Granite or LLamaGuard) for bias detection\n",
    "GUARDIAN_URL = os.environ.get(\"GUARDIAN_URL\", \"https://your-guardian-endpoint\")\n",
    "GUARDIAN_MODEL_NAME = os.environ.get(\"GUARDIAN_MODEL_NAME\", \"ibm-granite/granite-guardian-3.1-8b\")\n",
    "GUARDIAN_API_KEY = os.environ.get(\"GUARDIAN_API_KEY\", \"your-api-key-here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Guardian\n",
    "\n",
    "The Bias metric uses a Guardian model to detect potential bias in responses. Fair Forge supports:\n",
    "- **IBMGranite**: Uses IBM's Granite Guardian model\n",
    "- **LLamaGuard**: Uses Meta's LLamaGuard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_config = GuardianLLMConfig(\n",
    "    model=GUARDIAN_MODEL_NAME,\n",
    "    api_key=GUARDIAN_API_KEY,\n",
    "    url=GUARDIAN_URL,\n",
    "    temperature=0.5,\n",
    "    provider=OpenAIGuardianProvider,\n",
    "    logprobs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Bias Metric\n",
    "\n",
    "The Bias metric analyzes each Q&A interaction for potential bias across protected attributes:\n",
    "- Gender\n",
    "- Race\n",
    "- Religion\n",
    "- Nationality\n",
    "- Sexual orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Bias.run(\n",
    "    LocalRetriever,\n",
    "    guardian=IBMGranite,\n",
    "    config=guardian_config,\n",
    "    confidence_level=0.80,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Each BiasMetric contains:\n",
    "- `confidence_intervals`: Statistical confidence intervals for each protected attribute\n",
    "- `guardian_interactions`: Detailed bias assessments per Q&A interaction\n",
    "- `cluster_profiling`: Clustering analysis of biased content\n",
    "- `assistant_space`: Embedding space analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total metrics generated: {len(metrics)}\\n\")\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"Session: {metric.session_id}\")\n",
    "    print(f\"Assistant: {metric.assistant_id}\")\n",
    "    print(\"\\nConfidence Intervals by Protected Attribute:\")\n",
    "    for ci in metric.confidence_intervals:\n",
    "        print(f\"  - {ci.protected_attribute}: [{ci.lower_bound:.3f}, {ci.upper_bound:.3f}]\")\n",
    "        print(f\"    Probability: {ci.probability:.3f}, Samples: {ci.samples}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Guardian Interactions\n",
    "\n",
    "View detailed bias assessments for each Q&A interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    print(f\"\\nBias Interactions for {metric.assistant_id}:\")\n",
    "    for attribute, interactions in metric.guardian_interactions.items():\n",
    "        biased_count = sum(1 for i in interactions if i.is_biased)\n",
    "        print(f\"\\n  {attribute}: {biased_count}/{len(interactions)} flagged as biased\")\n",
    "        for interaction in interactions:\n",
    "            if interaction.is_biased:\n",
    "                print(f\"    - QA {interaction.qa_id}: certainty={interaction.certainty:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Export the results to JSON for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Custom serializer for nested structures\n",
    "def serialize_metric(metric):\n",
    "    data = {\n",
    "        \"session_id\": metric.session_id,\n",
    "        \"assistant_id\": metric.assistant_id,\n",
    "        \"confidence_intervals\": [\n",
    "            {\n",
    "                \"protected_attribute\": ci.protected_attribute,\n",
    "                \"lower_bound\": ci.lower_bound,\n",
    "                \"upper_bound\": ci.upper_bound,\n",
    "                \"probability\": ci.probability,\n",
    "                \"samples\": ci.samples,\n",
    "                \"k_success\": ci.k_success,\n",
    "                \"confidence_level\": ci.confidence_level,\n",
    "            }\n",
    "            for ci in metric.confidence_intervals\n",
    "        ],\n",
    "        \"guardian_interactions\": {\n",
    "            attr: [{\"qa_id\": i.qa_id, \"is_biased\": i.is_biased, \"certainty\": i.certainty} for i in interactions]\n",
    "            for attr, interactions in metric.guardian_interactions.items()\n",
    "        },\n",
    "    }\n",
    "    return data\n",
    "\n",
    "results = [serialize_metric(m) for m in metrics]\n",
    "\n",
    "with open(\"bias_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results exported to bias_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}