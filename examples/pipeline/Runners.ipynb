{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Fair Forge Runners Example\n",
    "\n",
    "This notebook demonstrates how to use the Fair Forge runners module to execute test datasets against AI systems.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The runners module provides:\n",
    "- **BaseRunner**: Abstract interface for implementing custom runners\n",
    "- **AlquimiaRunner**: Implementation for Alquimia AI agents\n",
    "- **Storage backends**: Local filesystem and LakeFS support for loading test datasets and saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install alquimia-fair-forge -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import uuid\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from fair_forge.runners import AlquimiaRunner\n",
    "from fair_forge.storage import create_local_storage, create_lakefs_storage\n",
    "from fair_forge.schemas import Batch, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mock-data",
   "metadata": {},
   "source": [
    "## Create Mock Test Data\n",
    "\n",
    "Let's create some mock test datasets to demonstrate the runner functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-mock-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock batches (test cases)\n",
    "mock_batches = [\n",
    "    Batch(\n",
    "        qa_id=\"test_001\",\n",
    "        query=\"What is the capital of France?\",\n",
    "        assistant=\"\",  # Will be filled by the runner\n",
    "        ground_truth_assistant=\"The capital of France is Paris.\",\n",
    "        observation=\"Basic geography question\",\n",
    "        agentic={},\n",
    "        ground_truth_agentic={},\n",
    "    ),\n",
    "    Batch(\n",
    "        qa_id=\"test_002\",\n",
    "        query=\"Explain quantum computing in simple terms.\",\n",
    "        assistant=\"\",\n",
    "        ground_truth_assistant=\"Quantum computing uses quantum mechanics principles to process information...\",\n",
    "        observation=\"Technical explanation test\",\n",
    "        agentic={},\n",
    "        ground_truth_agentic={},\n",
    "    ),\n",
    "    Batch(\n",
    "        qa_id=\"test_003\",\n",
    "        query=\"Write a haiku about programming.\",\n",
    "        assistant=\"\",\n",
    "        ground_truth_assistant=\"Code flows like water\\nBugs hide in silent shadows\\nDebugger reveals\",\n",
    "        observation=\"Creative writing test\",\n",
    "        agentic={},\n",
    "        ground_truth_agentic={},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create mock dataset\n",
    "mock_dataset = Dataset(\n",
    "    session_id=f\"test_session_{uuid.uuid4().hex[:8]}\",\n",
    "    assistant_id=\"test_assistant_001\",\n",
    "    language=\"english\",\n",
    "    context=\"General knowledge and creative writing test suite\",\n",
    "    conversation=mock_batches,\n",
    ")\n",
    "\n",
    "print(f\"Created mock dataset: {mock_dataset.session_id}\")\n",
    "print(f\"Number of test cases: {len(mock_dataset.conversation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "storage-setup",
   "metadata": {},
   "source": [
    "## Storage Setup\n",
    "\n",
    "### Option 1: Local Storage\n",
    "\n",
    "Use local filesystem for storing test datasets and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local storage instance\n",
    "local_storage = create_local_storage(\n",
    "    tests_dir=Path(\"./test_datasets\"),\n",
    "    results_dir=Path(\"./test_results\"),\n",
    "    enabled_suites=None,  # Load all test suites\n",
    ")\n",
    "\n",
    "# Save mock dataset to local storage for later loading\n",
    "test_datasets_dir = Path(\"./test_datasets\")\n",
    "test_datasets_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(test_datasets_dir / \"mock_test_suite.json\", \"w\") as f:\n",
    "    json.dump(mock_dataset.model_dump(), f, indent=2)\n",
    "\n",
    "print(\"Mock dataset saved to local storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lakefs-storage",
   "metadata": {},
   "source": [
    "### Option 2: LakeFS Storage (Optional)\n",
    "\n",
    "Use LakeFS for cloud-based storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lakefs-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and configure if using LakeFS\n",
    "# lakefs_storage = create_lakefs_storage(\n",
    "#     host=\"http://lakefs.example.com:8000\",\n",
    "#     username=\"admin\",\n",
    "#     password=\"your-password\",\n",
    "#     repo_id=\"fair-forge-tests\",\n",
    "#     enabled_suites=None,\n",
    "#     tests_prefix=\"tests/\",\n",
    "#     results_prefix=\"results/\",\n",
    "#     branch_name=\"main\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "runner-setup",
   "metadata": {},
   "source": [
    "## Runner Setup\n",
    "\n",
    "### AlquimiaRunner Configuration\n",
    "\n",
    "Configure the Alquimia runner to execute tests against your AI agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "runner-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Alquimia runner\n",
    "# NOTE: Set these environment variables or replace with your actual values\n",
    "runner = AlquimiaRunner(\n",
    "    base_url=os.getenv(\"ALQUIMIA_URL\", \"https://api.alquimia.ai\"),\n",
    "    api_key=os.getenv(\"ALQUIMIA_API_KEY\", \"your-api-key\"),\n",
    "    agent_id=os.getenv(\"AGENT_ID\", \"your-agent-id\"),\n",
    "    channel_id=os.getenv(\"CHANNEL_ID\", \"your-channel-id\"),\n",
    "    api_version=os.getenv(\"ALQUIMIA_VERSION\", \"\"),\n",
    ")\n",
    "\n",
    "print(\"Runner configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execute-tests",
   "metadata": {},
   "source": [
    "## Execute Tests\n",
    "\n",
    "### Run Single Batch\n",
    "\n",
    "Execute a single test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a single batch\n",
    "async def run_single_batch():\n",
    "    batch = mock_batches[0]\n",
    "    session_id = f\"test_session_{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    print(f\"Running batch: {batch.qa_id}\")\n",
    "    print(f\"Query: {batch.query}\")\n",
    "    \n",
    "    updated_batch, success, exec_time = await runner.run_batch(batch, session_id)\n",
    "    \n",
    "    print(f\"\\nSuccess: {success}\")\n",
    "    print(f\"Execution time: {exec_time:.2f}ms\")\n",
    "    print(f\"Response: {updated_batch.assistant}\")\n",
    "    \n",
    "    return updated_batch\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "# result = await run_single_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-dataset",
   "metadata": {},
   "source": [
    "### Run Complete Dataset\n",
    "\n",
    "Execute all test cases in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-full-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete dataset\n",
    "async def run_complete_dataset():\n",
    "    print(f\"Running dataset: {mock_dataset.session_id}\")\n",
    "    print(f\"Total batches: {len(mock_dataset.conversation)}\\n\")\n",
    "    \n",
    "    updated_dataset, summary = await runner.run_dataset(mock_dataset)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXECUTION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Session ID: {summary['session_id']}\")\n",
    "    print(f\"Total batches: {summary['total_batches']}\")\n",
    "    print(f\"Successes: {summary['successes']}\")\n",
    "    print(f\"Failures: {summary['failures']}\")\n",
    "    print(f\"Total execution time: {summary['total_execution_time_ms']:.2f}ms\")\n",
    "    print(f\"Average batch time: {summary['avg_batch_time_ms']:.2f}ms\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return updated_dataset, summary\n",
    "\n",
    "# Execute (uncomment to run)\n",
    "# results, summary = await run_complete_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-results",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Save execution results to storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-to-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to local storage\n",
    "async def save_results(updated_dataset):\n",
    "    run_id = str(uuid.uuid4())\n",
    "    timestamp = datetime.now()\n",
    "    \n",
    "    result_path = local_storage.save_results(\n",
    "        datasets=[updated_dataset],\n",
    "        run_id=run_id,\n",
    "        timestamp=timestamp,\n",
    "    )\n",
    "    \n",
    "    print(f\"Results saved to: {result_path}\")\n",
    "    return result_path\n",
    "\n",
    "# Execute (uncomment to run after running dataset)\n",
    "# result_path = await save_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-datasets",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "Load test datasets from storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-from-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets from local storage\n",
    "loaded_datasets = local_storage.load_datasets()\n",
    "\n",
    "print(f\"Loaded {len(loaded_datasets)} dataset(s)\")\n",
    "for ds in loaded_datasets:\n",
    "    print(f\"  - {ds.session_id}: {len(ds.conversation)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-pipeline",
   "metadata": {},
   "source": [
    "## Complete Pipeline Example\n",
    "\n",
    "Put it all together in a complete pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def complete_pipeline():\n",
    "    \"\"\"Complete test execution pipeline.\"\"\"\n",
    "    \n",
    "    # 1. Load datasets from storage\n",
    "    print(\"Step 1: Loading test datasets...\")\n",
    "    datasets = local_storage.load_datasets()\n",
    "    print(f\"Loaded {len(datasets)} dataset(s)\\n\")\n",
    "    \n",
    "    if not datasets:\n",
    "        print(\"No datasets found!\")\n",
    "        return\n",
    "    \n",
    "    # 2. Execute all datasets\n",
    "    print(\"Step 2: Executing datasets...\")\n",
    "    executed_datasets = []\n",
    "    all_summaries = []\n",
    "    \n",
    "    for i, dataset in enumerate(datasets, 1):\n",
    "        print(f\"\\n[{i}/{len(datasets)}] Processing: {dataset.session_id}\")\n",
    "        updated_dataset, summary = await runner.run_dataset(dataset)\n",
    "        executed_datasets.append(updated_dataset)\n",
    "        all_summaries.append(summary)\n",
    "    \n",
    "    # 3. Save results\n",
    "    print(\"\\nStep 3: Saving results...\")\n",
    "    run_id = str(uuid.uuid4())\n",
    "    timestamp = datetime.now()\n",
    "    result_path = local_storage.save_results(\n",
    "        datasets=executed_datasets,\n",
    "        run_id=run_id,\n",
    "        timestamp=timestamp,\n",
    "    )\n",
    "    \n",
    "    # 4. Print overall summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"OVERALL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    total_batches = sum(s['total_batches'] for s in all_summaries)\n",
    "    total_successes = sum(s['successes'] for s in all_summaries)\n",
    "    total_failures = sum(s['failures'] for s in all_summaries)\n",
    "    print(f\"Total datasets: {len(datasets)}\")\n",
    "    print(f\"Total test cases: {total_batches}\")\n",
    "    print(f\"Successes: {total_successes}\")\n",
    "    print(f\"Failures: {total_failures}\")\n",
    "    print(f\"Success rate: {(total_successes/total_batches*100):.1f}%\")\n",
    "    print(f\"Results saved to: {result_path}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Execute (uncomment to run complete pipeline)\n",
    "# await complete_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-runner",
   "metadata": {},
   "source": [
    "## Creating Custom Runners\n",
    "\n",
    "You can create custom runner implementations by extending `BaseRunner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-runner-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fair_forge.schemas.runner import BaseRunner\n",
    "from typing import Any\n",
    "\n",
    "class MockRunner(BaseRunner):\n",
    "    \"\"\"Example mock runner for testing.\"\"\"\n",
    "    \n",
    "    async def run_batch(self, batch: Batch, session_id: str, **kwargs: Any) -> tuple[Batch, bool, float]:\n",
    "        \"\"\"Return a mock response immediately.\"\"\"\n",
    "        import time\n",
    "        start = time.time()\n",
    "        \n",
    "        # Mock response\n",
    "        mock_response = f\"Mock response for: {batch.query}\"\n",
    "        updated_batch = batch.model_copy(update={\"assistant\": mock_response})\n",
    "        \n",
    "        exec_time = (time.time() - start) * 1000\n",
    "        return updated_batch, True, exec_time\n",
    "    \n",
    "    async def run_dataset(self, dataset: Dataset, **kwargs: Any) -> tuple[Dataset, dict[str, Any]]:\n",
    "        \"\"\"Run all batches in dataset.\"\"\"\n",
    "        import time\n",
    "        start = time.time()\n",
    "        \n",
    "        updated_batches = []\n",
    "        for batch in dataset.conversation:\n",
    "            updated_batch, _, _ = await self.run_batch(batch, dataset.session_id)\n",
    "            updated_batches.append(updated_batch)\n",
    "        \n",
    "        updated_dataset = dataset.model_copy(update={\"conversation\": updated_batches})\n",
    "        \n",
    "        summary = {\n",
    "            \"session_id\": dataset.session_id,\n",
    "            \"total_batches\": len(dataset.conversation),\n",
    "            \"successes\": len(dataset.conversation),\n",
    "            \"failures\": 0,\n",
    "            \"total_execution_time_ms\": (time.time() - start) * 1000,\n",
    "            \"avg_batch_time_ms\": 1.0,\n",
    "        }\n",
    "        \n",
    "        return updated_dataset, summary\n",
    "\n",
    "# Test mock runner\n",
    "mock_runner = MockRunner()\n",
    "print(\"Custom MockRunner created successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
