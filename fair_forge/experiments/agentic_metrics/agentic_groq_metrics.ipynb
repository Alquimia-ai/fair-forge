{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√©tricas Agentic con Groq como Juez\n",
    "\n",
    "Este notebook implementa las m√©tricas para evaluar respuestas de agentes:\n",
    "- **pass@K**: Al menos una respuesta de K es correcta\n",
    "- **pass^K**: Todas las K respuestas son correctas\n",
    "- **Tool Correctness**: Evaluaci√≥n del uso correcto de herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cliente Groq configurado con modelo: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Any\n",
    "from dataclasses import dataclass\n",
    "from groq import Groq\n",
    "\n",
    "# Configurar API key de Groq\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"‚ö†Ô∏è  GROQ_API_KEY no encontrada. Config√∫rala con: export GROQ_API_KEY='tu-key'\")\n",
    "    print(\"Obt√©n tu key en: https://console.groq.com/\")\n",
    "\n",
    "# Inicializar cliente Groq\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Modelo a usar (opciones: llama-3.3-70b-versatile, llama-3.1-70b-versatile, mixtral-8x7b-32768)\n",
    "MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "print(f\"‚úÖ Cliente Groq configurado con modelo: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estructuras de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Estructuras de datos definidas\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AgentResponse:\n",
    "    \"\"\"Respuesta de un agente a una query.\"\"\"\n",
    "    query: str\n",
    "    answer: str\n",
    "    agentic: dict[str, Any] | None = None  # Dict con tools_used, final_answer_uses_tools\n",
    "    \n",
    "@dataclass\n",
    "class GroundTruth:\n",
    "    \"\"\"Respuesta esperada (ground truth).\"\"\"\n",
    "    expected_answer: str\n",
    "    ground_truth_agentic: dict[str, Any] | None = None  # Dict con expected_tools, tool_sequence_matters\n",
    "\n",
    "@dataclass\n",
    "class ToolCorrectnessScore:\n",
    "    \"\"\"Scores de evaluaci√≥n de tool correctness.\"\"\"\n",
    "    tool_selection_correct: float  # 0.0 - 1.0\n",
    "    parameter_accuracy: float      # 0.0 - 1.0\n",
    "    sequence_correct: float        # 0.0 - 1.0\n",
    "    result_utilization: float      # 0.0 - 1.0\n",
    "    overall_correctness: float     # Promedio ponderado\n",
    "    is_correct: bool               # True si overall >= threshold\n",
    "\n",
    "@dataclass\n",
    "class AgenticMetric:\n",
    "    \"\"\"M√©tricas finales de evaluaci√≥n.\"\"\"\n",
    "    qa_id: str\n",
    "    k: int\n",
    "    threshold: float\n",
    "    correctness_scores: list[float]  # Score de correcci√≥n para cada respuesta (0.0 - 1.0)\n",
    "    pass_at_k: bool\n",
    "    pass_pow_k: bool\n",
    "    correct_indices: list[int]\n",
    "    tool_correctness: ToolCorrectnessScore | None = None\n",
    "\n",
    "print(\"‚úÖ Estructuras de datos definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A. Explicaci√≥n: Estructura de Diccionarios Agentic\n",
    "\n",
    "### üì• `agentic` (usado por el agente)\n",
    "\n",
    "Este diccionario describe **qu√© herramientas us√≥ el agente** en su respuesta:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"tools_used\": [\n",
    "        {\n",
    "            \"tool_id\": \"calc_001\",           # ID √∫nico de la herramienta\n",
    "            \"tool_name\": \"calculator\",       # Nombre de la herramienta\n",
    "            \"tool_step\": 1,                  # Paso en la secuencia (1, 2, 3...)\n",
    "            \"parameters\": {                  # Par√°metros pasados a la herramienta\n",
    "                \"operation\": \"multiply\",\n",
    "                \"a\": 15,\n",
    "                \"b\": 7\n",
    "            },\n",
    "            \"result\": 105                    # Resultado retornado por la herramienta\n",
    "        },\n",
    "        # ... m√°s herramientas si se usaron\n",
    "    ],\n",
    "    \"final_answer_uses_tools\": True        # ¬øLa respuesta final usa los resultados?\n",
    "}\n",
    "```\n",
    "\n",
    "### üì§ `ground_truth_agentic` (esperado)\n",
    "\n",
    "Este diccionario describe **qu√© herramientas DEBER√çA usar el agente**:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"expected_tools\": [\n",
    "        {\n",
    "            \"tool_id\": \"calc_001\",           # ID esperado\n",
    "            \"tool_name\": \"calculator\",       # Herramienta esperada\n",
    "            \"tool_step\": 1,                  # Paso esperado\n",
    "            \"parameters\": {                  # Par√°metros correctos\n",
    "                \"operation\": \"multiply\",\n",
    "                \"a\": 15,\n",
    "                \"b\": 7\n",
    "            }\n",
    "        },\n",
    "        # ... m√°s herramientas esperadas\n",
    "    ],\n",
    "    \"tool_sequence_matters\": True          # ¬øImporta el orden? (True/False)\n",
    "}\n",
    "```\n",
    "\n",
    "### üîç C√≥mo se Eval√∫a Tool Correctness\n",
    "\n",
    "La funci√≥n `evaluate_tool_correctness()` compara ambos diccionarios y eval√∫a:\n",
    "\n",
    "1. **Tool Selection** (0.25 peso):\n",
    "   - ¬øCoinciden los `tool_name`?\n",
    "   - ¬øFalta alguna herramienta o hay extras?\n",
    "\n",
    "2. **Parameter Accuracy** (0.25 peso):\n",
    "   - ¬øLos `parameters` son id√©nticos?\n",
    "   - Se compara valor por valor\n",
    "\n",
    "3. **Sequence Correct** (0.25 peso):\n",
    "   - ¬øLos `tool_step` est√°n en orden correcto?\n",
    "   - Solo aplica si `tool_sequence_matters=True`\n",
    "\n",
    "4. **Result Utilization** (0.25 peso):\n",
    "   - ¬ø`final_answer_uses_tools=True`?\n",
    "   - ¬øLos resultados aparecen en la respuesta final?\n",
    "\n",
    "**Overall Correctness** = Promedio ponderado de los 4 aspectos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Funciones para Llamar a Groq\n",
    "\n",
    "Groq evaluar√° las respuestas y devolver√° scores en formato JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n evaluate_answer_correctness definida\n"
     ]
    }
   ],
   "source": [
    "def evaluate_answer_correctness(query: str, answer: str, ground_truth: str) -> float:\n",
    "    \"\"\"\n",
    "    Usa Groq para evaluar si una respuesta es correcta.\n",
    "    \n",
    "    Returns:\n",
    "        float: Score de correcci√≥n entre 0.0 (incorrecto) y 1.0 (totalmente correcto)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a STRICT evaluator. Your task is to determine if an agent's answer is correct compared to the ground truth.\n",
    "\n",
    "**Question:** {query}\n",
    "\n",
    "**Agent's Answer:** {answer}\n",
    "\n",
    "**Ground Truth:** {ground_truth}\n",
    "\n",
    "Evaluate the correctness with STRICT criteria:\n",
    "\n",
    "1. **Factual Accuracy** (most important): Is the core information factually correct?\n",
    "2. **Precision**: Spelling errors, typos, or incorrect formatting should be penalized\n",
    "3. **Completeness**: Does it answer what was asked?\n",
    "4. **Format**: Natural language variations are acceptable ONLY if facts are perfect\n",
    "\n",
    "IMPORTANT SCORING RULES:\n",
    "- 1.0: Identical or perfectly correct with natural rephrasing (same facts, perfect spelling)\n",
    "- 0.85-0.95: Correct facts with slightly more verbose explanation\n",
    "- 0.65-0.75: Correct core fact BUT has typo/spelling error (e.g., \"Poris\" instead of \"Paris\")\n",
    "- 0.5-0.65: Mostly correct but missing important details\n",
    "- 0.3-0.5: Partially correct with significant errors\n",
    "- 0.0-0.3: Wrong answer or completely incorrect\n",
    "\n",
    "**Typo/Spelling Penalty**: \n",
    "- Single character typo in short answer: MAX score 0.75\n",
    "- Multiple typos: MAX score 0.5\n",
    "- Wrong word entirely: score below 0.3\n",
    "\n",
    "**Wrong Answer**: Factually incorrect information must score below 0.3\n",
    "\n",
    "Return ONLY a JSON object with this exact format:\n",
    "{{\n",
    "  \"correctness_score\": <float between 0.0 and 1.0>,\n",
    "  \"reasoning\": \"<brief explanation of your evaluation>\"\n",
    "}}\n",
    "\n",
    "Examples:\n",
    "- Q: \"Capital of France?\", A: \"Paris\", GT: \"Paris\" ‚Üí 1.0 (perfect match)\n",
    "- Q: \"Capital of France?\", A: \"The capital of France is Paris\", GT: \"Paris\" ‚Üí 0.95 (correct, verbose)\n",
    "- Q: \"Capital of France?\", A: \"Poris\", GT: \"Paris\" ‚Üí 0.7 (TYPO PENALTY - core fact known but misspelled)\n",
    "- Q: \"Capital of France?\", A: \"Pariis\", GT: \"Paris\" ‚Üí 0.7 (TYPO PENALTY)\n",
    "- Q: \"Capital of France?\", A: \"Lyon\", GT: \"Paris\" ‚Üí 0.0 (completely wrong city)\n",
    "- Q: \"Capital of France?\", A: \"London\", GT: \"Paris\" ‚Üí 0.0 (wrong country)\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return float(result.get(\"correctness_score\", 0.0))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error evaluando respuesta: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "print(\"‚úÖ Funci√≥n evaluate_answer_correctness definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ evaluate_tool_correctness definida (con threshold)\n",
      "   Returns: ToolCorrectnessScore con is_correct boolean\n"
     ]
    }
   ],
   "source": [
    "def evaluate_tool_correctness(\n",
    "    agentic: dict[str, Any],\n",
    "    ground_truth_agentic: dict[str, Any],\n",
    "    threshold: float = 0.75\n",
    ") -> ToolCorrectnessScore:\n",
    "    \"\"\"\n",
    "    Usa Groq para evaluar el uso correcto de herramientas.\n",
    "    \n",
    "    Args:\n",
    "        agentic: Dict con 'tools_used' y 'final_answer_uses_tools'\n",
    "        ground_truth_agentic: Dict con 'expected_tools' y 'tool_sequence_matters'\n",
    "        threshold: Umbral para considerar correctas las herramientas (default: 0.75)\n",
    "    \n",
    "    Returns:\n",
    "        ToolCorrectnessScore con scores individuales, overall, e is_correct\n",
    "    \"\"\"\n",
    "    tools_used = agentic.get(\"tools_used\", [])\n",
    "    final_answer_uses_tools = agentic.get(\"final_answer_uses_tools\", False)\n",
    "    expected_tools = ground_truth_agentic.get(\"expected_tools\", [])\n",
    "    sequence_matters = ground_truth_agentic.get(\"tool_sequence_matters\", True)\n",
    "    \n",
    "    prompt = f\"\"\"You are a STRICT evaluator of AI agent tool usage. Evaluate how correctly an agent used tools.\n",
    "\n",
    "**Tools Used by Agent:**\n",
    "{json.dumps(tools_used, indent=2)}\n",
    "\n",
    "**Final Answer Uses Tools:** {final_answer_uses_tools}\n",
    "\n",
    "**Expected Tools:**\n",
    "{json.dumps(expected_tools, indent=2)}\n",
    "\n",
    "**Sequence Matters:** {sequence_matters}\n",
    "\n",
    "Evaluate the following aspects with STRICT criteria (each scored 0.0 to 1.0):\n",
    "\n",
    "1. **tool_selection_correct**: \n",
    "   - Did the agent select the correct tools (by tool_name)?\n",
    "   - 1.0 = all correct tools, 0.0 = wrong tools\n",
    "\n",
    "2. **parameter_accuracy**: \n",
    "   - Were the parameters correct for each tool?\n",
    "   - Compare parameter values EXACTLY\n",
    "   - 1.0 = all parameters perfect, 0.0 = wrong parameters\n",
    "\n",
    "3. **sequence_correct**: \n",
    "   - Were tools called in the correct order?\n",
    "   - If sequence_matters=True: Compare tool_step for EACH tool\n",
    "   - For each tool_name, check if its tool_step matches expected\n",
    "   - Example: calculator at step 1 when expected at step 2 = WRONG ORDER\n",
    "   - 1.0 = ALL tool_steps match, 0.0 = wrong order\n",
    "   - If sequence_matters=False: Always return 1.0\n",
    "\n",
    "4. **result_utilization**: \n",
    "   - Did the agent use the tool results properly?\n",
    "   - Check if final_answer_uses_tools=True\n",
    "   - 1.0 = excellent use, 0.0 = didn't use\n",
    "\n",
    "CRITICAL FOR SEQUENCE:\n",
    "- Compare each tool's tool_step with its expected tool_step\n",
    "- \"calculator\" at step 1 when expected at step 2 = WRONG (0.0)\n",
    "- \"web_search\" at step 2 when expected at step 1 = WRONG (0.0)\n",
    "- Swapped steps = COMPLETELY WRONG ORDER (0.0)\n",
    "\n",
    "Return ONLY a JSON object:\n",
    "{{\n",
    "  \"tool_selection_correct\": <float 0.0-1.0>,\n",
    "  \"parameter_accuracy\": <float 0.0-1.0>,\n",
    "  \"sequence_correct\": <float 0.0-1.0>,\n",
    "  \"result_utilization\": <float 0.0-1.0>,\n",
    "  \"reasoning\": \"<explain why sequence_correct has its value>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        # Si sequence no importa, no penalizar\n",
    "        if not sequence_matters:\n",
    "            result[\"sequence_correct\"] = 1.0\n",
    "        \n",
    "        # Calcular overall (promedio ponderado 25% cada uno)\n",
    "        overall = 0.25 * (\n",
    "            result[\"tool_selection_correct\"] +\n",
    "            result[\"parameter_accuracy\"] +\n",
    "            result[\"sequence_correct\"] +\n",
    "            result[\"result_utilization\"]\n",
    "        )\n",
    "        \n",
    "        # Determinar si es correcto seg√∫n threshold\n",
    "        is_correct = overall >= threshold\n",
    "        \n",
    "        return ToolCorrectnessScore(\n",
    "            tool_selection_correct=result[\"tool_selection_correct\"],\n",
    "            parameter_accuracy=result[\"parameter_accuracy\"],\n",
    "            sequence_correct=result[\"sequence_correct\"],\n",
    "            result_utilization=result[\"result_utilization\"],\n",
    "            overall_correctness=overall,\n",
    "            is_correct=is_correct\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error evaluando tool correctness: {e}\")\n",
    "        return ToolCorrectnessScore(0.0, 0.0, 0.0, 0.0, 0.0, False)\n",
    "\n",
    "print(\"‚úÖ evaluate_tool_correctness definida (con threshold)\")\n",
    "print(\"   Returns: ToolCorrectnessScore con is_correct boolean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Funci√≥n Principal: Calcular M√©tricas Agentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ calculate_agentic_metrics definida (con tool_threshold)\n"
     ]
    }
   ],
   "source": [
    "def calculate_agentic_metrics(\n",
    "    qa_id: str,\n",
    "    responses: list[AgentResponse],\n",
    "    ground_truth: GroundTruth,\n",
    "    threshold: float = 0.7,\n",
    "    tool_threshold: float = 0.75,\n",
    "    verbose: bool = True\n",
    ") -> AgenticMetric:\n",
    "    \"\"\"\n",
    "    Calcula pass@K, pass^K y tool correctness para K respuestas.\n",
    "    \n",
    "    Args:\n",
    "        qa_id: Identificador de la pregunta\n",
    "        responses: Lista de K respuestas del agente\n",
    "        ground_truth: Respuesta correcta esperada\n",
    "        threshold: Umbral para considerar una respuesta correcta (default: 0.7)\n",
    "        tool_threshold: Umbral para considerar correcto el uso de herramientas (default: 0.75)\n",
    "        verbose: Mostrar progreso\n",
    "    \n",
    "    Returns:\n",
    "        AgenticMetric con todas las m√©tricas calculadas\n",
    "    \"\"\"\n",
    "    k = len(responses)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Evaluando QA ID: {qa_id}\")\n",
    "        print(f\"K = {k} | Answer Threshold = {threshold} | Tool Threshold = {tool_threshold}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # 1. Evaluar correcci√≥n de cada respuesta\n",
    "    correctness_scores = []\n",
    "    correct_indices = []\n",
    "    \n",
    "    for i, response in enumerate(responses):\n",
    "        if verbose:\n",
    "            print(f\"üìù Evaluando respuesta {i+1}/{k}...\")\n",
    "        \n",
    "        score = evaluate_answer_correctness(\n",
    "            query=response.query,\n",
    "            answer=response.answer,\n",
    "            ground_truth=ground_truth.expected_answer\n",
    "        )\n",
    "        \n",
    "        correctness_scores.append(score)\n",
    "        \n",
    "        if score >= threshold:\n",
    "            correct_indices.append(i)\n",
    "        \n",
    "        if verbose:\n",
    "            status = \"‚úÖ CORRECTO\" if score >= threshold else \"‚ùå INCORRECTO\"\n",
    "            print(f\"   Score: {score:.3f} {status}\\n\")\n",
    "    \n",
    "    # 2. Calcular pass@K y pass^K\n",
    "    pass_at_k = len(correct_indices) > 0  # Al menos una correcta\n",
    "    pass_pow_k = len(correct_indices) == k  # Todas correctas\n",
    "    \n",
    "    # 3. Evaluar tool correctness (si hay herramientas)\n",
    "    tool_correctness = None\n",
    "    if ground_truth.ground_truth_agentic:\n",
    "        if verbose:\n",
    "            print(\"üîß Evaluando tool correctness...\\n\")\n",
    "        \n",
    "        # Evaluar la primera respuesta correcta (o la primera si ninguna es correcta)\n",
    "        response_to_eval = responses[correct_indices[0]] if correct_indices else responses[0]\n",
    "        \n",
    "        if response_to_eval.agentic:\n",
    "            tool_correctness = evaluate_tool_correctness(\n",
    "                agentic=response_to_eval.agentic,\n",
    "                ground_truth_agentic=ground_truth.ground_truth_agentic,\n",
    "                threshold=tool_threshold\n",
    "            )\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"   Tool Selection: {tool_correctness.tool_selection_correct:.3f}\")\n",
    "                print(f\"   Parameter Accuracy: {tool_correctness.parameter_accuracy:.3f}\")\n",
    "                print(f\"   Sequence Correct: {tool_correctness.sequence_correct:.3f}\")\n",
    "                print(f\"   Result Utilization: {tool_correctness.result_utilization:.3f}\")\n",
    "                print(f\"   Overall: {tool_correctness.overall_correctness:.3f}\")\n",
    "                print(f\"   Is Correct: {tool_correctness.is_correct} ({'‚úÖ' if tool_correctness.is_correct else '‚ùå'})\\n\")\n",
    "    \n",
    "    # 4. Construir resultado\n",
    "    metric = AgenticMetric(\n",
    "        qa_id=qa_id,\n",
    "        k=k,\n",
    "        threshold=threshold,\n",
    "        correctness_scores=correctness_scores,\n",
    "        pass_at_k=pass_at_k,\n",
    "        pass_pow_k=pass_pow_k,\n",
    "        correct_indices=correct_indices,\n",
    "        tool_correctness=tool_correctness\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"üìä RESULTADOS FINALES\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"   pass@{k}: {pass_at_k} ({'‚úÖ' if pass_at_k else '‚ùå'})\")\n",
    "        print(f\"   pass^{k}: {pass_pow_k} ({'‚úÖ' if pass_pow_k else '‚ùå'})\")\n",
    "        print(f\"   Correctas: {len(correct_indices)}/{k}\")\n",
    "        if tool_correctness:\n",
    "            print(f\"   Tool Usage Correct: {tool_correctness.is_correct} ({'‚úÖ' if tool_correctness.is_correct else '‚ùå'})\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return metric\n",
    "\n",
    "print(\"‚úÖ calculate_agentic_metrics definida (con tool_threshold)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ejemplo de Uso: Pregunta Simple sin Herramientas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Evaluando QA ID: capital_france\n",
      "K = 4 | Answer Threshold = 0.8 | Tool Threshold = 0.75\n",
      "======================================================================\n",
      "\n",
      "üìù Evaluando respuesta 1/4...\n",
      "   Score: 1.000 ‚úÖ CORRECTO\n",
      "\n",
      "üìù Evaluando respuesta 2/4...\n",
      "   Score: 0.950 ‚úÖ CORRECTO\n",
      "\n",
      "üìù Evaluando respuesta 3/4...\n",
      "   Score: 0.000 ‚ùå INCORRECTO\n",
      "\n",
      "üìù Evaluando respuesta 4/4...\n",
      "   Score: 0.650 ‚ùå INCORRECTO\n",
      "\n",
      "======================================================================\n",
      "üìä RESULTADOS FINALES\n",
      "======================================================================\n",
      "   pass@4: True (‚úÖ)\n",
      "   pass^4: False (‚ùå)\n",
      "   Correctas: 2/4\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definir la pregunta\n",
    "query = \"What is the capital of France?\"\n",
    "\n",
    "# Definir K=3 respuestas de diferentes agentes\n",
    "responses = [\n",
    "    AgentResponse(\n",
    "        query=query,\n",
    "        answer=\"Paris\"\n",
    "    ),\n",
    "    AgentResponse(\n",
    "        query=query,\n",
    "        answer=\"The capital of France is Paris, a beautiful city known for the Eiffel Tower.\"\n",
    "    ),\n",
    "    AgentResponse(\n",
    "        query=query,\n",
    "        answer=\"London\"  # Respuesta incorrecta\n",
    "    ),\n",
    "    AgentResponse(\n",
    "    query=query,\n",
    "    answer=\"The caital of france is Parissss\"  # Respuesta parcialmente incorrecta\n",
    ")\n",
    "]\n",
    "\n",
    "# Definir ground truth\n",
    "ground_truth = GroundTruth(\n",
    "    expected_answer=\"Paris\"\n",
    ")\n",
    "\n",
    "# Calcular m√©tricas\n",
    "metrics = calculate_agentic_metrics(\n",
    "    qa_id=\"capital_france\",\n",
    "    responses=responses,\n",
    "    ground_truth=ground_truth,\n",
    "    threshold=0.8,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5A. Test de Rigor: Evaluando Typos\n",
    "\n",
    "Probemos que el sistema ahora penaliza correctamente los typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUANDO M√öLTIPLES RESPUESTAS CON TOOL USAGE\n",
      "================================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "RESPUESTA 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Answer Score: 1.000 (‚úÖ)\n",
      "\n",
      "üîß Tool Correctness:\n",
      "   Selection: 1.000\n",
      "   Parameters: 1.000\n",
      "   Sequence: 1.000\n",
      "   Utilization: 1.000\n",
      "   Overall: 1.000\n",
      "   Is Correct: True (‚úÖ)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "RESPUESTA 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Answer Score: 1.000 (‚úÖ)\n",
      "\n",
      "üîß Tool Correctness:\n",
      "   Selection: 1.000\n",
      "   Parameters: 1.000\n",
      "   Sequence: 0.000\n",
      "   Utilization: 1.000\n",
      "   Overall: 0.750\n",
      "   Is Correct: False (‚ùå)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "RESPUESTA 3\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Answer Score: 0.000 (‚ùå)\n",
      "\n",
      "üîß Tool Correctness:\n",
      "   Selection: 1.000\n",
      "   Parameters: 0.000\n",
      "   Sequence: 1.000\n",
      "   Utilization: 1.000\n",
      "   Overall: 0.750\n",
      "   Is Correct: False (‚ùå)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "RESPUESTA 4\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    }
   ],
   "source": [
    "# Query que requiere m√∫ltiples herramientas en orden espec√≠fico\n",
    "query = \"Search for Tokyo's population, then calculate 10% of it\"\n",
    "\n",
    "# Ground truth: Orden esperado y par√°metros correctos\n",
    "ground_truth = GroundTruth(\n",
    "    expected_answer=\"Tokyo has ~14 million people. 10% is 1.4 million.\",\n",
    "    ground_truth_agentic={\n",
    "        \"expected_tools\": [\n",
    "            {\n",
    "                \"tool_id\": \"search_001\",\n",
    "                \"tool_name\": \"web_search\",\n",
    "                \"tool_step\": 1,  # Primero buscar\n",
    "                \"parameters\": {\"query\": \"Tokyo population 2024\"}\n",
    "            },\n",
    "            {\n",
    "                \"tool_id\": \"calc_001\",\n",
    "                \"tool_name\": \"calculator\",\n",
    "                \"tool_step\": 2,  # Luego calcular\n",
    "                \"parameters\": {\"operation\": \"multiply\", \"a\": 14000000, \"b\": 0.1}\n",
    "            }\n",
    "        ],\n",
    "        \"tool_sequence_matters\": True  # El orden S√ç importa\n",
    "    }\n",
    ")\n",
    "\n",
    "# K=4 respuestas con diferentes casos\n",
    "responses = [\n",
    "    # 1. TODO CORRECTO: herramientas, par√°metros y secuencia\n",
    "    AgentResponse(\n",
    "        query=query,\n",
    "        answer=\"Tokyo has 14 million people. 10% is 1.4 million.\",\n",
    "        agentic={\n",
    "            \"tools_used\": [\n",
    "                {\"tool_id\": \"search_001\", \"tool_name\": \"web_search\", \"tool_step\": 1,\n",
    "                 \"parameters\": {\"query\": \"Tokyo population 2024\"}, \"result\": \"14 million\"},\n",
    "                {\"tool_id\": \"calc_001\", \"tool_name\": \"calculator\", \"tool_step\": 2,\n",
    "                 \"parameters\": {\"operation\": \"multiply\", \"a\": 14000000, \"b\": 0.1}, \"result\": 1400000}\n",
    "            ],\n",
    "            \"final_answer_uses_tools\": True\n",
    "        }\n",
    "    ),\n",
    "    # 2. SECUENCIA INCORRECTA: Calcula antes de buscar\n",
    "    AgentResponse(\n",
    "        query=query,\n",
    "        answer=\"10% is 1.4 million.\",\n",
    "        agentic={\n",
    "            \"tools_used\": [\n",
    "                {\"tool_id\": \"calc_002\", \"tool_name\": \"calculator\", \"tool_step\": 1,  # ‚ùå Deber√≠a ser 2\n",
    "                 \"parameters\": {\"operation\": \"multiply\", \"a\": 14000000, \"b\": 0.1}, \"result\": 1400000},\n",
    "                {\"tool_id\": \"search_002\", \"tool_name\": \"web_search\", \"tool_step\": 2,  # ‚ùå Deber√≠a ser 1\n",
    "                 \"parameters\": {\"query\": \"Tokyo population 2024\"}, \"result\": \"14 million\"}\n",
    "            ],\n",
    "            \"final_answer_uses_tools\": True\n",
    "        }\n",
    "    ),\n",
    "    # 3. PAR√ÅMETROS INCORRECTOS: Usa suma en lugar de multiplicaci√≥n\n",
    "    AgentResponse(\n",
    "        query=query,\n",
    "        answer=\"The result is around 14 million.\",\n",
    "        agentic={\n",
    "            \"tools_used\": [\n",
    "                {\"tool_id\": \"search_003\", \"tool_name\": \"web_search\", \"tool_step\": 1,\n",
    "                 \"parameters\": {\"query\": \"Tokyo population 2024\"}, \"result\": \"14 million\"},\n",
    "                {\"tool_id\": \"calc_003\", \"tool_name\": \"calculator\", \"tool_step\": 2,\n",
    "                 \"parameters\": {\"operation\": \"add\", \"a\": 14000000, \"b\": 0.1}, \"result\": 14000000.1}  # ‚ùå Suma\n",
    "            ],\n",
    "            \"final_answer_uses_tools\": True\n",
    "        }\n",
    "    ),\n",
    "    # 4. SIN HERRAMIENTAS: Responde manualmente (incorrecto)\n",
    "    AgentResponse(\n",
    "        query=query,\n",
    "        answer=\"Tokyo has around 10 million people, so 10% is 1 million.\",\n",
    "        agentic=None\n",
    "    )\n",
    "]\n",
    "\n",
    "# Evaluar todas las respuestas\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUANDO M√öLTIPLES RESPUESTAS CON TOOL USAGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, response in enumerate(responses, 1):\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"RESPUESTA {i}\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    # Evaluar correcci√≥n de answer\n",
    "    answer_score = evaluate_answer_correctness(query, response.answer, ground_truth.expected_answer)\n",
    "    print(f\"Answer Score: {answer_score:.3f} ({'‚úÖ' if answer_score >= 0.7 else '‚ùå'})\")\n",
    "    \n",
    "    # Evaluar tool correctness si hay herramientas\n",
    "    if response.agentic and ground_truth.ground_truth_agentic:\n",
    "        tool_score = evaluate_tool_correctness(\n",
    "            response.agentic,\n",
    "            ground_truth.ground_truth_agentic,\n",
    "            threshold=0.8\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüîß Tool Correctness:\")\n",
    "        print(f\"   Selection: {tool_score.tool_selection_correct:.3f}\")\n",
    "        print(f\"   Parameters: {tool_score.parameter_accuracy:.3f}\")\n",
    "        print(f\"   Sequence: {tool_score.sequence_correct:.3f}\")\n",
    "        print(f\"   Utilization: {tool_score.result_utilization:.3f}\")\n",
    "        print(f\"   Overall: {tool_score.overall_correctness:.3f}\")\n",
    "        print(f\"   Is Correct: {tool_score.is_correct} ({'‚úÖ' if tool_score.is_correct else '‚ùå'})\")\n",
    "    else:\n",
    "        print(f\"\\nüîß Tool Correctness: N/A (no usa herramientas)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESUMEN\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"Respuesta 1: ‚úÖ TODO correcto\")\n",
    "print(\"Respuesta 2: ‚ùå Secuencia incorrecta (invierte orden)\")\n",
    "print(\"Respuesta 3: ‚ùå Par√°metros incorrectos (usa suma en vez de multiplicaci√≥n)\")\n",
    "print(\"Respuesta 4: ‚ùå No usa herramientas\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
