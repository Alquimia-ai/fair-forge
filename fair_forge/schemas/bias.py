"""Bias metric schemas."""
from pydantic import BaseModel
from typing import Optional, Type
from enum import Enum
from abc import ABC, abstractmethod
from functools import partial
from transformers import AutoTokenizer
from .metrics import BaseMetric


class GuardianBias(BaseModel):
    """
    A data model that represents the result of a bias detection analysis.

    Attributes:
        is_biased (bool): Indicates whether bias was detected in the interaction
        attribute (str): The specific attribute that was analyzed for bias
        certainty (Optional[float]): A confidence score for the bias detection, if available
    """
    is_biased: bool
    attribute: str
    certainty: Optional[float]


class ProtectedAttribute(BaseModel):
    """
    Protected attributes for bias detection.
    """
    class Attribute(str, Enum):
        age = "age"
        gender = "gender"
        race = "race"
        religion = "religion"
        nationality = "nationality"
        sexual_orientation = "sexual_orientation"

    attribute: Attribute
    description: str


class BiasMetric(BaseMetric):
    """
    Bias metric for evaluating the bias of the assistant's responses.
    """
    class ConfidenceInterval(BaseModel):
        lower_bound: float
        upper_bound: float
        probability: float
        samples: int
        k_success: int
        alpha: float
        confidence_level: float
        protected_attribute: str

    class GuardianInteraction(GuardianBias):
        qa_id: str

    confidence_intervals: list[ConfidenceInterval]
    guardian_interactions: dict[str, list[GuardianInteraction]]


class LLMGuardianProviderInfer(BaseModel):
    """Result from an LLM guardian provider inference."""
    is_bias: bool
    probability: float


class LLMGuardianProvider(ABC):
    """Abstract base class for LLM guardian providers."""

    def __init__(
        self,
        model: str,
        tokenizer: AutoTokenizer,
        api_key: Optional[str] = None,
        url: Optional[str] = None,
        temperature: float = 0.0,
        safe_token: str = "No",
        unsafe_token: str = "Yes",
        max_tokens: int = 5,
        logprobs: bool = True,
        **kwargs
    ):
        self.model = model
        self.api_key = api_key
        self.url = url
        self.temperature = temperature
        self.safe_token = safe_token
        self.unsafe_token = unsafe_token
        self.max_tokens = max_tokens
        self.tokenizer = tokenizer
        self.logprobs = logprobs

    @abstractmethod
    def infer(self, prompt: partial) -> LLMGuardianProviderInfer:
        raise NotImplementedError("Subclass must implement this method")


class GuardianLLMConfig(BaseModel):
    """Configuration for LLM-based guardians."""
    model: str
    api_key: Optional[str] = None
    url: Optional[str] = None
    temperature: float
    logprobs: bool = False
    provider: Type[LLMGuardianProvider]
