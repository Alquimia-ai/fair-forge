"""Unit tests for {{MetricName}} metric."""

import pytest

from fair_forge.metrics.{{metric_name}} import {{MetricName}}
from fair_forge.schemas.{{metric_name}} import {{MetricName}}Metric
from tests.fixtures.mock_data import create_sample_batch, create_sample_dataset
from tests.fixtures.mock_retriever import MockRetriever


class Test{{MetricName}}Metric:
    """Test suite for {{MetricName}} metric."""

    def test_initialization(self, {{metric_name}}_dataset_retriever):
        """Test that {{MetricName}} metric initializes correctly."""
        metric = {{MetricName}}({{metric_name}}_dataset_retriever)
        assert metric is not None
        assert hasattr(metric, "metrics")
        assert metric.metrics == []
        assert hasattr(metric, "dataset")
        assert len(metric.dataset) > 0

    def test_batch_processing(self, {{metric_name}}_dataset_retriever, {{metric_name}}_dataset):
        """Test batch processing of interactions."""
        metric = {{MetricName}}({{metric_name}}_dataset_retriever)

        dataset = {{metric_name}}_dataset
        batch_interactions = dataset.conversation

        metric.batch(
            session_id=dataset.session_id,
            context=dataset.context,
            assistant_id=dataset.assistant_id,
            batch=batch_interactions,
            language=dataset.language,
        )

        assert len(metric.metrics) == len(batch_interactions)

        for m in metric.metrics:
            assert isinstance(m, {{MetricName}}Metric)
            assert hasattr(m, "{{metric_name}}_score")
            assert hasattr(m, "{{metric_name}}_insight")
            assert isinstance(m.{{metric_name}}_score, float)
            assert 0.0 <= m.{{metric_name}}_score <= 1.0

    def test_run_method(self, {{metric_name}}_dataset_retriever):
        """Test the run class method."""
        metrics = {{MetricName}}.run({{metric_name}}_dataset_retriever, verbose=False)

        assert isinstance(metrics, list)
        assert len(metrics) > 0

        for m in metrics:
            assert isinstance(m, {{MetricName}}Metric)

    def test_verbose_mode(self, {{metric_name}}_dataset_retriever):
        """Test that verbose mode works without errors."""
        metrics = {{MetricName}}.run({{metric_name}}_dataset_retriever, verbose=True)
        assert isinstance(metrics, list)

    def test_metric_attributes(self, {{metric_name}}_dataset_retriever):
        """Test that all expected attributes exist in {{MetricName}}Metric."""
        metrics = {{MetricName}}.run({{metric_name}}_dataset_retriever, verbose=False)

        assert len(metrics) > 0
        m = metrics[0]

        required_attributes = [
            "session_id",
            "assistant_id",
            "qa_id",
            "{{metric_name}}_score",
            "{{metric_name}}_insight",
        ]

        for attr in required_attributes:
            assert hasattr(m, attr), f"Missing attribute: {attr}"

    def test_with_custom_dataset(self, mock_retriever):
        """Test {{MetricName}} with a custom dataset."""
        batch = create_sample_batch(
            qa_id="custom_qa_001",
            query="Test query for {{metric_name}}",
            assistant="Test assistant response",
            ground_truth_assistant="Expected response",
        )

        dataset = create_sample_dataset(conversation=[batch])
        retriever = type(
            "TestRetriever",
            (MockRetriever,),
            {"load_dataset": lambda self: [dataset]},
        )

        metric = {{MetricName}}(retriever)
        metric.batch(
            session_id=dataset.session_id,
            context=dataset.context,
            assistant_id=dataset.assistant_id,
            batch=dataset.conversation,
            language=dataset.language,
        )

        assert len(metric.metrics) == 1
        result = metric.metrics[0]
        assert result.qa_id == "custom_qa_001"

    def test_empty_batch(self, {{metric_name}}_dataset_retriever, {{metric_name}}_dataset):
        """Test handling of empty batch."""
        metric = {{MetricName}}({{metric_name}}_dataset_retriever)

        metric.batch(
            session_id="empty_session",
            context="test context",
            assistant_id="test_assistant",
            batch=[],
            language="english",
        )

        # No metrics should be added for empty batch
        assert len(metric.metrics) == 0

    def test_score_bounds(self, {{metric_name}}_dataset_retriever):
        """Test that scores are within valid bounds."""
        metrics = {{MetricName}}.run({{metric_name}}_dataset_retriever, verbose=False)

        for m in metrics:
            assert 0.0 <= m.{{metric_name}}_score <= 1.0, (
                f"Score {m.{{metric_name}}_score} out of bounds for {m.qa_id}"
            )
