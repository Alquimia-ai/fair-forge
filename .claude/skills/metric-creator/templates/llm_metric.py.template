"""{{MetricName}} metric using LLM-as-judge for evaluation."""

from langchain_core.language_models.chat_models import BaseChatModel
from pydantic import BaseModel, Field

from fair_forge.core import FairForge, Retriever
from fair_forge.llm import Judge
from fair_forge.schemas import Batch
from fair_forge.schemas.{{metric_name}} import {{MetricName}}Metric


class {{MetricName}}JudgeOutput(BaseModel):
    """Structured output schema for LLM judge evaluation."""

    score: float = Field(ge=0.0, le=1.0, description="Evaluation score (0.0-1.0)")
    insight: str = Field(description="Brief explanation of the evaluation")
    # Add additional output fields as needed


# Define your system prompt for the LLM judge
{{METRIC_NAME_UPPER}}_SYSTEM_PROMPT = """You are an expert evaluator for AI assistant responses.
Your task is to evaluate the response for {{evaluation_criteria}}.

**Context:** {context}

**User Query:** {query}

**Assistant Response:** {assistant_answer}

**Ground Truth (if available):** {ground_truth}

Evaluate the response and provide:
1. A score from 0.0 to 1.0 where:
   - 1.0 = Excellent, fully meets criteria
   - 0.7-0.9 = Good, meets most criteria
   - 0.4-0.6 = Acceptable, partially meets criteria
   - 0.1-0.3 = Poor, fails to meet most criteria
   - 0.0 = Completely fails criteria

2. A brief insight explaining your evaluation

Be fair and consistent in your evaluation.
"""


class {{MetricName}}(FairForge):
    """{{Description}} using LLM-as-judge.

    This metric uses an LLM to evaluate AI assistant responses for {{metric_purpose}}.

    Args:
        retriever: Retriever class for loading datasets
        model: LangChain BaseChatModel instance for evaluation
        use_structured_output: If True, use LangChain's with_structured_output()
        bos_json_clause: Opening marker for JSON blocks in unstructured output
        eos_json_clause: Closing marker for JSON blocks in unstructured output
        **kwargs: Additional arguments passed to FairForge base class

    Example:
        >>> from langchain_groq import ChatGroq
        >>> model = ChatGroq(model="llama-3.3-70b-versatile", temperature=0)
        >>> results = {{MetricName}}.run(MyRetriever, model=model)
    """

    def __init__(
        self,
        retriever: type[Retriever],
        model: BaseChatModel,
        use_structured_output: bool = False,
        bos_json_clause: str = "```json",
        eos_json_clause: str = "```",
        **kwargs,
    ):
        """Initialize {{MetricName}} metric.

        Args:
            retriever: Data retriever class
            model: LangChain chat model for LLM-based evaluation
            use_structured_output: Whether to use structured output parsing
            bos_json_clause: Begin of JSON marker for parsing
            eos_json_clause: End of JSON marker for parsing
            **kwargs: Additional configuration
        """
        super().__init__(retriever, **kwargs)
        self.model = model
        self.use_structured_output = use_structured_output
        self.bos_json_clause = bos_json_clause
        self.eos_json_clause = eos_json_clause

        self.logger.info("--{{METRIC_NAME_UPPER}} CONFIGURATION--")
        self.logger.debug(f"Model: {model.__class__.__name__}")
        self.logger.debug(f"Structured output: {use_structured_output}")

    def batch(
        self,
        session_id: str,
        context: str,
        assistant_id: str,
        batch: list[Batch],
        language: str | None = "english",
    ):
        """Process a batch of conversations using LLM judge.

        Args:
            session_id: Unique session identifier
            context: Context information for the conversation
            assistant_id: ID of the assistant being evaluated
            batch: List of Q&A interactions to evaluate
            language: Language of the conversation
        """
        judge = Judge(
            model=self.model,
            use_structured_output=self.use_structured_output,
            bos_json_clause=self.bos_json_clause,
            eos_json_clause=self.eos_json_clause,
            verbose=self.verbose,
        )

        for interaction in batch:
            self.logger.debug(f"QA ID: {interaction.qa_id}")

            data = {
                "context": context,
                "query": interaction.query,
                "assistant_answer": interaction.assistant,
                "ground_truth": interaction.ground_truth_assistant or "Not provided",
            }

            reasoning, result = judge.check(
                {{METRIC_NAME_UPPER}}_SYSTEM_PROMPT,
                interaction.query,
                data,
                output_schema={{MetricName}}JudgeOutput,
            )

            if result is None:
                raise ValueError(
                    f"[FAIR FORGE/{{METRIC_NAME_UPPER}}] No valid response from judge "
                    f"for QA ID: {interaction.qa_id}"
                )

            # Handle both Pydantic model and dict results
            if isinstance(result, dict):
                score = result["score"]
                insight = result["insight"]
            else:
                score = result.score
                insight = result.insight

            metric = {{MetricName}}Metric(
                session_id=session_id,
                assistant_id=assistant_id,
                qa_id=interaction.qa_id,
                {{metric_name}}_score=score,
                {{metric_name}}_insight=insight,
                {{metric_name}}_reasoning=reasoning,
            )

            self.logger.debug(f"Score: {metric.{{metric_name}}_score}")
            self.logger.debug(f"Insight: {metric.{{metric_name}}_insight}")
            self.metrics.append(metric)
