\documentclass[12pt]{article}

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm

\title{Fair Forge: A Framework for Explainable and Fair AI Assistant Evaluation Through Comprehensive Metrics and Assurance} 

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{cleveref}

\begin{document}

\maketitle

\section{Humanity Metrics in AI Assistant Evaluation}

The assessment of human-like interaction in AI assistants requires sophisticated metrics that capture both emotional complexity and response alignment. This section presents two key metrics: Emotional Entropy and Ground Truth Spearman Correlation.

\subsection{Emotional Entropy}

Emotional entropy quantifies the diversity and natural distribution of emotions in AI responses, based on Plutchik's Wheel of Emotions \cite{plutchik2001nature}. Given a vocabulary $V$ and the NRC Emotion Lexicon dataset \cite{mohammad2013nrc}, we define the emotional distribution as follows:

For each word $w \in V$, we have a set of emotions $E = \{e_1, e_2, ..., e_8\}$ corresponding to Plutchik's eight basic emotions. For a given response $R$, we calculate the probability distribution of emotions $P(e|R)$ as:

\begin{equation}
    P(e|R) = \frac{\sum_{w \in R} \mathbb{I}(e \in E_w)}{\sum_{e' \in E} \sum_{w \in R} \mathbb{I}(e' \in E_w)}
\end{equation}

where $\mathbb{I}$ is the indicator function and $E_w$ represents the set of emotions associated with word $w$.

The emotional entropy $H(R)$ is then calculated using Shannon's entropy formula:

\begin{equation}
    H(R) = -\sum_{e \in E} P(e|R) \log_2 P(e|R)
\end{equation}

This metric provides a measure of emotional diversity in the response, where:
\begin{itemize}
    \item Higher entropy indicates more diverse and natural emotional expression
    \item Lower entropy suggests more focused or limited emotional range
\end{itemize}

\subsection{Ground Truth Spearman Correlation}

To evaluate how well an AI assistant's emotional response aligns with expected human responses, we employ Spearman's rank correlation coefficient. Given the emotional distributions of the AI response $P_{AI}(e|R)$ and the ground truth response $P_{GT}(e|R)$, we calculate the correlation as:

\begin{equation}
    \rho = 1 - \frac{6\sum_{i=1}^{n} d_i^2}{n(n^2-1)}
\end{equation}

where $d_i$ is the difference between the ranks of corresponding emotions in $P_{AI}$ and $P_{GT}$, and $n$ is the number of emotions (8 in our case).

The correlation coefficient $\rho$ ranges from -1 to 1, where:
\begin{itemize}
    \item $\rho = 1$ indicates perfect positive correlation
    \item $\rho = 0$ indicates no correlation
    \item $\rho = -1$ indicates perfect negative correlation
\end{itemize}

\section{Bias and Risk Assessment Metrics}

The evaluation of AI assistant interactions requires robust mechanisms to detect and mitigate potential biases and risks. This section presents a comprehensive framework for bias assessment and risk detection using the Granite Guardian model \cite{ibm2024granite} and AI ATLAS risk framework \cite{ibm2024atlas}.

\subsection{Bias Metric Implementation}

The bias metric implementation follows a flexible guardian-based approach, where different guardian models can be used to assess bias in AI responses. The core bias assessment is defined as:

\begin{equation}
    B(R) = \sum_{i=1}^{n} w_i G_i(R)
\end{equation}

where:
\begin{itemize}
    \item $B(R)$ is the total bias score for response $R$
    \item $G_i(R)$ represents the assessment from the $i$th guardian
    \item $w_i$ are the weights assigned to each guardian
    \item $n$ is the number of guardians used
\end{itemize}

Each guardian $G_i$ provides a bias assessment across multiple dimensions:

\begin{equation}
    G_i(R) = \frac{1}{m}\sum_{j=1}^{m} d_{ij}(R)
\end{equation}

where:
\begin{itemize}
    \item $d_{ij}(R)$ represents the $j$th dimension score from guardian $i$
    \item $m$ is the number of bias dimensions assessed
\end{itemize}

The bias dimensions include:
\begin{enumerate}
    \item \textbf{Demographic Bias}: Assessment of unfair treatment based on demographic characteristics
    \item \textbf{Cultural Bias}: Evaluation of cultural insensitivity or stereotyping
    \item \textbf{Language Bias}: Detection of discriminatory language patterns
    \item \textbf{Content Bias}: Analysis of biased content or viewpoints
\end{enumerate}

\subsection{Confidence Intervals and Statistical Analysis}

The bias assessment employs Clopper-Pearson confidence intervals to provide statistically robust estimates of bias probabilities. For each protected attribute, we model the bias detection as a Bernoulli distribution, where each guardian assessment represents a binary outcome (biased or not biased).

Given $n$ samples and $k$ successful outcomes (non-biased responses), the Clopper-Pearson confidence interval is calculated as:

\begin{equation}
    [p_l, p_u] = [\beta_{\alpha/2}(k, n-k+1), \beta_{1-\alpha/2}(k+1, n-k)]
\end{equation}

where:
\begin{itemize}
    \item $p_l$ is the lower bound of the confidence interval
    \item $p_u$ is the upper bound of the confidence interval
    \item $\beta_{\alpha}(a,b)$ is the $\alpha$ quantile of the Beta distribution with parameters $a$ and $b$
    \item $\alpha$ is the significance level (1 - confidence level)
\end{itemize}

The true probability of a non-biased response is estimated as:

\begin{equation}
    p_{truth} = \frac{k}{n}
\end{equation}


\subsection{Risk Detection Framework}

The risk assessment framework operates across three primary dimensions:

\begin{equation}
    R_{total} = \sum_{i=1}^{3} w_i R_i
\end{equation}

where $R_i$ represents the risk scores for each dimension and $w_i$ are their respective weights. The three primary dimensions are:

\begin{enumerate}
    \item \textbf{Prompt Risk} ($R_1$): Assessment of user-supplied text
    \item \textbf{Response Risk} ($R_2$): Evaluation of model-generated content
    \item \textbf{Context Risk} ($R_3$): Analysis of retrieved information relevance
\end{enumerate}

\section{Conversational Quality Metrics}

The assessment of conversational quality in AI assistants requires a multi-dimensional approach that evaluates various aspects of human-like communication. This section presents a comprehensive framework for evaluating conversational quality through multiple metrics.

\subsection{Memory and Context Retention}

The memory score $M$ is evaluated on a scale of 0 to 10, measuring the assistant's ability to maintain context and recall previous interactions:

\begin{equation}
    M = \frac{1}{n}\sum_{i=1}^{n} m_i
\end{equation}

where $m_i$ represents individual memory assessments for $n$ previous interactions, evaluated by an LLM judge.

\subsection{Language Adaptation}

The language score $L$ measures the assistant's ability to adapt to the user's preferred language:

\begin{equation}
    L = \sum_{i=1}^{k} w_i l_i
\end{equation}

where:
\begin{itemize}
    \item $l_i$ represents different aspects of language adaptation
    \item $w_i$ are weighting factors for each aspect
    \item $k$ is the number of language adaptation criteria
\end{itemize}

\subsection{Grice's Maxims Compliance}

Following Grice's Cooperative Principle \cite{grice1975logic}, we evaluate the assistant's adherence to four fundamental maxims:

\begin{enumerate}
    \item \textbf{Maxim of Quantity}: Information should be as informative as required
    \item \textbf{Maxim of Quality}: Information should be true and supported by evidence
    \item \textbf{Maxim of Relation}: Information should be relevant to the conversation
    \item \textbf{Maxim of Manner}: Information should be clear and unambiguous
\end{enumerate}

The Gricean compliance score $G$ is calculated as:

\begin{equation}
    G = \frac{1}{4}\sum_{i=1}^{4} g_i
\end{equation}

where $g_i$ represents the compliance score for each maxim, evaluated on a scale of 0 to 1.

\subsection{Sensibleness and Specificity}

Based on the Sensibleness and Specificity Average (SSA) metric \cite{adolphs2020evaluation}, we define a composite score:

\begin{equation}
    SSA = \frac{S + Sp}{2}
\end{equation}

where:
\begin{itemize}
    \item $S$ is the sensibleness score
    \item $Sp$ is the specificity score
\end{itemize}

The sensibleness score $S$ evaluates whether the response makes sense in the given context, while the specificity score $Sp$ measures how specific and detailed the response is.


and $\gamma_i$ are the respective weights that sum to 1.

\section{Context Adherence Metrics}

The evaluation of context adherence in AI assistant responses is crucial for ensuring relevant and appropriate interactions. This section presents a framework for assessing how well an assistant's responses align with the provided context and expected outcomes.

\subsection{Context Evaluation Framework}

The context evaluation process employs an LLM-as-a-judge approach, where a specialized language model (specifically deepseek-r1) evaluates the following components:

\begin{enumerate}
    \item \textbf{Context}: The provided background information and conversation history
    \item \textbf{Human Question}: The user's query or input
    \item \textbf{Assistant Answer}: The actual response generated by the AI assistant
    \item \textbf{Ground Truth/Observation}: The expected or ideal response
\end{enumerate}

\subsection{Evaluation Process}

The evaluation process follows a structured approach:

\begin{enumerate}
    \item \textbf{Context Analysis}: The judge model analyzes the provided context and its relevance to the conversation
    \item \textbf{Response Assessment}: The assistant's answer is evaluated against the ground truth
    \item \textbf{Scoring}: A numerical score is assigned based on context adherence
    \item \textbf{Insight Generation}: The judge provides detailed reasoning for the assigned score
\end{enumerate}

\subsection{Chain-of-Thought Evaluation}

The evaluation process is enhanced by the judge model's ability to provide its reasoning through Chain-of-Thought (CoT) analysis. This includes:

\begin{itemize}
    \item Step-by-step reasoning about context relevance
    \item Analysis of response alignment with ground truth
    \item Identification of potential context mismatches
    \item Suggestions for improvement
\end{itemize}

\subsection{Storage and Analysis}

The evaluation results are stored in an Elasticsearch database, containing:

\begin{itemize}
    \item Context adherence scores
    \item Generated insights
    \item Complete thinking process
    \item Ground truth comparisons
    \item Timestamps and metadata
\end{itemize}

This structured storage enables:
\begin{itemize}
    \item Longitudinal analysis of context adherence
    \item Pattern identification in context mismatches
    \item Performance tracking over time
    \item Quality improvement opportunities
\end{itemize}

\subsection{Integration with Other Metrics}

The context adherence evaluation complements other metrics by providing:
\begin{itemize}
    \item Additional validation of response quality
    \item Insights into context-aware performance
    \item Ground truth alignment verification
    \item Continuous improvement feedback
\end{itemize}

\section{Toxicity Detection Through Clustering Analysis}

The toxicity metric provides automated detection and quantification of toxic language patterns in AI assistant responses. Unlike the bias metric which focuses on fairness across protected attributes, the toxicity metric specifically identifies harmful, offensive, or inappropriate language through advanced clustering techniques and lexicon-based analysis.

\subsection{Clustering Analysis and Latent Space Representation}

The toxicity analysis incorporates a sophisticated clustering approach to identify patterns of toxic language in assistant responses. The process involves several key steps:

\subsubsection{Embedding Generation}

Responses are first converted into dense vector representations using a sentence transformer model:

\begin{equation}
    E = \text{Transformer}(R)
\end{equation}

where $R$ represents the set of assistant responses and $E$ is the resulting embedding matrix.

\subsubsection{Dimensionality Reduction}

The high-dimensional embeddings are reduced to a lower-dimensional space using UMAP (Uniform Manifold Approximation and Projection) \cite{mcinnes2018umap}:

\begin{equation}
    L = \text{UMAP}(E, n_{components}, n_{neighbors}, min_{dist})
\end{equation}

where:
\begin{itemize}
    \item $L$ is the reduced latent space representation
    \item $n_{components}$ is the target dimensionality (default: 2)
    \item $n_{neighbors}$ is the number of neighbors for local structure preservation
    \item $min_{dist}$ is the minimum distance between points in the embedding
\end{itemize}

\subsubsection{Cluster Detection}

The latent space is then analyzed using HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) \cite{mcinnes2017hdbscan}:

\begin{equation}
    C = \text{HDBSCAN}(L, min_{cluster\_size}, \epsilon)
\end{equation}

where:
\begin{itemize}
    \item $C$ represents the cluster assignments
    \item $min_{cluster\_size}$ is the minimum number of samples in a cluster
    \item $\epsilon$ is the cluster selection epsilon parameter
\end{itemize}

\subsection{Toxicity Scoring}

Each cluster is evaluated for potential toxicity using lexicon-based analysis combined with a Laplace-smoothed scoring mechanism. The toxicity score for cluster $c$ is calculated as:

\begin{equation}
    \text{ToxicityScore}(c) = \frac{n_{toxic}}{n_{total}}
\end{equation}

where:
\begin{itemize}
    \item $n_{toxic}$ is the count of toxic words from the lexicon found in cluster $c$
    \item $n_{total}$ is the total word count in cluster $c$
\end{itemize}

The toxicity lexicon used is the HurtLex database \cite{bassignana2018hurtlex}, which categorizes offensive terms across multiple dimensions including:
\begin{enumerate}
    \item Negative stereotypes ethnic slurs
    \item Profanity and vulgarity
    \item Animal metaphors
    \item Physical disabilities and diversity
    \item Cognitive disabilities and diversity
    \item Moral and behavioral defects
\end{enumerate}

\subsection{Clustering Validation}

The quality of the clustering is assessed using silhouette scores:

\begin{equation}
    s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
\end{equation}

where:
\begin{itemize}
    \item $a(i)$ is the mean distance between sample $i$ and all other points in the same cluster
    \item $b(i)$ is the mean distance between sample $i$ and all points in the nearest cluster
\end{itemize}

\section{Best-of-N Evaluation Through Tournament Selection}

The BestOf metric implements a tournament-style evaluation framework to determine the optimal response among multiple candidates. This approach mirrors human evaluation processes and provides robust, comparative assessment of response quality.

\subsection{Tournament Structure}

For a set of $n$ candidate responses $\mathcal{R} = \{r_1, r_2, \ldots, r_n\}$, the tournament is organized as a single-elimination bracket:

\begin{equation}
    \text{Rounds} = \lceil \log_2(n) \rceil
\end{equation}

Each match in round $k$ is defined as:

\begin{equation}
    M_k^{(i,j)} = \text{Judge}(r_i, r_j, q, c)
\end{equation}

where:
\begin{itemize}
    \item $q$ is the user query
    \item $c$ is the conversation context
    \item $\text{Judge}$ is an LLM-based evaluation function
\end{itemize}

\subsection{Pairwise Comparison}

For each match, the judge evaluates both responses according to multiple criteria:

\begin{equation}
    \text{Score}(r_i, r_j) = \sum_{k=1}^{m} w_k \cdot f_k(r_i, r_j)
\end{equation}

where:
\begin{itemize}
    \item $f_k$ represents individual evaluation criteria (relevance, accuracy, coherence, helpfulness)
    \item $w_k$ are the weights assigned to each criterion
    \item $m$ is the total number of criteria
\end{itemize}

\subsection{Winner Selection}

The winner of each match is determined by:

\begin{equation}
    \text{Winner}(r_i, r_j) = \begin{cases}
        r_i & \text{if } \text{Score}(r_i, r_j) > \text{Score}(r_j, r_i) \\
        r_j & \text{otherwise}
    \end{cases}
\end{equation}

Each evaluation includes:
\begin{itemize}
    \item Confidence score $\in [0, 1]$
    \item Detailed verdict explanation
    \item Structured reasoning chain
    \item Comparative analysis of strengths and weaknesses
\end{itemize}

\subsection{Metrics Output}

The final BestOf metric produces:

\begin{equation}
    \text{BestOfMetric} = \{w_{final}, \mathcal{C}, \theta\}
\end{equation}

where:
\begin{itemize}
    \item $w_{final}$ is the tournament winner
    \item $\mathcal{C}$ is the complete set of contests with verdicts
    \item $\theta$ is the aggregate confidence score
\end{itemize}

\begin{thebibliography}{9}
\bibitem{plutchik2001nature}
Plutchik, R. (2001).
\newblock The nature of emotions.
\newblock {\em American Scientist}, 89(4), 344-350.

\bibitem{mohammad2013nrc}
Mohammad, S. M., \& Turney, P. D. (2013).
\newblock Crowdsourcing a word-emotion association lexicon.
\newblock {\em Computational Intelligence}, 29(3), 436-465.

\bibitem{ibm2024granite}
IBM. (2024).
\newblock Granite Guardian: Enterprise-grade risk detection model.
\newblock {\em Hugging Face Model Hub}.

\bibitem{grice1975logic}
Grice, H. P. (1975).
\newblock Logic and conversation.
\newblock {\em Syntax and Semantics}, 3, 41-58.

\bibitem{adolphs2020evaluation}
Adolphs, L., et al. (2020).
\newblock Evaluation of neural response generation models.
\newblock {\em arXiv preprint arXiv:2001.09977}.

\bibitem{deepseek2024}
DeepSeek. (2024).
\newblock DeepSeek-R1: Advanced reasoning model.
\newblock {\em DeepSeek AI Documentation}.

\bibitem{mcinnes2018umap}
McInnes, L., Healy, J., \& Melville, J. (2018).
\newblock UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.
\newblock {\em arXiv preprint arXiv:1802.03426}.

\bibitem{mcinnes2017hdbscan}
McInnes, L., Healy, J., \& Astels, S. (2017).
\newblock hdbscan: Hierarchical density based clustering.
\newblock {\em The Journal of Open Source Software}, 2(11), 205.

\bibitem{bassignana2018hurtlex}
Bassignana, E., Basile, V., \& Patti, V. (2018).
\newblock Hurtlex: A multilingual lexicon of words to hurt.
\newblock {\em Proceedings of the Fifth Italian Conference on Computational Linguistics (CLiC-it 2018)}.
\end{thebibliography}

\end{document}
