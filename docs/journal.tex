\documentclass[12pt]{article}

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm

\title{Fair Forge: A Framework for Explainable and Fair AI Assistant Evaluation Through Comprehensive Metrics and Assurance} 

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{cleveref}

\begin{document}

\maketitle

\section{Humanity Metrics in AI Assistant Evaluation}

The assessment of human-like interaction in AI assistants requires sophisticated metrics that capture both emotional complexity and response alignment. This section presents two key metrics: Emotional Entropy and Ground Truth Spearman Correlation.

\subsection{Emotional Entropy}

Emotional entropy quantifies the diversity and natural distribution of emotions in AI responses, based on Plutchik's Wheel of Emotions \cite{plutchik2001nature}. Given a vocabulary $V$ and the NRC Emotion Lexicon dataset \cite{mohammad2013nrc}, we define the emotional distribution as follows:

For each word $w \in V$, we have a set of emotions $E = \{e_1, e_2, ..., e_8\}$ corresponding to Plutchik's eight basic emotions. For a given response $R$, we calculate the probability distribution of emotions $P(e|R)$ as:

\begin{equation}
    P(e|R) = \frac{\sum_{w \in R} \mathbb{I}(e \in E_w)}{\sum_{e' \in E} \sum_{w \in R} \mathbb{I}(e' \in E_w)}
\end{equation}

where $\mathbb{I}$ is the indicator function and $E_w$ represents the set of emotions associated with word $w$.

The emotional entropy $H(R)$ is then calculated using Shannon's entropy formula:

\begin{equation}
    H(R) = -\sum_{e \in E} P(e|R) \log_2 P(e|R)
\end{equation}

This metric provides a measure of emotional diversity in the response, where:
\begin{itemize}
    \item Higher entropy indicates more diverse and natural emotional expression
    \item Lower entropy suggests more focused or limited emotional range
\end{itemize}

\subsection{Ground Truth Spearman Correlation}

To evaluate how well an AI assistant's emotional response aligns with expected human responses, we employ Spearman's rank correlation coefficient. Given the emotional distributions of the AI response $P_{AI}(e|R)$ and the ground truth response $P_{GT}(e|R)$, we calculate the correlation as:

\begin{equation}
    \rho = 1 - \frac{6\sum_{i=1}^{n} d_i^2}{n(n^2-1)}
\end{equation}

where $d_i$ is the difference between the ranks of corresponding emotions in $P_{AI}$ and $P_{GT}$, and $n$ is the number of emotions (8 in our case).

The correlation coefficient $\rho$ ranges from -1 to 1, where:
\begin{itemize}
    \item $\rho = 1$ indicates perfect positive correlation
    \item $\rho = 0$ indicates no correlation
    \item $\rho = -1$ indicates perfect negative correlation
\end{itemize}

\section{Bias and Risk Assessment Metrics}

The evaluation of AI assistant interactions requires robust mechanisms to detect and mitigate potential biases and risks. This section presents a comprehensive framework for bias assessment and risk detection using the Granite Guardian model \cite{ibm2024granite} and AI ATLAS risk framework \cite{ibm2024atlas}.

\subsection{Bias Metric Implementation}

The bias metric implementation follows a flexible guardian-based approach, where different guardian models can be used to assess bias in AI responses. The core bias assessment is defined as:

\begin{equation}
    B(R) = \sum_{i=1}^{n} w_i G_i(R)
\end{equation}

where:
\begin{itemize}
    \item $B(R)$ is the total bias score for response $R$
    \item $G_i(R)$ represents the assessment from the $i$th guardian
    \item $w_i$ are the weights assigned to each guardian
    \item $n$ is the number of guardians used
\end{itemize}

Each guardian $G_i$ provides a bias assessment across multiple dimensions:

\begin{equation}
    G_i(R) = \frac{1}{m}\sum_{j=1}^{m} d_{ij}(R)
\end{equation}

where:
\begin{itemize}
    \item $d_{ij}(R)$ represents the $j$th dimension score from guardian $i$
    \item $m$ is the number of bias dimensions assessed
\end{itemize}

The bias dimensions include:
\begin{enumerate}
    \item \textbf{Demographic Bias}: Assessment of unfair treatment based on demographic characteristics
    \item \textbf{Cultural Bias}: Evaluation of cultural insensitivity or stereotyping
    \item \textbf{Language Bias}: Detection of discriminatory language patterns
    \item \textbf{Content Bias}: Analysis of biased content or viewpoints
\end{enumerate}

\subsection{Confidence Intervals and Statistical Analysis}

The bias assessment employs Clopper-Pearson confidence intervals to provide statistically robust estimates of bias probabilities. For each protected attribute, we model the bias detection as a Bernoulli distribution, where each guardian assessment represents a binary outcome (biased or not biased).

Given $n$ samples and $k$ successful outcomes (non-biased responses), the Clopper-Pearson confidence interval is calculated as:

\begin{equation}
    [p_l, p_u] = [\beta_{\alpha/2}(k, n-k+1), \beta_{1-\alpha/2}(k+1, n-k)]
\end{equation}

where:
\begin{itemize}
    \item $p_l$ is the lower bound of the confidence interval
    \item $p_u$ is the upper bound of the confidence interval
    \item $\beta_{\alpha}(a,b)$ is the $\alpha$ quantile of the Beta distribution with parameters $a$ and $b$
    \item $\alpha$ is the significance level (1 - confidence level)
\end{itemize}

The true probability of a non-biased response is estimated as:

\begin{equation}
    p_{truth} = \frac{k}{n}
\end{equation}


\subsection{Risk Detection Framework}

The risk assessment framework operates across three primary dimensions:

\begin{equation}
    R_{total} = \sum_{i=1}^{3} w_i R_i
\end{equation}

where $R_i$ represents the risk scores for each dimension and $w_i$ are their respective weights. The three primary dimensions are:

\begin{enumerate}
    \item \textbf{Prompt Risk} ($R_1$): Assessment of user-supplied text
    \item \textbf{Response Risk} ($R_2$): Evaluation of model-generated content
    \item \textbf{Context Risk} ($R_3$): Analysis of retrieved information relevance
\end{enumerate}

\section{Conversational Quality Metrics}

The assessment of conversational quality in AI assistants requires a multi-dimensional approach that evaluates various aspects of human-like communication. This section presents a comprehensive framework for evaluating conversational quality through multiple metrics.

\subsection{Memory and Context Retention}

The memory score $M$ is evaluated on a scale of 0 to 10, measuring the assistant's ability to maintain context and recall previous interactions:

\begin{equation}
    M = \frac{1}{n}\sum_{i=1}^{n} m_i
\end{equation}

where $m_i$ represents individual memory assessments for $n$ previous interactions, evaluated by an LLM judge.

\subsection{Language Adaptation}

The language score $L$ measures the assistant's ability to adapt to the user's preferred language:

\begin{equation}
    L = \sum_{i=1}^{k} w_i l_i
\end{equation}

where:
\begin{itemize}
    \item $l_i$ represents different aspects of language adaptation
    \item $w_i$ are weighting factors for each aspect
    \item $k$ is the number of language adaptation criteria
\end{itemize}

\subsection{Grice's Maxims Compliance}

Following Grice's Cooperative Principle \cite{grice1975logic}, we evaluate the assistant's adherence to four fundamental maxims:

\begin{enumerate}
    \item \textbf{Maxim of Quantity}: Information should be as informative as required
    \item \textbf{Maxim of Quality}: Information should be true and supported by evidence
    \item \textbf{Maxim of Relation}: Information should be relevant to the conversation
    \item \textbf{Maxim of Manner}: Information should be clear and unambiguous
\end{enumerate}

The Gricean compliance score $G$ is calculated as:

\begin{equation}
    G = \frac{1}{4}\sum_{i=1}^{4} g_i
\end{equation}

where $g_i$ represents the compliance score for each maxim, evaluated on a scale of 0 to 1.

\subsection{Sensibleness and Specificity}

Based on the Sensibleness and Specificity Average (SSA) metric \cite{adolphs2020evaluation}, we define a composite score:

\begin{equation}
    SSA = \frac{S + Sp}{2}
\end{equation}

where:
\begin{itemize}
    \item $S$ is the sensibleness score
    \item $Sp$ is the specificity score
\end{itemize}

The sensibleness score $S$ evaluates whether the response makes sense in the given context, while the specificity score $Sp$ measures how specific and detailed the response is.


and $\gamma_i$ are the respective weights that sum to 1.

\section{Context Adherence Metrics}

The evaluation of context adherence in AI assistant responses is crucial for ensuring relevant and appropriate interactions. This section presents a framework for assessing how well an assistant's responses align with the provided context and expected outcomes.

\subsection{Context Evaluation Framework}

The context evaluation process employs an LLM-as-a-judge approach, where a specialized language model (specifically deepseek-r1) evaluates the following components:

\begin{enumerate}
    \item \textbf{Context}: The provided background information and conversation history
    \item \textbf{Human Question}: The user's query or input
    \item \textbf{Assistant Answer}: The actual response generated by the AI assistant
    \item \textbf{Ground Truth/Observation}: The expected or ideal response
\end{enumerate}

\subsection{Evaluation Process}

The evaluation process follows a structured approach:

\begin{enumerate}
    \item \textbf{Context Analysis}: The judge model analyzes the provided context and its relevance to the conversation
    \item \textbf{Response Assessment}: The assistant's answer is evaluated against the ground truth
    \item \textbf{Scoring}: A numerical score is assigned based on context adherence
    \item \textbf{Insight Generation}: The judge provides detailed reasoning for the assigned score
\end{enumerate}

\subsection{Chain-of-Thought Evaluation}

The evaluation process is enhanced by the judge model's ability to provide its reasoning through Chain-of-Thought (CoT) analysis. This includes:

\begin{itemize}
    \item Step-by-step reasoning about context relevance
    \item Analysis of response alignment with ground truth
    \item Identification of potential context mismatches
    \item Suggestions for improvement
\end{itemize}

\subsection{Storage and Analysis}

The evaluation results are stored in an Elasticsearch database, containing:

\begin{itemize}
    \item Context adherence scores
    \item Generated insights
    \item Complete thinking process
    \item Ground truth comparisons
    \item Timestamps and metadata
\end{itemize}

This structured storage enables:
\begin{itemize}
    \item Longitudinal analysis of context adherence
    \item Pattern identification in context mismatches
    \item Performance tracking over time
    \item Quality improvement opportunities
\end{itemize}

\subsection{Integration with Other Metrics}

The context adherence evaluation complements other metrics by providing:
\begin{itemize}
    \item Additional validation of response quality
    \item Insights into context-aware performance
    \item Ground truth alignment verification
    \item Continuous improvement feedback
\end{itemize}

\section{Toxicity Detection with Statistical Modes and DIDT Framework}

The toxicity metric provides comprehensive detection and quantification of toxic language patterns in AI assistant responses with a focus on fairness across demographic groups. The metric combines clustering analysis, lexicon-based detection, and a rigorous fairness framework (DIDT) with pluggable statistical computation modes (Frequentist and Bayesian).

\subsection{Statistical Computation Modes (Strategy Pattern)}

The toxicity metric implements the Strategy Pattern to support multiple statistical computation approaches without coupling to specific implementations. This allows users to choose between frequentist point estimates or full Bayesian posterior distributions.

\subsubsection{StatisticalMode Interface}

All statistical modes provide four core primitives:

\begin{enumerate}
    \item \textbf{Distribution Divergence}: Compute divergence between observed and reference distributions
    \item \textbf{Rate Estimation}: Estimate rates with uncertainty quantification
    \item \textbf{Aggregation}: Combine multiple metrics with weights
    \item \textbf{Dispersion}: Compute spread/variability across groups
\end{enumerate}

\subsubsection{Frequentist Mode}

Returns point estimates (scalars):

\begin{equation}
    D_{KL}(P||Q) = \sum_{i} p_i \log\frac{p_i}{q_i}
\end{equation}

where $P$ is the observed distribution and $Q$ is the reference distribution.

Rate estimation uses maximum likelihood:

\begin{equation}
    \hat{\theta} = \frac{k}{n}
\end{equation}

where $k$ is the number of successes and $n$ is the number of trials.

\subsubsection{Bayesian Mode}

Returns full posterior distributions with credible intervals:

For distribution divergence, we use a Dirichlet-Multinomial model with prior $\alpha$:

\begin{equation}
    P(p|\mathbf{n}, \alpha) = \text{Dir}(p|\mathbf{n} + \alpha)
\end{equation}

where $\mathbf{n}$ are observed counts. Divergence is computed via Monte Carlo sampling:

\begin{equation}
    D_{KL} \approx \frac{1}{S}\sum_{s=1}^{S} D_{KL}(p^{(s)}||q)
\end{equation}

For rate estimation, we use a Beta-Binomial model:

\begin{equation}
    P(\theta|k,n) = \text{Beta}(\theta|k + \alpha, n - k + \beta)
\end{equation}

\subsection{The DIDT Fairness Framework}

The DIDT framework quantifies fairness across demographic groups through four components:

\subsubsection{DR: Demographic Representation}

Measures how evenly demographic groups are represented:

\begin{equation}
    \text{DR} = D_{KL}(P||Q)
\end{equation}

where:
\begin{itemize}
    \item $P = \{p_i\}$ is the observed distribution of group mentions
    \item $Q = \{q_i\}$ is the reference (expected) distribution
    \item Lower DR indicates more balanced representation
\end{itemize}

\subsubsection{DTO: Directed Toxicity per Group}

Measures dispersion in toxicity rates across groups:

\begin{equation}
    \text{DTO} = \text{Var}(\{T_i\})
\end{equation}

where $T_i = k_i / n_i$ is the toxicity rate for group $i$, with $k_i$ toxic mentions out of $n_i$ total mentions.

In Bayesian mode:

\begin{equation}
    T_i \sim \text{Beta}(k_i + \alpha, n_i - k_i + \beta)
\end{equation}

\subsubsection{ASB: Associated Sentiment Bias}

Placeholder for sentiment analysis across groups (currently returns 0):

\begin{equation}
    \text{ASB} = 0
\end{equation}

\subsubsection{DIDT: Composite Index}

Weighted aggregation of all components:

\begin{equation}
    \text{DIDT} = w_{\text{DR}} \cdot \text{DR} + w_{\text{DTO}} \cdot \text{DTO} + w_{\text{ASB}} \cdot \text{ASB}
\end{equation}

where weights are normalized: $\sum w_i = 1$ (default: $w_i = 1/3$).

\subsection{Group Detection via Embedding Similarity}

Groups are detected using cosine similarity between response embeddings and prototype embeddings:

\begin{equation}
    \text{sim}(r, p_g) = \frac{r \cdot p_g}{\|r\| \|p_g\|}
\end{equation}

A response $r$ is assigned to group $g$ if:

\begin{equation}
    \text{sim}(r, p_g) > \tau_g
\end{equation}

where $\tau_g$ is the threshold for group $g$ (default: 0.5).

\subsection{Clustering Analysis and Latent Space Representation}

In addition to group-based fairness analysis, the metric performs clustering to identify toxic patterns.

\subsubsection{Embedding Generation}

Responses are converted into dense vectors using sentence transformers:

\begin{equation}
    E = \text{Transformer}(R)
\end{equation}

\subsubsection{Dimensionality Reduction}

UMAP reduces embeddings to latent space \cite{mcinnes2018umap}:

\begin{equation}
    L = \text{UMAP}(E, n_{components}, n_{neighbors}, min_{dist})
\end{equation}

\subsubsection{Cluster Detection}

HDBSCAN identifies clusters in latent space \cite{mcinnes2017hdbscan}:

\begin{equation}
    C = \text{HDBSCAN}(L, min_{cluster\_size}, \epsilon)
\end{equation}

\subsubsection{Cluster Toxicity Scoring}

Each cluster $c$ receives a toxicity score:

\begin{equation}
    \text{ToxicityScore}(c) = \frac{n_{toxic}}{n_{total}}
\end{equation}

where $n_{toxic}$ is the count of toxic words from HurtLex \cite{bassignana2018hurtlex} found in cluster $c$.

\section{Best-of-N Evaluation Through Tournament Selection}

The BestOf metric implements a tournament-style evaluation framework to determine the optimal response among multiple candidates. This approach mirrors human evaluation processes and provides robust, comparative assessment of response quality.

\subsection{Tournament Structure}

For a set of $n$ candidate responses $\mathcal{R} = \{r_1, r_2, \ldots, r_n\}$, the tournament is organized as a single-elimination bracket:

\begin{equation}
    \text{Rounds} = \lceil \log_2(n) \rceil
\end{equation}

Each match in round $k$ is defined as:

\begin{equation}
    M_k^{(i,j)} = \text{Judge}(r_i, r_j, q, c)
\end{equation}

where:
\begin{itemize}
    \item $q$ is the user query
    \item $c$ is the conversation context
    \item $\text{Judge}$ is an LLM-based evaluation function
\end{itemize}

\subsection{Pairwise Comparison}

For each match, the judge evaluates both responses according to multiple criteria:

\begin{equation}
    \text{Score}(r_i, r_j) = \sum_{k=1}^{m} w_k \cdot f_k(r_i, r_j)
\end{equation}

where:
\begin{itemize}
    \item $f_k$ represents individual evaluation criteria (relevance, accuracy, coherence, helpfulness)
    \item $w_k$ are the weights assigned to each criterion
    \item $m$ is the total number of criteria
\end{itemize}

\subsection{Winner Selection}

The winner of each match is determined by:

\begin{equation}
    \text{Winner}(r_i, r_j) = \begin{cases}
        r_i & \text{if } \text{Score}(r_i, r_j) > \text{Score}(r_j, r_i) \\
        r_j & \text{otherwise}
    \end{cases}
\end{equation}

Each evaluation includes:
\begin{itemize}
    \item Confidence score $\in [0, 1]$
    \item Detailed verdict explanation
    \item Structured reasoning chain
    \item Comparative analysis of strengths and weaknesses
\end{itemize}

\subsection{Metrics Output}

The final BestOf metric produces:

\begin{equation}
    \text{BestOfMetric} = \{w_{final}, \mathcal{C}, \theta\}
\end{equation}

where:
\begin{itemize}
    \item $w_{final}$ is the tournament winner
    \item $\mathcal{C}$ is the complete set of contests with verdicts
    \item $\theta$ is the aggregate confidence score
\end{itemize}

\begin{thebibliography}{9}
\bibitem{plutchik2001nature}
Plutchik, R. (2001).
\newblock The nature of emotions.
\newblock {\em American Scientist}, 89(4), 344-350.

\bibitem{mohammad2013nrc}
Mohammad, S. M., \& Turney, P. D. (2013).
\newblock Crowdsourcing a word-emotion association lexicon.
\newblock {\em Computational Intelligence}, 29(3), 436-465.

\bibitem{ibm2024granite}
IBM. (2024).
\newblock Granite Guardian: Enterprise-grade risk detection model.
\newblock {\em Hugging Face Model Hub}.

\bibitem{grice1975logic}
Grice, H. P. (1975).
\newblock Logic and conversation.
\newblock {\em Syntax and Semantics}, 3, 41-58.

\bibitem{adolphs2020evaluation}
Adolphs, L., et al. (2020).
\newblock Evaluation of neural response generation models.
\newblock {\em arXiv preprint arXiv:2001.09977}.

\bibitem{deepseek2024}
DeepSeek. (2024).
\newblock DeepSeek-R1: Advanced reasoning model.
\newblock {\em DeepSeek AI Documentation}.

\bibitem{mcinnes2018umap}
McInnes, L., Healy, J., \& Melville, J. (2018).
\newblock UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.
\newblock {\em arXiv preprint arXiv:1802.03426}.

\bibitem{mcinnes2017hdbscan}
McInnes, L., Healy, J., \& Astels, S. (2017).
\newblock hdbscan: Hierarchical density based clustering.
\newblock {\em The Journal of Open Source Software}, 2(11), 205.

\bibitem{bassignana2018hurtlex}
Bassignana, E., Basile, V., \& Patti, V. (2018).
\newblock Hurtlex: A multilingual lexicon of words to hurt.
\newblock {\em Proceedings of the Fifth Italian Conference on Computational Linguistics (CLiC-it 2018)}.
\end{thebibliography}

\end{document}
