---
title: Agentic
description: Evaluate AI agent responses with pass@K metrics and tool correctness
---

# Agentic Metric

The Agentic metric evaluates AI agent performance by measuring both answer correctness across multiple generations and the quality of tool usage.

## Overview

This metric evaluates:

- **pass@K**: At least one of K responses is correct (similarity ≥ threshold)
- **pass^K**: All K responses are correct
- **Tool Correctness**: Evaluates tool selection, parameter accuracy, execution sequence, and result utilization **for each of K responses**

<Note>
**Important:** The metric evaluates tool correctness for **ALL K responses**, not just one. This allows you to identify which responses had correct tool usage and which didn't. The default `tool_threshold=1.0` requires perfect tool usage (100% correct).
</Note>

## Installation

```bash
uv add "alquimia-fair-forge[agentic]"
uv add langchain-groq  # Or your preferred LLM provider
```

## Basic Usage

```python
from fair_forge.metrics.agentic import Agentic
from langchain_groq import ChatGroq
from your_retriever import AgenticRetriever

# Initialize the judge model
judge_model = ChatGroq(
    model="llama-3.3-70b-versatile",
    api_key="your-api-key",
    temperature=0.0,
)

# Run evaluation
metrics = Agentic.run(
    AgenticRetriever,
    model=judge_model,
    threshold=0.7,
    tool_threshold=1.0,
    verbose=True,
)

# Access results
for metric in metrics:
    print(f"pass@{metric.k}: {metric.pass_at_k}")
    print(f"pass^{metric.k}: {metric.pass_pow_k}")

    # Tool correctness scores for each of K responses
    if metric.tool_correctness_scores:
        for i, tc in enumerate(metric.tool_correctness_scores):
            if tc:
                print(f"Response {i+1} tool correctness: {tc.overall_correctness:.2f}")
```

## Parameters

### Required Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `retriever` | `Type[Retriever]` | Data source class returning multiple responses per question |
| `model` | `BaseChatModel` | LangChain-compatible model for LLM-as-judge evaluation |

### Optional Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `threshold` | `float` | `0.7` | Answer correctness threshold (0.6-0.9) |
| `tool_threshold` | `float` | `1.0` | Tool correctness threshold (0.6-1.0) - requires perfect tool usage |
| `tool_weights` | `dict[str, float]` | `0.25` each | Weights for tool aspects (selection, parameters, sequence, utilization) |
| `use_structured_output` | `bool` | `False` | Use LangChain structured output |
| `bos_json_clause` | `str` | `"```json"` | JSON block start marker |
| `eos_json_clause` | `str` | `"```"` | JSON block end marker |
| `verbose` | `bool` | `False` | Enable verbose logging |

## Data Requirements

The Agentic metric requires **multiple responses** (K) for the **same question** with different `assistant_id` values:

```python
from fair_forge.core.retriever import Retriever
from fair_forge.schemas.common import Dataset, Batch

class AgenticRetriever(Retriever):
    def load_dataset(self) -> list[Dataset]:
        # Question to evaluate
        question = "What is the capital of France?"
        ground_truth = "Paris"

        # K=3 different responses to the same question
        responses = [
            "Paris",
            "The capital of France is Paris.",
            "Poris",  # Typo - will score lower
        ]

        datasets = []
        for i, response in enumerate(responses):
            batch = Batch(
                qa_id="q1",  # Same qa_id for all K responses
                query=question,
                assistant=response,
                ground_truth_assistant=ground_truth,
                agentic={
                    "tools_used": [
                        {
                            "tool_name": "web_search",
                            "parameters": {"query": "capital of France"},
                            "result": "Paris",
                            "step": 1
                        }
                    ],
                    "final_answer_uses_tools": True
                },
                ground_truth_agentic={
                    "expected_tools": [
                        {
                            "tool_name": "web_search",
                            "parameters": {"query": "capital of France"},
                            "step": 1
                        }
                    ],
                    "tool_sequence_matters": True
                }
            )

            datasets.append(Dataset(
                session_id=f"eval-{i}",
                assistant_id=f"agent_{i}",  # Different assistant_id per response
                language="english",
                context="",
                conversation=[batch],
            ))

        return datasets
```

### Agentic Data Structure

The `Batch` schema includes optional `agentic` and `ground_truth_agentic` fields for tool evaluation:

**`agentic`** (actual tool usage):
```python
{
    "tools_used": [
        {
            "tool_name": "calculator",
            "parameters": {"operation": "add", "a": 5, "b": 7},
            "result": 12,
            "step": 1
        }
    ],
    "final_answer_uses_tools": True  # Did agent use tool results in final answer?
}
```

**`ground_truth_agentic`** (expected tool usage):
```python
{
    "expected_tools": [
        {
            "tool_name": "calculator",
            "parameters": {"operation": "add", "a": 5, "b": 7},
            "step": 1
        }
    ],
    "tool_sequence_matters": True  # Does order matter?
}
```

## Output Schema

### AgenticMetric

```python
class AgenticMetric(BaseMetric):
    session_id: str
    assistant_id: str
    qa_id: str
    k: int                                        # Number of responses evaluated
    threshold: float                              # Answer correctness threshold
    correctness_scores: list[float]               # Individual scores for each response
    pass_at_k: bool                               # At least one correct
    pass_pow_k: bool                              # All correct
    correct_indices: list[int]                    # Indices of correct responses
    tool_correctness_scores: list[ToolCorrectnessScore | None]  # Tool evaluation for each K response
```

### ToolCorrectnessScore

```python
class ToolCorrectnessScore(BaseModel):
    tool_selection_correct: float   # 0-1: Correct tools chosen
    parameter_accuracy: float       # 0-1: Correct parameters passed
    sequence_correct: float         # 0-1: Correct order (if required)
    result_utilization: float       # 0-1: Tool results used in answer
    overall_correctness: float      # Weighted average
    is_correct: bool                # overall >= tool_threshold
    reasoning: str | None           # Explanation
```

## Interpretation

### pass@K Metrics

| Metric | Description | Use Case |
|--------|-------------|----------|
| **pass@1** | Single response correct | Standard evaluation |
| **pass@3** | At least 1 of 3 correct | Reliability with retries |
| **pass@5** | At least 1 of 5 correct | High-stakes scenarios |
| **pass^K** | All K responses correct | Consistency evaluation |

### Tool Correctness Scores

<Note>
**Default threshold: 1.0** - By default, tool correctness requires perfect execution (100% correct). You can lower this with the `tool_threshold` parameter if you want to allow minor deviations.
</Note>

| Score Range | Interpretation |
|-------------|----------------|
| **1.0** | **Perfect - All aspects correct (default threshold)** |
| 0.9 - 0.99 | Excellent - Near perfect with tiny issues |
| 0.75 - 0.89 | Good - Minor issues |
| 0.6 - 0.74 | Moderate - Several issues |
| 0.4 - 0.59 | Poor - Significant problems |
| 0.0 - 0.39 | Very Poor - Incorrect tool usage |

### Tool Aspects

- **Selection** (25%): Are the right tools chosen?
- **Parameters** (25%): Are parameters correct?
- **Sequence** (25%): Is execution order correct?
- **Utilization** (25%): Are tool results used in final answer?

## Complete Example

```python
import os
from fair_forge.metrics.agentic import Agentic
from fair_forge.core.retriever import Retriever
from fair_forge.schemas.common import Dataset, Batch
from langchain_groq import ChatGroq

class MathAgentRetriever(Retriever):
    """Retriever with K=3 responses to math questions."""

    def load_dataset(self) -> list[Dataset]:
        questions = [
            {
                "qa_id": "q1",
                "query": "What is 15 + 27?",
                "ground_truth": "42",
                "responses": [
                    "42",
                    "The answer is 42",
                    "43",  # Wrong answer
                ],
                "ground_truth_agentic": {
                    "expected_tools": [
                        {
                            "tool_name": "calculator",
                            "parameters": {"operation": "add", "a": 15, "b": 27},
                            "step": 1
                        }
                    ],
                    "tool_sequence_matters": True
                }
            },
            {
                "qa_id": "q2",
                "query": "What is 100 - 35?",
                "ground_truth": "65",
                "responses": [
                    "65",
                    "65",
                    "65",  # All correct
                ],
                "ground_truth_agentic": {
                    "expected_tools": [
                        {
                            "tool_name": "calculator",
                            "parameters": {"operation": "subtract", "a": 100, "b": 35},
                            "step": 1
                        }
                    ],
                    "tool_sequence_matters": True
                }
            }
        ]

        datasets = []
        for question in questions:
            for i, response in enumerate(question["responses"]):
                # Simulate tool usage
                agentic_data = {
                    "tools_used": [
                        {
                            "tool_name": "calculator",
                            "parameters": question["ground_truth_agentic"]["expected_tools"][0]["parameters"],
                            "result": int(response) if response.isdigit() else 0,
                            "step": 1
                        }
                    ],
                    "final_answer_uses_tools": True
                }

                batch = Batch(
                    qa_id=question["qa_id"],
                    query=question["query"],
                    assistant=response,
                    ground_truth_assistant=question["ground_truth"],
                    agentic=agentic_data,
                    ground_truth_agentic=question["ground_truth_agentic"]
                )

                datasets.append(Dataset(
                    session_id=f"{question['qa_id']}-{i}",
                    assistant_id=f"agent_{i}",
                    language="english",
                    context="",
                    conversation=[batch],
                ))

        return datasets

# Initialize judge
judge = ChatGroq(
    model="llama-3.3-70b-versatile",
    api_key=os.getenv("GROQ_API_KEY"),
    temperature=0.0,
)

# Run evaluation
metrics = Agentic.run(
    MathAgentRetriever,
    model=judge,
    threshold=0.7,
    tool_threshold=1.0,
    verbose=True,
)

# Analyze results
print("Agentic Evaluation Results")
print("=" * 60)

for metric in metrics:
    print(f"\nQuestion ID: {metric.qa_id}")
    print(f"K (responses evaluated): {metric.k}")
    print(f"pass@{metric.k}: {metric.pass_at_k}")
    print(f"pass^{metric.k}: {metric.pass_pow_k}")
    print(f"Correct responses: {len(metric.correct_indices)}/{metric.k}")
    print(f"Individual scores: {[f'{s:.2f}' for s in metric.correctness_scores]}")

    # Tool correctness for each of K responses
    if metric.tool_correctness_scores:
        print(f"\nTool Correctness (K={metric.k} responses):")
        for i, tc in enumerate(metric.tool_correctness_scores, 1):
            if tc:
                print(f"\n  Response {i}:")
                print(f"    Selection:    {tc.tool_selection_correct:.2f}")
                print(f"    Parameters:   {tc.parameter_accuracy:.2f}")
                print(f"    Sequence:     {tc.sequence_correct:.2f}")
                print(f"    Utilization:  {tc.result_utilization:.2f}")
                print(f"    Overall:      {tc.overall_correctness:.2f} {'✓' if tc.is_correct else '✗'}")
                if tc.reasoning:
                    print(f"    Reasoning:    {tc.reasoning}")
            else:
                print(f"\n  Response {i}: No tools used")

# Calculate aggregate statistics
total_pass_at_k = sum(m.pass_at_k for m in metrics)
total_pass_pow_k = sum(m.pass_pow_k for m in metrics)

# Average tool correctness across all responses
all_tool_scores = [
    tc.overall_correctness
    for m in metrics
    for tc in m.tool_correctness_scores
    if tc is not None
]
avg_tool_correctness = sum(all_tool_scores) / len(all_tool_scores) if all_tool_scores else 0.0

print(f"\n{'=' * 60}")
print(f"Aggregate Statistics:")
print(f"  pass@K rate: {total_pass_at_k}/{len(metrics)} ({total_pass_at_k/len(metrics):.1%})")
print(f"  pass^K rate: {total_pass_pow_k}/{len(metrics)} ({total_pass_pow_k/len(metrics):.1%})")
print(f"  Avg tool correctness: {avg_tool_correctness:.2f}")
```

## LLM Provider Options

<CodeGroup>
```python Groq
from langchain_groq import ChatGroq

model = ChatGroq(
    model="llama-3.3-70b-versatile",
    api_key="your-api-key",
    temperature=0.0,
)
```

```python OpenAI
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4o",
    api_key="your-api-key",
    temperature=0.0,
)
```

```python Anthropic Claude
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    api_key="your-api-key",
    temperature=0.0,
)
```

```python Ollama (Local)
from langchain_ollama import ChatOllama

model = ChatOllama(
    model="llama3.1:70b",
    temperature=0.0,
)
```
</CodeGroup>

## Understanding Tool Correctness for K Responses

The metric evaluates tool correctness **independently for each of K responses**. This helps identify:
- Which responses had correct tool usage
- Common patterns in tool mistakes
- Whether better answers correlate with better tool usage

### Example with K=3

```python
# Assume we have K=3 responses for "What is 5 + 7?"
# Response 1: Uses calculator with correct parameters → Overall: 1.0 ✓
# Response 2: Uses calculator with WRONG parameters (a=5, b=8) → Overall: 0.75 ✗
# Response 3: Doesn't use tools at all → tool_correctness_scores[2] = None

metrics = Agentic.run(AgenticRetriever, model=judge_model, tool_threshold=1.0)

for metric in metrics:
    # metric.tool_correctness_scores = [ToolCorrectnessScore(...), ToolCorrectnessScore(...), None]
    print(f"Response 1 tool score: {metric.tool_correctness_scores[0].overall_correctness}")  # 1.0
    print(f"Response 2 tool score: {metric.tool_correctness_scores[1].overall_correctness}")  # 0.75
    print(f"Response 3 tool score: {metric.tool_correctness_scores[2]}")  # None
```

### Why tool_threshold=1.0 by Default?

Tool usage is **deterministic** - either the agent called the right tool with right parameters or it didn't. Unlike answer correctness (which can be subjective), tool correctness should be strict:

- **1.0 (Perfect)**: Agent used exactly the right tools, parameters, sequence, and utilized results
- **&lt;1.0 (Imperfect)**: Something was wrong - wrong tool, wrong params, wrong order, or didn't use results

If you want to allow **small deviations** (e.g., extra parameters that don't affect the result), lower the threshold:

```python
metrics = Agentic.run(
    AgenticRetriever,
    model=judge_model,
    tool_threshold=0.9,  # Allow 10% deviation
)
```

## Custom Tool Weights

Adjust weights based on your use case:

```python
# Emphasize tool selection and utilization
weights = {
    "selection": 0.4,      # Tool choice is critical
    "parameters": 0.2,     # Parameters less important
    "sequence": 0.1,       # Order doesn't matter much
    "utilization": 0.3,    # Using results is important
}

metrics = Agentic.run(
    AgenticRetriever,
    model=judge_model,
    tool_weights=weights,
)
```

## Use Cases

<CardGroup cols={2}>
  <Card title="Agent Reliability" icon="shield-check">
    Measure consistency across multiple agent runs with pass@K metrics
  </Card>
  <Card title="Tool Usage Quality" icon="wrench">
    Evaluate if agents correctly select and use available tools
  </Card>
  <Card title="Code Generation" icon="code">
    Test code generation with K samples (pass@1, pass@5, pass@10)
  </Card>
  <Card title="Multi-Turn Tasks" icon="messages">
    Assess agentic behavior in complex multi-step workflows
  </Card>
</CardGroup>

## Best Practices

<AccordionGroup>
  <Accordion title="Choose Appropriate K Values">
    - **K=1**: Standard single-response evaluation
    - **K=3-5**: Balance between reliability and cost
    - **K=10+**: High-stakes scenarios (code generation, medical advice)
  </Accordion>

  <Accordion title="Set Meaningful Thresholds">
    - **Strict (0.8-0.9)**: Factual accuracy matters (medical, legal)
    - **Moderate (0.7)**: General purpose (recommended default)
    - **Lenient (0.6)**: Creative or subjective tasks
  </Accordion>

  <Accordion title="Define Clear Tool Expectations">
    Provide complete `ground_truth_agentic` with:
    - Expected tool names
    - Required parameters
    - Whether sequence matters
    - Whether tool results should influence final answer
  </Accordion>

  <Accordion title="Use Strong Judge Models">
    Larger models (GPT-4, Claude-3, Llama-3-70B+) provide more reliable correctness evaluations.
  </Accordion>
</AccordionGroup>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Judge Returns Low Scores for Correct Answers">
    **Cause**: Judge model may be too strict or ground truth is ambiguous.

    **Solution**:
    - Lower the `threshold` parameter (try 0.6-0.65)
    - Use a more capable judge model
    - Ensure ground truth is clear and unambiguous
    - Check verbose logs to see judge reasoning
  </Accordion>

  <Accordion title="Tool Correctness Always Fails">
    **Cause**: Default `tool_threshold=1.0` requires perfect tool usage, or `ground_truth_agentic` expectations don't match actual tool usage.

    **Solution**:
    - **Lower the threshold** if you want to allow small deviations: `tool_threshold=0.9`
    - Verify tool names match exactly (case-sensitive)
    - Check parameter structure (keys and values must match exactly)
    - Confirm step numbers if sequence matters
    - Set `tool_sequence_matters: False` if order doesn't matter
    - Check each response's tool score individually: `metric.tool_correctness_scores[i]`
  </Accordion>

  <Accordion title="Some Responses Have None for Tool Correctness">
    **Cause**: Not all K responses used tools, which is expected behavior.

    **Solution**:
    - This is normal - `tool_correctness_scores[i]` will be `None` if response didn't use tools
    - Check if tools were needed: `batch.agentic.get('tools_used', [])`
    - Filter out None values when calculating averages:
      ```python
      valid_scores = [tc for tc in metric.tool_correctness_scores if tc is not None]
      avg = sum(tc.overall_correctness for tc in valid_scores) / len(valid_scores)
      ```
  </Accordion>

  <Accordion title="Multiple Datasets with Same qa_id Not Grouping">
    **Cause**: Datasets may have different `qa_id` values.

    **Solution**:
    - Ensure all K responses have **identical** `qa_id`
    - Use different `assistant_id` for each response
    - Check logs with `verbose=True` to see grouping
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={3}>
  <Card title="BestOf Metric" icon="trophy" href="/metrics/best-of">
    Compare multiple agents in tournament-style evaluation
  </Card>
  <Card title="Runners" icon="play" href="/runners/overview">
    Execute agentic tests against AI systems
  </Card>
  <Card title="AWS Lambda" icon="aws" href="/examples/aws-lambda">
    Deploy Agentic as serverless function
  </Card>
</CardGroup>
