{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff02e77-ce1c-4b99-b3dd-c8ed2db1ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a3e22-58e8-4953-8abf-24bfd9791e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.runtime import Judge\n",
    "from helpers.dataset import Conversation\n",
    "from helpers.fair_forge import FairForge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80672b81-e63c-417f-93d6-3933a4c9fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from pydantic import BaseModel\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELASTIC_URL = os.environ.get('ELASTIC_URL')\n",
    "ELASTIC_AUTH = [os.environ.get('ELASTIC_AUTH_USER'), os.environ.get('ELASTIC_AUTH_PASSWORD')]\n",
    "dataset = os.environ.get(\"dataset\", \"asb\")\n",
    "conversation_index = f\"{dataset}-conversational\"\n",
    "\n",
    "reasoning_system_prompt = \"\"\"\n",
    "You are an expert evaluator of conversational dialogue quality. Your task is to evaluate the assistant's response with a focus on its ability to recall and reference past details mentioned earlier in the conversation. Follow these steps:\n",
    "\n",
    "1. Analyze the provided observation for clarity, relevance, and accuracy regarding the dialogue performance.\n",
    "2. Evaluate the assistant's response in the context of the observation.\n",
    "3. Determine if the assistant's answer effectively addresses or aligns with the points raised in the observation.\n",
    "4. Assess the overall consistency, accuracy, and contextual relevance of the assistant's answer.\n",
    "5. Clearly explain under 'insight' anything you thought about.\n",
    "6. The answer from Assistant (Actual Answer) must be {preferred_language} , otherwise give it a low score even though the question from the human and the answer are in the same language.\n",
    "7. The memory score it must be 100% if the question is not referring to past events.\n",
    "8. Use the Grice's Maxims to check the Assistant Actual Answer, the Maxims are:\n",
    "\n",
    "- The maxim of quantity, where one tries to be as informative as one possibly can, and gives as much information as is needed, and no more.\n",
    "\n",
    "- The maxim of quality, where one tries to be truthful, and does not give information that is false or that is not supported by evidence.\n",
    "\n",
    "- The maxim of relation, where one tries to be relevant, and says things that are pertinent to the discussion.\n",
    "\n",
    "- The maxim of manner, when one tries to be as clear, as brief, and as orderly as one can in what one says, and where one avoids obscurity and ambiguity.\n",
    "\n",
    "9. For the sensibleness metric you should take into account how much does the Actual Answer makes sense from the question. Based on SSA (Sensibleness and specificity Average) metric\n",
    "10. Score values must go from a range between 0 and 10, 0 Is the lowest and 10 the highest.\n",
    "\n",
    "After your internal reasoning, provide only the final answer strictly in the following JSON format. Do not include any additional text or explanation:\n",
    "\n",
    "```json\n",
    "{{ \n",
    "     \"memory\": <score value>, \n",
    "     \"language\": <score value>, \n",
    "     \"insight\": \"<your insight>\",\n",
    "     \"quality_maxim\": <score value>,\n",
    "     \"quantity_maxim\": <score value>,\n",
    "     \"relation_maxim\": <score value>,\n",
    "     \"manner_maxim\": <score value>,\n",
    "     \"sensibleness\": <score value>\n",
    "}}\n",
    "```\n",
    "\n",
    "Assistant (Actual Answer):\n",
    "{assistant_answer}\n",
    "\n",
    "Ground Truth Assistant (Reference Answer):\n",
    "{ground_truth_assistant}\n",
    "\"\"\"\n",
    "\n",
    "reasoning_system_prompt_observation=\"\"\"\n",
    "You are an expert evaluator of conversational dialogue quality. Your task is to evaluate the performance of an assistant based on the given observation and the corresponding assistant response. Follow a detailed chain-of-thought reasoning process, taking into account the following:\n",
    "\n",
    "1. Analyze the provided observation for clarity, relevance, and accuracy regarding the dialogue performance.\n",
    "2. Evaluate the assistant's response in the context of the observation.\n",
    "3. Determine if the assistant's answer effectively addresses or aligns with the points raised in the observation.\n",
    "4. Assess the overall consistency, accuracy, and contextual relevance of the assistant's answer.\n",
    "5. Clearly explain under 'insight' anything you thought about.\n",
    "6. The answer from Assistant (Actual Answer) must be {preferred_language} , otherwise give it a low score even though the question from the human and the answer are in the same language.\n",
    "7. The memory score it must be 100% if the question is not referring to past events.\n",
    "8. Use the Grice's Maxims to check the Assistant Actual Answer, the Maxims are:\n",
    "\n",
    "- The maxim of quantity, where one tries to be as informative as one possibly can, and gives as much information as is needed, and no more.\n",
    "\n",
    "- The maxim of quality, where one tries to be truthful, and does not give information that is false or that is not supported by evidence.\n",
    "\n",
    "- The maxim of relation, where one tries to be relevant, and says things that are pertinent to the discussion.\n",
    "\n",
    "- The maxim of manner, when one tries to be as clear, as brief, and as orderly as one can in what one says, and where one avoids obscurity and ambiguity.\n",
    "\n",
    "9. For the sensibleness metric you should take into account how much does the Actual Answer makes sense from the question. Based on SSA (Sensibleness and specificity Average) metric\n",
    "10. Score values must go from 0 to 10. Being 0 the lowest and 10 the highest\n",
    "\n",
    "After your internal reasoning, provide only the final answer strictly in the following JSON format. Do not include any additional text or explanation:\n",
    "\n",
    "```json\n",
    "{{ \n",
    "     \"memory\": <score value>, \n",
    "     \"language\": <score value>, \n",
    "     \"insight\": \"<your insight>\",\n",
    "     \"quality_maxim\": \"<score value>\",\n",
    "     \"quantity_maxim\": \"<score value>\",\n",
    "     \"relation_maxim\": \"<score value>\",\n",
    "     \"manner_maxim\": \"<score value>\",\n",
    "     \"sensibleness\": \"<score value>\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Observation:\n",
    "{observation}\n",
    "\n",
    "Assistant (Actual Answer):\n",
    "{assistant_answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalBatch(BaseModel):\n",
    "    conversational_memory: float\n",
    "    conversational_insight: str\n",
    "    conversational_language: float\n",
    "    conversational_quality_maxim: float\n",
    "    conversational_quantity_maxim: float\n",
    "    conversational_relation_maxim: float\n",
    "    conversational_manner_maxim: float\n",
    "    conversational_sensibleness: float\n",
    "    conversational_thinkings: str\n",
    "    session_id: str\n",
    "    assistant_id: str\n",
    "    qa_id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0cd6d-be23-4a3b-8eb6-3351e72125dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_index(index_name: str, mapping: dict):\n",
    "    if es.indices.exists(index=index_name):\n",
    "        es.indices.delete(index=index_name)\n",
    "        print(f\"Index '{index_name}' deleted.\")\n",
    "    es.indices.create(index=index_name, body=mapping)\n",
    "    print(f\"Index '{index_name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d34a60-b798-45a4-8f1b-51420cb7a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationAnalyzer(Logos):\n",
    "    def process(self, thread: Conversation):\n",
    "        judge = Judge()\n",
    "        for batch in tqdm(thread.conversation, desc=\"Processing conversation batches\", leave=False):\n",
    "            query = batch.question\n",
    "            logging.info(f\"Processing query: {query}\")\n",
    "            data = {\"preferred_language\": thread.preferred_language, \"assistant_answer\": batch.assistant}\n",
    "            if batch.observation:\n",
    "                logging.info(\"Observation found; invoking reasoning with observation\")\n",
    "                thinking, json = judge.reason(\n",
    "                    reasoning_system_prompt_observation,\n",
    "                    query,\n",
    "                    {\"observation\": batch.observation, **data}\n",
    "                )\n",
    "            else:\n",
    "                logging.info(\"No observation; invoking standard reasoning\")\n",
    "                thinking, json = judge.reason(\n",
    "                    reasoning_system_prompt,\n",
    "                    query, {\"ground_truth_assistant\": batch.assistant, **data})\n",
    "\n",
    "            batch = ConversationalBatch(\n",
    "                    conversational_insight=json['insight'],\n",
    "                    conversational_memory=json['memory'],\n",
    "                    conversational_language=json['language'],\n",
    "                    conversational_quality_maxim=json['quality_maxim'],\n",
    "                    conversational_quantity_maxim=json['quantity_maxim'],\n",
    "                    conversational_relation_maxim=json['relation_maxim'],\n",
    "                    conversational_manner_maxim=json['manner_maxim'],\n",
    "                    conversational_sensibleness=json['sensibleness'],\n",
    "                    conversational_thinkings=thinking,\n",
    "                    session_id=thread.session_id,\n",
    "                    assistant_id= thread.assistant_id,\n",
    "                    qa_id=batch.qa_id\n",
    "                )\n",
    "            self.metrics.append(batch)\n",
    "        logging.info(f\"Finished processing thread for session_id: {thread.session_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    ELASTIC_URL,\n",
    "    basic_auth=tuple(ELASTIC_AUTH),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational = ConversationAnalyzer()\n",
    "metrics = conversational.pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_conversational = {\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"session_id\": {\"type\": \"keyword\"},\n",
    "      \"conversational_memory\": {\"type\": \"float\"},\n",
    "      \"conversational_insight\": {\"type\": \"text\"},\n",
    "      \"conversational_language\": {\"type\": \"float\"},\n",
    "      \"conversational_quality_maxim\": {\"type\": \"float\"},\n",
    "      \"conversational_quantity_maxim\": {\"type\": \"float\"},\n",
    "      \"conversational_relation_maxim\": {\"type\": \"float\"},\n",
    "      \"conversational_manner_maxim\": {\"type\": \"float\"},\n",
    "      \"conversational_sensibleness\": {\"type\": \"float\"},\n",
    "      \"conversational_thinkings\": {\"type\": \"text\"},\n",
    "      \"qa_id\": {\"type\": \"keyword\"},\n",
    "      \"assistant_id\": {\"type\": \"keyword\"}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "recreate_index(conversation_index, mapping_conversational)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for metric in metrics:\n",
    "    docs.append({\n",
    "            \"_index\": conversation_index,\n",
    "            \"_source\": metric.model_dump()\n",
    "    })\n",
    "\n",
    "helpers.bulk(es, docs)\n",
    "print(f\"Indexed {len(docs)} documents.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
